\documentclass{article}
\input{macros.tex}
\title{Stochastic Ising Block Model on Multiple Communities}
\author{Feng Zhao}
\begin{document}
	\maketitle
\begin{abstract}
	Recently a composition of Stochastic Block Model (SBM) and Ising model, called SIBM, was proposed for the case
	of two communities, and a sharp threshold on the sample complexity for exact recovery was established.
	In this paper, we study the SIBM model in the general case of multiple communities and prove a similar sharp
	threshold phenomena. We modify an existing recovery algorithm to achieve the threshold
	and carry out more detailed analysis for this problem. Finally, we show some connections between SIBM and the modularity maximization method.
\end{abstract}

\section{Introduction}
	In network analysis, Stochastic Block Model (SBM) is a commonly used random graph model, in which the probability of edge existence is higher within the community than between different communities \cite{holland1983stochastic, Abbe17}. For SBM, the condition on exact recovery of community labels has been studied extensively and the phase transition property has been established \cite{abbe2015community, mossel2016}. Meanwhile, Ising model is a well-known statistical model in physics which has some similarity with SBM \cite{ising1925beitrag}. The nodes with a common edge are more likely to share the same state in Ising model\label{key}.
As a probabilistic model, Ising model has been applied to investigate the social voting phenomenon \cite{banerjee2008model}, further indicating there is some hidden relationship between SBM and Ising model.

Recently a new model called Stochastic Ising Block Model (SIBM) was proposed in \cite{ye2020exact}. SIBM concatenates SBM and Ising model in the sense that it uses SBM to generate the graph and then uses Ising model to generate node labels. The sample complexity for exact recovery in SIBM was investigated in \cite{ye2020exact} and a sharp threshold was established. However, the formulation of SIBM in \cite{ye2020exact} is restricted for the case of two communities, and the problem for multiple communities case remains open.
We notice that Ising model is not limited to two states, 
and we use multiple-state Ising model \cite{potts1952some} to obtain similar results of SIBM on multiple communities.

In this paper, we  investigate SIBM on multiple communities and focus on the problem of exactly recovery of the node label.
We  compute the feasible regime of parameters and the sample complexity for exact recovery. Besides, the relationship between SIBM and the modularity maximization method
is also discussed. 

	% notation convetion
Throughout this paper, the number of community is denoted by $k$; $m$ is the number of samples; $\lfloor x \rfloor$ is the floor function of $x$; the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$;
the label of each node is a random variable $X_i$; $X_i$ is chosen from $W= \{1, \omega, \dots, \omega^{k-1}\}$ and we further require $W$
is a cyclic group with order $k$; $W^n$ is the n-ary Cartesian power of $W$; $f$ is a permutation function on $W$ and applied to $W^n$ in elementwise way; the set $S_k$ is used to represent all permutation functions on $W$ and $S_k(\sigma):=\{f(\sigma)| f\in S_k\}$ for $\sigma \in W^n$; the indicator function $\delta_{xy}$ or $\delta(x,y)$ is defined as
$\delta_{xy} = 1 $ when $x=y$, and $\delta_{xy}=0$ when $x\neq y$; $g(n) = \Theta(f(n))$ if there exists constant $c_1 < c_2$ such that $c_1 f(n) \leq g(n) \leq c_2 f(n)$
for large $n$;
$\Lambda := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k-1\}$
where $\mathbf{1}_n$ is the all one vector with dimension $n$;
we define the distance of two vectors as:
$\dist(\sigma, X)
=|\{i\in[n]:\sigma_i\neq X_i\}| \textrm{ for } \sigma,X\in W^n
$ and the distance of a vector to a space $S\subseteq W^n$
as
$\dist(\sigma,S)
:=\min\{\dist(\sigma, \sigma') | \sigma' \in S\}
$.
\section{Problem setup and main results} \label{s:Preliminaries}
We first recall the definition of Symmetric Stochastic Block Model (SSBM) with $k$ communities and the definition of Ising model with $k$ state (See Definition 4 in \cite{Abbe17}).
\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
Let $n$ be a positive even integer, $W= \{1, \omega, \dots, \omega^{k-1}\}$ is a cyclic group with order $k$ and let $p,q\in[0,1]$ be two real numbers. Let $X=(X_1,\dots,X_n)\in W^n$, and let $G=([n],E(G))$ be an undirected graph with vertex set $[n]$ and edge set $E(G)$. The pair $(X,G)$ is drawn under $\SSBM(n,k,p,q)$ if 

\noindent
(i) $X$ is drawn uniformly with the constraint that $|\{v \in [n] : X_v = \omega^i\}| = \frac{n}{k}$;

\noindent
(ii) the vertices $i$ and $j$ in $G$ are connected with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$, independently of other pairs of vertices.
\end{definition}
 
	From the symmetric property of SBM, the conditional distribution $P(G|X=x) = P(G|X=f(x)), \forall f \in S_k$. Therefore, it is only possible to recover $X$ from $G$ up to a global permutation. That is, it is only possible to recover $S_k(X)$.

In this paper, we focus on the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. In this regime, it is well known that exact recovery of $X$ (up to a global permutation) from $G$ is possible if $\sqrt{a}-\sqrt{b} > \sqrt{k}$ \cite{abbe2015community}.

Given a labeling $X$ of $n$ vertices, the SBM specifies how to generate a random graph on these $n$ vertices according to the labeling. In some sense, Ising model works in the opposite direction, i.e., given a graph $G$, Ising model defines a probability distribution on all possible labels of these $n$ vertices. 

 \begin{definition}[Ising model]
Define an Ising model on a graph $G=([n],E(G))$ with parameters $\alpha,\beta>0$ as the probability distribution on the configurations $\sigma\in W^n$ such that\footnote{When there is only one sample, we usually denote it as $\bar{\sigma}$. When there are $m$ (independent) samples, we usually denote them as $\sigma^{(1)},\dots,\sigma^{(m)}$.}
\begin{equation} \label{eq:isingma}
P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\beta\sum_{\{i,j\}\in E(G)} \delta_{\bar{\sigma}_i \bar{\sigma}_j}
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} \delta_{\bar{\sigma}_i \bar{\sigma}_j} \Big),
\end{equation}
where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and 
\begin{equation}  \label{eq:zg}
Z_G(\alpha,\beta)=\sum_{\bar{\sigma} \in W^n} \exp\Big(\beta\sum_{\{i,j\}\in E(G)}\delta_{\bar{\sigma}_i \bar{\sigma}_j} 
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} \delta_{\bar{\sigma}_i \bar{\sigma}_j}  \Big) 
\end{equation}
is the normalizing constant.
\end{definition}






By definition we always have $P_{\sigma|G}(\sigma=\bar{\sigma})=P_{\sigma|G}(\sigma=f(\bar{\sigma}))$ in the Ising model. Next we present our new model, the Stochastic Ising Block Model (SIBM), which can be viewed as a natural composition of the SSBM and the Ising model. In SIBM, we first draw a pair $(X,G)$ under $\SSBM(n,k, p,q)$.  Then we draw $m$ independent samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ from the Ising model on the graph $G$, where $\sigma^{(u)}\in W^n$ for all $u\in[m]$.

\begin{definition}[Stochastic Ising Block Model]
Let $n$ and $m$ be positive integers such that $n$ is even. Let $p,q\in[0,1]$ be two real numbers and let $\alpha,\beta>0$. The triple $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ is drawn under $\SIBM(n,k, p,q,\alpha,\beta,m)$ if

\noindent
(i) the pair $(X,G)$ is drawn under $\SSBM(n,k, p,q)$;

\noindent
(ii) for every $i\in[m]$, each sample $\sigma^{(i)}=(\sigma_1^{(i)},\dots,\sigma_n^{(i)}) \in W^n$ is drawn independently according to the distribution \eqref{eq:isingma}.
\end{definition}

Notice that we only draw the graph $G$ once in SIBM, and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ are drawn independently from the Ising model on the {\em same} graph $G$.
Our objective is to recover the underlying partition (or the ground truth) $X$ from the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$, and we would like to use the smallest possible number of samples to guarantee the exact recovery of $X$ up to a global permutation.
Below we use the notation $P_{\SIBM}(A):=E_G[P_{\sigma|G}(A)]$ for an event $A$, where the expectation $E_G$ is taken with respect to the distribution given by SSBM. In other words, $P_{\sigma|G}$ is the conditional distribution of  $\sigma$ given a fixed $G$ while $P_{\SIBM}$ is the joint distribution of both $\sigma$ and $G$.
By definition, we have $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f \in S_k$.

\begin{definition}[Exact recovery in SIBM]
Let $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k, p,q,\alpha,\beta,m)$.
We say that exact recovery is solvable for $\SIBM(n,k,p,q,\alpha,\beta,m)$ if there is an algorithm that takes $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ as inputs and outputs $\hat{X}=\hat{X}(\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ such that
$$
P_{\SIBM}(\hat{X} \in S_k(X)) \to 1
\text{~~~as~} n\to\infty ,
$$
and we call $P_{\SIBM}(\hat{X} \in S_k(X))$ the success probability of the recovery/decoding algorithm.
\end{definition}


As mentioned above, we consider the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. By definition, the ground truth $X$, the graph $G$ and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ form a Markov chain $X\to G\to \{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Therefor, if we cannot recover $X$ from $G$, then there is no hope to recover $X$ from $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Thus a necessary condition for the exact recovery in SIBM is $\sqrt{a}-\sqrt{b}> \sqrt{k}$, and we will limit ourselves to this case throughout the paper.

\vspace*{.1in}
\noindent{\bf Main Problem:} \emph{For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, what is the smallest sample size $m^\ast$ such that exact recovery is solvable for $\SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m^\ast)$?}

\vspace*{.1in}  It is this optimal sample size problem that we address---and resolve---in this paper. 
Our main results read as follows.

\begin{theorem} \label{thm:wt1}
For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, let
\begin{equation} \label{eq:defstar}
\beta^\ast := 
\log\frac{a+b-k-\sqrt{(a+b-k)^2-4ab}}{2 b} \text{~~and~~}
m^\ast := k \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1  .
\end{equation}
{\bf Case (i) when $\alpha>b\beta$}: If $m\ge m^\ast$, then exact recovery is solvable in $O(n)$ time for $\SIBM(n,k, a\log(n)/n, \linebreak[4] b\log(n)/n,\alpha,\beta,m)$.
If $\beta^\ast/\beta$ is not an integer and $m < m^*$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$. If $\beta^\ast/\beta$ is an integer and $m < m^* - k$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$.
{\bf Case (ii) when $\alpha<b\beta$}: Exact recovery of $~\SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m)$ is not solvable for any $m=O(\log^{\delta}(n))$ for given $\delta \in (0, 1)$, and in particular, it is not solvable for any constant $m$ that does not grow with $n$.
\end{theorem}
Note that the condition $\sqrt{a}-\sqrt{b} > \sqrt{k}$ guarantees that the term $\sqrt{(a+b-k)^2-4ab}$ in the definition of $\beta^\ast$ is a  real number.
When $\alpha>b\beta$ and $\beta^\ast/\beta$ is not an integer,
the above theorem establishes a sharp recovery threshold $m^\ast$ on the number of samples. It is worth mentioning that the threshold $m^\ast$ do {\em not} depend on the value of the parameter $\alpha$, as long as $\alpha$ satisfies $\alpha>b\beta$.
Below we present an equivalent characterization of the recovery threshold in terms of $\beta$.
\begin{theorem} \label{thm:wt2}
	Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$. 
	Let 
	$$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n, k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m).
	$$
	If $\lfloor \frac{m+k-1}{k} \rfloor \beta>\beta^\ast$, then there is an algorithm that recovers $X$ from the samples in $O(n)$ time with success probability $1-o(1)$. If $\lfloor \frac{m+k-1}{k} \rfloor \beta <\beta^\ast$, then the success probability of any recovery algorithm is $o(1)$. 
\end{theorem}
From Lemma \ref{lem:thm23eq} in the Appendix, Theorem~\ref{thm:wt1} and Theorem~\ref{thm:wt2} give the same threshold.


\begin{theorem}  \label{thm:wt3}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$. Let $m$ be a constant integer that does not grow with $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m).
$$
Define
\begin{equation}\label{eq:gbeta}
g(\beta)  := \frac{b e^{\beta}+a e^{-\beta}}{k}-\frac{a+b}{k}+1
\end{equation}
If $\beta>\beta^\ast$, then
$$
P_{\SIBM}(\sigma^{(i)} \in S_k(X) \text{~for all~} i\in[m]) = 1-o(1).
$$
If $\beta\le \beta^\ast$, then
$$
P_{\SIBM}(\dist(\sigma^{(i)}, S_k(X))= \Theta(n^{g(\beta)}) \text{~for all~} i\in[m]) = 1-o(1) .
$$
\end{theorem}
One can show that (i) $g(\beta)$ is a strictly decreasing function in $[0,\beta^\ast]$, (ii) $g(0)=1$ and (iii) $g(\beta^\ast)=0$. Therefore, $0<g(\beta)<1$ when $0<\beta<\beta^\ast$. Thus Theorem~\ref{thm:wt3} implies that for all $\beta\le \beta^\ast$, $\dist(\sigma^{(i)},S_k(X))=o(n)$ for all $i\in[m]$. In particular, for $\beta = \beta^\ast$, $\dist(\sigma^{(i)},S_k(X))=\Theta(1)$ for all $i\in[m]$.
\section{Sketch of the proof}
\label{sect:sketch}

In this section, we illustrate the main ideas and explain the important steps in the proof of the main results. The complete proofs are given in Section~\ref{sect:aln}--\ref{sect:converse}.
As the first step, we prove that for $a>b>0$, if $\alpha>b\beta$, then all the samples are centered in $\Gamma$ with probability $1-o(1)$; if $\alpha<b\beta$, then all the samples are centered in $\Lambda$.
\subsection{Why $\alpha > b \beta$}
To successfully recover the community, a necessary condition is that the probability of the following event converges to zero as $n\to\infty$.
\begin{align}
P_{\sigma | G}(\sigma =  \mathbf{1}_n) & > P_{\sigma | G}(\sigma = X) \label{eq:1x}
\end{align}
For \eqref{eq:1x}, we can treat it as a hypothesis testing problem between $\sigma = X$ versus $\sigma = \mathbf{1}_n$.
The ML algorithm makes Type I error when event \eqref{eq:1x} happens. We use log-likelihood to simplify
\eqref{eq:1x} to:
\begin{equation}\label{eq:Zij}
\sum_{(i,j)\in E, X_i \neq X_j} Z_{ij} > (\frac{\alpha}{\beta} + o(1)) \frac{\log n}{n} \frac{k-1}{2k}n^2 =: c
\end{equation}
% \frac{k-1}{2k}n^2 = \sum_{(i,j)\in E, X_i \neq X_j} 1
where $Z_{ij}$ i.i.d. $\sim \textrm{Bernoulli}(\frac{b\log n }{n})$, which represents the edge existence between different communities. The mean value on the left hand side is $\frac{b \log n }{n} \frac{k-1}{2k}n^2$. Therefore we can use large
deviation theory to bound the probability of \eqref{eq:Zij}. To be more specific, by Chernoff inequality we have
\begin{equation}\label{eq:CZij}
\Pr\left(\sum_{(i,j)\in E, X_i \neq X_j} Z_{ij} >  c \right)\leq \frac{\mathbb{E}[\exp(t Z_{ij})]^{ \frac{k-1}{2k}n^2 }}{\exp(ct)}
\end{equation}
where $ t  = \log \frac{\alpha}{b\beta} > 0$ is chosen to minimize the function on the right hand side of \eqref{eq:CZij}. It follows that
\begin{equation}\label{eq:nlogn}
\Pr(\sum_{\substack{(i,j)\in E \\ X_i \neq X_j}} Z_{ij} >  c )\leq \exp(n\log n  \frac{k-1}{2k} (h_b(\frac{\alpha}{\beta}) + o(1)))
\end{equation}
where $h_b(x) = x - b - x \log\frac{x}{b}$. We can verify that $h_b(x) < 0 $ when $ x > b$.
Therefore, we choose $\alpha > b \beta$	to make \eqref{eq:nlogn} decreases to zero as $n\to\infty$.

When $\alpha < b \beta$, using the same techniques as above we can show that
\begin{align}
P_{\sigma | G}(\sigma = X ) & > \exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n) \label{eq:1x_e}
\end{align}
where $C$ is any positive constant.
$\exp(-Cn)$ can be added since the dominant term of decreasing rate is $\exp(-n\log n)$, as shown by \eqref{eq:nlogn}.

Generally, when $\dist(\bar{\sigma}, \mathbf{1}_n) \geq \frac{n}{\log^{\delta} n}$ and $\bar{\sigma}$
is nearer to $\mathbf{1}_{n}$ than other $\omega^r \cdot \mathbf{1}_n$, we could show that
$P_{\sigma | G}(\sigma = \bar{\sigma} ) > \exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n)$
happens with probability $O(\exp(-n \log^{1-\delta} n ))$. Using union bound we have
$P_{\sigma | G}(\sigma = \bar{\sigma} ) \leq \exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n)$
happens in probability $1-o(1)$ for all such $\bar{\sigma}$.
Since $P_{\sigma | G}(\dist(\bar{\sigma}, \Lambda)\geq \frac{n}{\log^{\delta} n}) \leq
(k-1)\sum_{\dist(\bar{\sigma}, \mathbf{1}_n) \geq \frac{n}{\log^{\delta} n}}\exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n)
\leq (k-1)k^n \exp(-Cn) = o(1)$ for $C> \log k$, we conclude that
$\dist(\sigma^{(i)}, \Lambda)< n/\log^{\delta}(n)$ for all $i\in[m]$ with probability $1-o(1)$ if $\alpha<b\beta$. See Proposition~\ref{prop:1} for a rigorous proof.
In this case, each sample only take $\sum_{j=0}^{n/\log^{\delta}(n)}\binom{n}{j}j^{k-1}$ values, so each sample contains at most $\log_k(\sum_{j=0}^{n/\log^{\delta}(n)}\binom{n}{j}j^{k-1})=O(\frac{\log\log(n)}{\log^{\delta}(n)} n)$ bits of information about $X$. On the other hand, $X$ itself is uniformly distributed over a set of $\frac{n!}{(n/k)^k}$ vectors, so one needs at least $\log_k\frac{n!}{(n/k)^k}=\Theta(n)$ bits of information to recover $X$. Thus if $\alpha<b\beta$, then exact recovery of $X$ requires at least $\Omega(\frac{\log^{\delta}(n)}{\log\log(n)})\ge \Omega(\log^{\delta'}(n))$ samples for $\delta'<\delta$. see Proposition~\ref{prop:ab} for a rigorous proof.


For the rest of this section, we will focus on the case $\alpha>b\beta$ and establish the sharp threshold on the sample complexity. We first analyze the typical behavior of one sample and explain how to prove Theorem~\ref{thm:wt3}. Then we use the method developed for the one sample case to analyze the distribution of multiple samples, which allows us to prove Theorem~\ref{thm:wt2}. Note that Theorem~\ref{thm:wt1} follows directly from Theorem~\ref{thm:wt2}.

\subsection{Why is $\beta^\ast$ the threshold?} \label{sect:why}
	Similar to the analysis of the last section, when exact recovery is possible, the probability of the following event should be $o(1)$
\begin{align}
P_{\sigma | G}(\dist(\sigma, X) = 1) & > P_{\sigma | G}(\sigma = X)\label{eq:betastar}
\end{align}
We consider the $i$-th coordinate in which $\sigma_i \neq X_i$ and denote the event $T_{ir}=\{\sigma_i = \omega^r \cdot X_i, \sigma_j = X_j \forall j \neq i\}$.
Then $P_{\sigma | G}(\dist(\sigma, X) = 1) = \sum_{i=1}^n\sum_{r=1}^{k-1} P_{\sigma | G}(T_{ir})$.
We also define
\begin{equation*}
A^r_i=A^r_i(G):=|\{j\in[n]\setminus\{i\}:\{i,j\}\in E(G), X_j=\omega^r \cdot X_i\} |
\end{equation*}
%	to represent the number of edges connected with $i$ whose two nodes differ by $\omega^r$.
By definition,
$A^0_i\sim \Binom(\frac{n}{k}-1,\frac{a\log(n)}{n})$, $A^r_i\sim \Binom(\frac{n}{k}, \frac{b\log(n)}{n})$ for $r=1,\dots, k-1$, and they are independent.

We then have
\begin{align}
&\frac{P_{\sigma|G}(T_{ir})}
{P_{\sigma|G}(\sigma=X)}
= \exp\Big(\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i) \nonumber\\
&-\frac{\alpha\log(n)}{n} \Big) 
= (1+o(1)) \exp ( \beta(A^r_i-A^0_i))
\end{align}
By Chernoff inequality $ \Pr(\sum_{i=1}^n\sum_{r=1}^{k-1}\exp ( \beta(A^r_i-A^0_i)) > 1) \leq (k-1)n\mathbb{E}_G[\exp (\beta (A^r_i-A^0_i))] $.
\begin{align}
&\mathbb{E}_G[\exp (\beta (A^r_i-A^0_i))]
=\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^{\beta} \Big)^{n/k} \nonumber \\
&\cdot \Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-\beta} \Big)^{n/k-1}\nonumber\\
& = 
\exp\Big(\frac{\log(n)}{k} ( a e^{-\beta}+b e^{\beta} -a-b )
+o(1) \Big)\nonumber \\
& = (1+o(1)) n^{g(\beta)-1} \label{eq:gbetaminus1}
\end{align}
Therefore the probability of event \eqref{eq:betastar} is bounded above by $ (k-1 + o(1)) n^{g(\beta)}$
where $g(\beta)  := \frac{b e^{\beta}+a e^{-\beta}}{k}-\frac{a+b}{k}+1$. This $g(\beta)$ is also defined in Theorem~\ref{thm:wt3}.
The critical value is the smaller zero point of $g(\beta)$, which is exactly
\begin{equation}\label{eq:beta_star}
\beta^* = \log\frac{a+b-k-\sqrt{(a+b-k)^2-4ab}}{2 b}
\end{equation}
also given in Theorem~\ref{thm:wt1}.
When $\beta > \beta^*$ and $\beta$ is smaller than the larger root, $g(\beta) < 0$ and the probability of \eqref{eq:betastar} decreases to
$0$ as $n\to \infty$.

We notice that when $\beta$ is large, $g(\beta) > 0$. Therefore, Equation \eqref{eq:gbetaminus1} is not satisfactory, and
we need finer control by introducing some extra parameter which we can optimize. To be more specific, let
the event $D_r : = \sum_{i=1}^n\exp ( \beta(A^r_i-A^0_i)) > s$
and we are going to give an upper bound of $P(D_r)$. We proceed as follows: 
\begin{align*}
&\Pr(D_r) = 
\Pr(D_r| A_i^r - A_i^0 \geq 0, \exists i\in [n])
\cdot I_1 \\
&+ \Pr(D_r | A_i^r - A_i^0  < 0, \forall i\in [n])
\Pr(  A_i^r - A_i^0  < 0 , \forall i \in [n] ) \\
& \leq I_1
+ \Pr(D_r | A_i^r - A_i^0  < 0, \forall i\in [n])
\end{align*}
where $I_1 = \Pr( A_i^r - A_i^0 \geq 0, \exists i\in [n])$.
To bound the two terms above, we need the following lemma, which can be proved by standard Chernoff inequality techniques:
\begin{lemma}\label{lem:fb}
	For $t\in [\frac{1}{k}(b-a), 0]$,
	define a function
	\begin{align}
	&f_{\beta}(t):=\frac{1}{k}\sqrt{k^2t^2+4ab} -\frac{a+b}{k} +1 +\beta t  \notag\\
	&-t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big). \label{eq:fbetat}
	\end{align}
	It follows that
	\begin{align} 
	& P(A^1_i-A^0_i\ge t\log(n))  \notag\\
	\le &  \exp\Big(\log n \Big(f_{\beta}(t) -\beta t  - 1 + O\big(\frac{\log(n)}{n}\big) \Big)\Big) .
	\end{align}
\end{lemma}
Choosing $t=0$ in Lemma \ref{lem:fb}, we have
$\Pr(A^1_i-A^0_i\ge 0 ) \leq \exp(-\log n \frac{(\sqrt{a}-\sqrt{b})^2}{k})$.
Then
\begin{align*}
I_1 \leq \sum_{i=1}^n \Pr( A_i^r - A_i^0 \geq 0) \leq n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}}
\end{align*}
For the second term,
conditioned on $A_i^r - A_i^0  < 0, \forall i\in [n]$ we have
\begin{align*}
&\sum_{i=1}^n\exp ( \beta(A^r_i-A^0_i)) \\
& = \sum_{t\log n =-\frac{n}{k}}^{-1}\sum_{i=1}^n \mathbbm{1}[A^r_i - A^0_i = t \log n] \exp ( \beta  t\log n)\leq \\ 
&
\sum_{t\log n =\tau}^{-1}\sum_{i=1}^n \mathbbm{1}[A^r_i - A^0_i = t \log n]\exp ( \beta  t\log n) + n^{1+\beta(b-a)/k}
\end{align*}
where $\tau =\frac{b-a}{k}\log n$ in short. Therefore
\begin{align*}
&\Pr(D_r | A_i^r - A_i^0  < 0, \forall i\in [n])  \\
&\leq\Pr(\sum_{t\log n =\tau}^{-1}\sum_{i=1}^n \mathbbm{1}[A^r_i - A^0_i = t\log n]\exp ( \beta  t\log n)  > \tilde{s} ) \\
& \leq \mathbb{E}[\sum_{t\log n =\tau}^{-1}\sum_{i=1}^n \mathbbm{1}[A^r_i - A^0_i= t\log n]\exp ( \beta  t\log n)] /  \tilde{s} \\
& \leq \frac{a-b}{k}\log n \cdot n^{f_{\beta}(t) + o(1)} / \tilde{s} \textrm{ using Lemma \ref{lem:fb} }
\end{align*}
where $\tilde{s} = s - n^{1+\beta(b-a)/k}$. 
The maximization of $f_{\beta}(t)$ is given by the following lemma:
\begin{lemma}\label{lem:tilde_g}
	Define
	\begin{equation}\label{eq:gbt}
	\tilde{g}(\beta) = \begin{cases}
	g(\beta)   & \text{~if~} \beta< \frac{1}{2}\log\frac{a}{b} \\
	g(\frac{1}{2} \log\frac{a}{b}) = 1 - \frac{(\sqrt{a}-\sqrt{b})^2}{k} & \text{~if~} \beta\ge \frac{1}{2}\log\frac{a}{b}
	\end{cases}
	\end{equation}
	then $f_{\beta}(t) \leq \tilde{g}(\beta)$ for $t\leq 0$.
\end{lemma}
By choosing $s = n^{\tilde{g}(\beta)/2}$ and using Lemma \ref{lem:tilde_g}, we have
\begin{align*}
\Pr( D_r) \leq  n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}} + O(\log n)  \cdot n^{\tilde{g}(\beta)/2 + o(1)} \leq 2n^{\tilde{g}(\beta)/4}
\end{align*}
Replacing $D_r$ by $\cup_{r=1}^{k-1} D_r$ in the above deduction we could get the same conclusion.
That is, the probability of event
$$
P_{\sigma | G}(\dist(\sigma, X) = 1) > n^{\tilde{g}(\beta)/2}P_{\sigma | G}(\sigma = X)\label{eq:betastar_xx}
$$
decreases faster than $2n^{\tilde{g}(\beta)/4}$.
Similar techniques shows that $P_{\sigma | G}(\dist(\sigma, X) = m)> n^{m\tilde{g}(\beta)/2}P_{\sigma | G}(\sigma = X)$
decreases faster than $2(k-1)^m n^{m\tilde{g}(\beta)/4}$. Therefore, $P_{\sigma | G}(\dist(\sigma, X) > 1) < n^{\tilde{g}(\beta)/2} = o(1)$ by summing geometric series.

\subsection{Proof for $\beta\le\beta^\ast$: Structural results and tight concentration}
\begin{equation} \label{eq:nn}
E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = r)}{P_{\sigma|G}(\sigma= X)} \Big]
\le \sum_{\cI\subseteq[n],|\cI|=r} 
n^{k (g(\beta)-1+o(1))}
= \binom{n}{r} (k-1)^r n^{r (g(\beta)-1+o(1))}
\end{equation}
In the last inequality of \eqref{eq:nn}, we use a coarse bound $\binom{n}{r}<n^r$. Now let us use a tighter bound $\binom{n}{r}<n^r/(r!)$. 
For $r>n^{\theta}$, we have
$
r!>(r/e)^r
=\exp(r\log(r)-r)
>\exp(r(\theta)\log(n)-r)
=n^{r(\theta-o(1))} .
$
Taking these into the last inequality of \eqref{eq:nn}, we obtain that for all $r>n^{\theta}$,
$$
E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = r )}{P_{\sigma|G}(\sigma= X)} \Big]
\le  (k-1)^r\binom{n}{k} n^{k (g(\beta)-1+o(1))}
< n^{r (g(\beta)+o(1))} /(r!) 
< (k-1)^rn^{-r(\delta-o(1))} .
$$
This immediately implies that  $P_{\SIBM} (\dist(\sigma, \Gamma)<n^{\theta} ) = 1- o(1)$ for any $\delta>0$. Since $g(\beta)<1$ for all $0<\beta\le\beta^\ast$, we have $P_{\SIBM} (\dist(\sigma, \Gamma)<n^{\theta} ) = 1- o(1)$ for all $\theta\in (g(\beta), 1)$. This improves upon the upper bound 
$\dist(\sigma, \Gamma)< n/\log^{\delta}(n)$ we obtained using spectral method at the beginning of this section.

More importantly, this allows us to prove a powerful structural result. (All the discussions below are conditioning on the event $\dist(\sigma,X)\le n/k$, i.e., $\sigma$ is closer to $X$ than to $\Gamma\backslash\{X\}$.) We say that $j$ is a ``bad" neighbor of vertex $i$ if the edge $\{i,j\}$ is connected in graph $G$ and $\sigma_j\neq X_j$. Then there is an integer $z>0$ such that with probability $1-o(1)$, every vertex has at most $z$ ``bad" neighbors.
Conditioning on the event every vertex has at most $z$ ``bad" neighbors, it is easy to show that $P_{\sigma|G}(\sigma_i \neq X_i)$ differs from $\sum_{r=1}^{k-1}\exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor $\exp(k^2\beta z)$.
Therefore, $E_{\sigma|G}[\dist(\sigma,X)]=\sum_{i=1}^n P_{\sigma|G}(\sigma_i \neq X_i)$ differs from $\sum_{r=1}^{k-1} \sum_{i=1}^n\exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor for almost all $G$.
We can further prove that $\dist(\sigma,X)$ concentrates around its expectation. Thus we conclude that $\dist(\sigma,X)$ differs from $\sum_{r=1}^{k-1} \sum_{i=1}^n\exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor for almost all $G$.
Quite surprisingly, when $\beta\le\beta^\ast$, we can prove a very tight concentration around the expectation: For almost all graph $G$, we have $\sum_{i=1}^n\exp (k \beta (A^r_i-A^0_i))=(1+o(1))n^{g(\beta)}$; see Proposition~\ref{prop:con} in Section~\ref{sect:struct} for a proof. Combining this with the above analysis, we conclude that $\dist(\sigma,X)=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ when $\beta\le\beta^\ast$. This completes the sketched proof of Theorem~\ref{thm:wt3}.
See Sections~\ref{sect:theta} and Sections~\ref{sect:struct} for the rigorous proof of the above arguments.

\subsection{Multiple sample case: Proof of Theorem~\ref{thm:wt2}}
\label{sect:multi}

\begin{center}
	\begin{minipage}{.55\textwidth}
		\begin{algorithm}[H]
			\caption{\texttt{LearnSIBM} in $O(n)$ time} \label{alg:ez}
			Inputs: the samples $\sigma^{(1)},\sigma^{(2)}\dots,\sigma^{(m)}$ \\
			Output: $\hat{X}$
			\begin{algorithmic}[1]
				\Statex \hspace*{-0.3in} 
				{\bf Step 1: Align all the samples with $\sigma^{(1)}$ }
				\For {$j=2,3,\dots,m$}
				\State $f=\arg\min_{f_{\gamma}} d(f_{\gamma}(\sigma^{(j)}), \sigma^{(1)})$
				\State $\sigma^{(j)} \gets f(\sigma^{(j)})$
				\EndFor
				\Statex \hspace*{-0.3in}
				{\bf Step 2: Majority vote at each coordinate}
				\For {i=1,2,\dots,n}
				\State $g(r) = |\{j | \sigma^{(j)}_i = \omega^r,1\leq j \leq m\}|$ for $ 0 \leq r \leq k-1$
				\State $\hat{X}_i \gets  \arg\max_r \omega^{g(r)}$
				\State \Comment{If the max of $g(r)$ is not unique, assign $\hat{X}_i$ randomly to one of \texttt{argmax}}
				\EndFor
				\State Output $\hat{X}$
			\end{algorithmic}
		\end{algorithm}
	\end{minipage}
\end{center}
For the multiple-sample case, we prove that the above simple algorithm can recover $X$ with probability $1-o(1)$ if and only if the Maximum Likelihood (ML) algorithm recovers $X$ with probability $1-o(1)$. We already showed that each sample is very close to $\Gamma(X)$, so after the alignment step in Algorithm~\ref{alg:ez}, all the samples are either simultaneously aligned with one of $\Gamma$. WLOG, we assume all samples are aligned with $X$.
By the structural results discussed above,
with probability $1-o(1)$, $P_{\sigma|G}(\sigma_i^{(j)} = \omega^r \cdot X_i)$ differs from
$\exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor for all $j\in[m]$. Since the samples are independent, we further obtain that $P_{\sigma|G}(\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i]\ge u)$ differs from $\sum_{r=1}^{k-1} \exp (k u \beta (A^r_i-A^0_i))$
by at most a constant factor.
Here $u\beta$ plays the role of $\beta$ in the single-sample case.
Therefore, if $u\beta>\beta^\ast$, then with probability $1-o(1)$
we have $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)}= \omega^r \cdot X_i] \le u-1$ for all $i\in[n]$.
Let $u=\lfloor \frac{m+k-1}{k} \rfloor$,
then we have $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} = \omega^r \cdot X_i] \le \lfloor \frac{m-1}{k} \rfloor $
while $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} = X_i]
= m - (k-1)\lfloor \frac{m-1}{k} \rfloor > \sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} = \omega^r \cdot X_i]$
which implies that $\hat{X}=X$ after the majority voting step in Algorithm~\ref{alg:ez}. See Section~\ref{sect:direct} for a rigorous proof of the above argument.

The proof of the converse results, i.e., even ML algorithm cannot recover $X$ with probability $1-o(1)$ when $\lfloor \frac{m+k-1}{k} \rfloor \beta < \beta^\ast$, also relies on the structural result and it is rather similar to the proof of $\beta\le\beta^\ast$ for the single-sample case. We refer the readers to Section~\ref{sect:converse} for details.

\section{Samples are concentrated around $S_k(X)$ or $\Lambda$} \label{sect:aln}
In this section, we show that for $a>b$, if $\alpha<b\beta$, then the samples differ from  $\Lambda$ in at most $n/\log^{\delta}(n)$ coordinates.
We prove that the number of samples needed for exact recovery of $X$ is at least $\Omega(\log^{\delta}(n))$. We also show that if $\alpha > b\beta$, then the samples differ from  $S_k(x)$ in at most $n/\log^{\delta}(n)$ coordinates.

\begin{proposition} \label{prop:1}
Let $a>b>0$ and $\alpha,\beta>0$ be constants. Let $m$ be a positive integer that is upper bounded by some polynomial of $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n, \alpha,\beta, m) .
$$
If $\alpha < b\beta$, then for any (arbitrarily large) $r>0$ and $0<\delta<1$, there exists $n_0(r)$ such that for all even integers $n>n_0(\delta, r)$,
$$
P_{\SIBM} \Big(\dist(\sigma^{(i)}, \Lambda)< n/\log^{\delta}(n)
\text{~~for all~} i\in[m] \Big) \ge 1- n^{-r} .
$$
If $\alpha > b\beta$, then for any (arbitrarily large) $r>0$ and $0<\delta<1$, there exists $n_0(r)$ such that for all even integers $n>n_0(\delta, r)$,
$$
P_{\SIBM} \Big(\dist(\sigma^{(i)}, S_k(X)) < n/\log^{\delta}(n)
\text{~~for all~} i\in[m] \Big) \ge 1- n^{-r} .
$$
\end{proposition}
\begin{proof}
For the case $\alpha < b \beta$, we only need to show $P_{\SIBM} \Big(\dist(\bar{\sigma}, \Lambda)\geq n/\log^{\delta}(n)
| \arg\,\min_{\sigma'\in \Lambda} \dist(\bar{\sigma}, \sigma') = \mathbf{1}_n
 \Big) \leq n^{-r}$.
 Let the event $M_1:= \exists \bar{\sigma}$ satisfying the condition of Lemma \ref{lem:small}
  $s.t. P_{\sigma | G}(\sigma = \bar{\sigma} ) > \exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n)$.
 Then by union bound, we have $P_{G}(M_1) \leq k^n\exp(-\tau(\alpha, \beta )n \log^{1-\delta} n )$ from Lemma \ref{lem:small}.
 
\begin{align*}
&P_{\SIBM} \Big(\dist(\bar{\sigma}, \Lambda)\geq n/\log^{\delta}(n)
| \arg\,\min_{\sigma'\in \Lambda} \dist(\bar{\sigma}, \sigma') = \mathbf{1}_n \Big) \\
&\leq
P_{\SIBM} \Big(\dist(\bar{\sigma}, \Lambda)\geq n/\log^{\delta}(n)
| \arg\,\min_{\sigma'\in \Lambda} \dist(\bar{\sigma}, \sigma')= \mathbf{1}_n, G \not\in M_1^c\Big) + P_G(M_1) \\
&\leq \sum_{G \in M_1^c} P(G) k^n \exp(-Cn)P_{\sigma|G}(\sigma=\mathbf{1}_n) + P_G(M_1) \\
& \leq k^n \exp(-Cn) (1 + o(1)) \leq n^{-r} \textrm{ for } C> \log k
\end{align*}
The case $\alpha > b \beta$ can be proved in the same way as above by applying Lemma \ref{lem:sigmaX}.
\end{proof}
\begin{lemma}\label{lem:small}
	$\alpha< b \beta$. When $\dist(\bar{\sigma}, \mathbf{1}_n) \geq \frac{n}{\log^{\delta} n}$ where $0<\delta < 1$ and $\arg\,\min_{\sigma'\in \Lambda} \dist(\bar{\sigma}, \sigma') = \mathbf{1}_n$. Show that
	$P_{\sigma | G}(\sigma = \bar{\sigma} ) > \exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n)$
	happens with probability less than $\exp(-\tau(\alpha,\beta) n \log^{1-\delta} n )$ where $C$ is an arbitrary constant, $\tau(\alpha,\beta)$ is a positive number.
\end{lemma}
\begin{proof}
	Let $n_r = |\{\bar{\sigma}_i = w^r | i\in [n] \}|$. Then $n_0 \geq n_r$ for $r=1, \dots, k-1$ since  $\arg\,\min_{\sigma'\in \Lambda} \dist(\bar{\sigma}, \sigma') = \mathbf{1}_n$.
	WLOG, suppose $n_0 \geq n_1 \dots \geq n_{k-1}$.
	We are concerned with the number $N_{\bar{\sigma}} = \frac{1}{2}(n(n-1) - \sum_{r=0}^{k-1} n_r(n_r-1))
	=\frac{1}{2}(n^2 - \sum_{r=0}^{k-1} n_r^2)$.
	Taking the $\log$ on both sides of $P_{\sigma | G}(\sigma = \bar{\sigma} ) > \exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n)$ we can get
	\begin{equation}\label{eq:small}
	(\beta + \frac{\alpha \log n}{n}) \sum_{\bar{\sigma}_i  \neq \bar{\sigma}_j} Z_{ij} \leq \frac{\alpha \log n}{n} N_{\bar{\sigma}} + C n
	\end{equation}
	
	Firstly we estimate the order of $N_{\bar{\sigma}}$, obviously $N_{\bar{\sigma}} \leq \frac{1}{2} n^2$.
	Using the conclusion in Appendix A of \cite{chen2016information} we have
	\begin{equation}
	\sum_{r=0}^{k-1} n_r^2 \leq
	\begin{cases}
	n n_0 & n_0 \leq \frac{n}{2} \\
	n^2 - 2n_0(n-n_0) & n_0 > \frac{n}{2}
	\end{cases}
	\end{equation}
	We have $n_0 \leq n - \frac{n}{\log^{\delta} n}$ and $n_0 \geq \frac{n}{k}$. When $n_0 > \frac{n}{2}$ we take $n_0 = n - \frac{n}{\log^{1/3} n}$
	and we have $N_{\bar{\sigma}} \geq n_0 (n - n_0) = \frac{n^2}{\log^{\delta} n}(1+o(1))$. When $n_0 < \frac{n}{2}$ we take $n_0 = \frac{n}{2}$ such that
	$N_{\bar{\sigma}} \geq \frac{n^2}{4}$. So generally we have $\frac{n^2}{\log^{\delta} n}(1+o(1)) \leq N_{\bar{\sigma}} \leq \frac{1}{2}n^2$.
	The Bernoulli random variables are independent, taking either $Bern(\frac{a\log n}{n})$ or $Bern(\frac{b \log n}{n})$ depending on whether
	$X_i$ equals to $X_j$.
	Since $\frac{\log n}{n} N_{\bar{\sigma}}$ is dominated than $Cn$ we neglect inferior terms to rewrite \eqref{eq:small} as
	\begin{equation}
	\sum_{ \bar{\sigma}_i  \neq \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_{\bar{\sigma}}}{n}(1+o(1))
	\end{equation}
	Let $N_1$ be the number of random variables of $Z_{ij} \sim Bern(\frac{a\log n}{n})$.
	and $N_2 = N_{\bar{\sigma}} - N_1$ is that of $Z_{ij} \sim Bern(\frac{b\log n}{n})$.
	
	Using Chernoff Inequality we have
	\begin{align*}
	\Pr(\sum_{ \bar{\sigma}_i  = \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_{\bar{\sigma}}}{n})& \leq (E[\exp(-s Z_{ij})])^{N_1} (E[-exp(-s Z_{ij})])^{N_2} \exp(\frac{\alpha}{\beta} \frac{\log n N_{\bar{\sigma}} s}{n}(1+o(1))) \\
	&= \exp( \frac{\log n}{n}(1+o(1))(\exp(-s)-1)(aN_1 + bN_2)+\frac{\alpha}{\beta} \frac{\log n N_{\bar{\sigma}} s}{n}(1+o(1)))
	\end{align*}
	Since $s > 0$, we further have
	\begin{align*}
	\Pr(\sum_{ \bar{\sigma}_i  = \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_{\bar{\sigma}}}{n})
	& \leq \exp( \frac{N_{\bar{\sigma}}\log n }{n}(b(\exp(-s)-1)+ \frac{\alpha}{\beta}s + o(1))) 
	\end{align*}
	Let $h_b(x) = x - b -x\log \frac{x}{b}$, which satisfies $h_b(x) < 0$ for $0<x<b$,
	and take $s=-\log\frac{\alpha}{b\beta} > 0$, using 
	$N_{\bar{\sigma}} \geq \frac{n^2}{\log^{1/3} n}$ we have
	\begin{align*}
	\Pr(\sum_{ \bar{\sigma}_i  = \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_{\bar{\sigma}}}{n})&\leq \exp( N_{\bar{\sigma}} \frac{\log n}{n} h_b(\frac{\alpha}{\beta})(1+o(1))) \\
	& \leq \exp (h_b(\frac{\alpha}{\beta}) n \log^{1-\delta} n (1+o(1)))
	\end{align*}
\end{proof}
\begin{lemma}\label{lem:minus}
	For SSBM$(n,k,p,q)$, suppose the ground truth label is $X$ while $\bar{\sigma}$ differs from $X$ in $|\cI|$ coordinate.
	Let $I_{ij} = |\{r\in [n] | X_r = w^i, \sigma_r = w^j \}$ for $i\neq j$ and $I_{ii} = 0$.
	Then
	\begin{equation}\label{eq:general_expansion}
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}{P_{\sigma|G}(\sigma=X)} = \exp((\beta + \frac{\alpha \log n}{n})[B_{\bar{\sigma}} - A_{\bar{\sigma}}] - \frac{\alpha \log n}{n} N_{\bar{\sigma}})
\end{equation}
	where 
	\begin{align}
	N_{\bar{\sigma}} &= \frac{1}{2}\sum_{i=0}^{k-1} (I_i - I_i')^2 \label{eq:N_w} \\
	B_{\bar{\sigma}} & \sim Binom(\frac{n}{k}|\cI| + \frac{1}{2}\sum_{i=0}^{k-1}  (-2 I'_i I_i  + I'^2_i - \sum_{j=0}^{k-1} I^2_{ji}) , q)\\
	A_{\bar{\sigma}} & \sim Binom(\frac{n}{k}|\cI| - \frac{1}{2}\sum_{i=0}^{k-1}  (I^2_i + \sum_{j=0}^{k-1} I^2_{ij}), p) \label{eq:A_w}
	\end{align}
	We write the row sum $I_i = \sum_{j=0}^{k-1} I_{ij}$ and column sum $I'_i = \sum_{j=0}^{k-1} I_{ji}$.
\end{lemma}
\begin{proof}
	$$ \log P_{\sigma|G}(\sigma=\bar{\sigma}) = (\beta + \frac{\alpha \log n}{n}) \sum_{\bar{\sigma}_i = \bar{\sigma}_j} Z_{ij}
	- \frac{\alpha \log n}{n} \sum_{\bar{\sigma}_i = \bar{\sigma}_j} 1  - \log Z_G(\alpha, \beta)
	$$
	where $Z_{ij}$ is Bernoulli random variable which takes value 1 if there is an edge between $i$ and $j$.
	
	For $\sigma = X$, we have
	\begin{align*}
	\sum_{X_i = X_j} Z_{ij} &= \sum_{X_i = X_j, \bar{\sigma}_i = \bar{\sigma}_j} Z_{ij} + \sum_{X_i = X_j, \bar{\sigma}_i \neq \bar{\sigma}_j} Z_{ij} \\
	\sum_{X_i = X_j} 1 &= \frac{1}{2} \sum_{i=0}^{k-1} \frac{n}{k} ( \frac{n}{k} - 1 )
	\end{align*}
	For $\sigma = \bar{\sigma}$, we have
	\begin{align*}
	\sum_{\bar{\sigma}_i = \bar{\sigma}_j} Z_{ij} &= \sum_{X_i = X_j, \bar{\sigma}_i = \bar{\sigma}_j} Z_{ij} + \sum_{X_i \neq X_j, \bar{\sigma}_i = \bar{\sigma}_j} Z_{ij} \\
	\sum_{\bar{\sigma}_i = \bar{\sigma}_j} 1 &= \frac{1}{2} \sum_{i=0}^{k-1} (\frac{n}{k} - I_i + I_i') ( \frac{n}{k} - 1 - I_i + I_i')
	\end{align*}
	Therefore, we get
	$N_{\bar{\sigma}} = \sum_{\bar{\sigma}_i = \bar{\sigma}_j} 1  -\sum_{X_i = X_j} 1 = \frac{1}{2}\sum_{i=0}^{k-1} (I_i - I_i')^2 $.
	On the other hand, for $Z_{ij}$ in the sum $\sum_{X_i \neq X_j, \bar{\sigma}_i = \bar{\sigma}_j} Z_{ij}$, $Z_{ij} \sim Bern(q)$ and
	$\sum_{X_i \neq X_j, \bar{\sigma}_i = \bar{\sigma}_j} 1 = \sum_{i=0}^{k-1}[(\frac{n}{k} - I_i) I_i' + \frac{1}{2} I'^2_i ]$;
	for $Z_{ij}$ in the sum $\sum_{X_i = X_j, \bar{\sigma}_i \neq \bar{\sigma}_j} Z_{ij}$, $Z_{ij} \sim Bern(p)$ and
	$\sum_{X_i = X_j, \bar{\sigma}_i \neq \bar{\sigma}_j} 1 = \sum_{i=0}^{k-1}[(\frac{n}{k} - I_i) I_i + \frac{1}{2} I^2_i ]$.
\end{proof}
\begin{lemma}\label{lem:sigmaX}
		When $\dist(\bar{\sigma}, X) \geq \frac{n}{\log^{\delta} n}$ where $0<\delta < 1$ and $\arg\,\min_{\sigma'\in S_k(X)} \dist(\bar{\sigma}, \sigma') = X$. Show that
	$P_{\sigma | G}(\sigma = \bar{\sigma} ) > \exp(-Cn) P_{\sigma | G}(\sigma = X)$
	happens with probability less than $\exp(-\tau(\alpha,\beta) n \log^{1-\delta} n )$ where $C$ is an arbitrary constant, $\tau(\alpha,\beta)$ is a positive number.
\end{lemma}
\begin{proof}
	Using Lemma \ref{lem:minus} we only need to consider the event:
	\begin{equation}\label{eq:BwA}
	(\beta + \frac{\alpha \log n}{n})[B_{\bar{\sigma}} - A_{\bar{\sigma}}] >  \frac{\alpha \log n}{n} N_{\bar{\sigma}}  - Cn
	\end{equation}
	We divide the cases for $\bar{\sigma}$ to two cases:
	\begin{enumerate}
		\item $\exists i\neq j$ s.t. $\frac{1}{k(k-1)}\frac{n}{\log^{\delta} n} \leq I_{ij} \leq \frac{n}{k} - \frac{1}{k(k-1)}\frac{n}{\log^{\delta} n}$
		\item $\exists i \neq j$ s.t. $I_{ij} > \frac{n}{k} - \frac{1}{k(k-1)}\frac{n}{\log^{\delta} n}$ and $I_{ji} < \frac{1}{k(k-1)}\frac{n}{\log^{\delta} n}$
	\end{enumerate}
	If we have $\sigma = \bar{\sigma}$ which belongs to neither of the above two cases, then from condition 1 we have
	$I_{ij} < \frac{1}{k(k-1)}\frac{n}{\log^{\delta} n}$ or $I_{ij} > \frac{n}{k} - \frac{1}{k(k-1)}\frac{n}{\log^{\delta} n}$.
	Since $\sum_{i,j} I_{ij} = |\cI| \geq \frac{n}{\log^{\delta} n}$ there should exist $I_{ij} > \frac{n}{k} - \frac{1}{k(k-1)}\frac{n}{\log^{\delta} n}$.
	Under such condition, if $I_{ji} > \frac{n}{k} - \frac{1}{k(k-1)}\frac{n}{\log^{\delta} n}$.
	Let $X'$ be the vector which exchanges value of $w^i$ with $w^j$ in $X$. We consider
	\begin{align*}
	\dist(\bar{\sigma}, X') - \dist(\bar{\sigma}, X) &= |\{ r \in [n]|X=w^i, \bar{\sigma}_r \neq w^j \}| + |\{ r \in [n]|X=w^j, \bar{\sigma}_r \neq w^i \}| \\
	&-|\{ r \in [n]|X=w^i, \bar{\sigma}_r \neq w^i \}| - |\{ r \in [n]|X=w^j, \bar{\sigma}_r \neq w^j \}| \\
	& = \frac{n}{k} - I_{ij} +  \frac{n}{k} - I_{ji} - I_i - I_j \\
	& < \frac{2}{k(k-1)}\frac{n}{\log^{\delta} n} - I_i - I_j < 0
	\end{align*}
	which contracts with that $\bar{\sigma}$ is nearest to $X$.
	Therefore, we should have $I_{ji} < \frac{1}{k(k-1)}\frac{n}{\log^{\delta} n}$.
	Now the $(i, j)$ pair satisfies condition 2, which contracts with $\bar{\sigma}$ belongs to neither of the two conditions.
	
	Under condition 1, we can get a lower bound on $|A_{\bar{\sigma}}|$ from \eqref{eq:A_w}. Let $I'_{ij} = I_{ij}$ for $i\neq j$ and
	$I'_{ii} = \frac{n}{k} - I_i$. Then we can simply $|A_{\bar{\sigma}}|$ as:
	\begin{align*}
	|A_{\bar{\sigma}}| &= \frac{n}{k}|\cI| - \frac{1}{2}\sum_{i=0}^{k-1}  (I^2_i + \sum_{j=0}^{k-1} I^2_{ij}) \\
	&= \frac{n^2}{2k} - \frac{1}{2} \sum_{i=0}^{k-1} \sum_{j=0}^{k-1} I'^2_{ij}
	\end{align*}
	We further have $\sum_{i=0}^{k-1} \sum_{j=0}^{k-1} I'^2_{ij} \leq (k-1)\frac{n^2}{k^2} + (\frac{n}{k} - I_{ij})^2 + I^2_{ij}$ where
	$I_{ij}$ satisfies condition 1. Therefore, $\sum_{i=0}^{k-1} \sum_{j=0}^{k-1} I'^2_{ij} \leq (k-1)\frac{n^2}{k^2} + (\frac{1}{k(k-1)}\frac{n}{\log^{\delta} n})^2
	+ (\frac{n}{k} - \frac{1}{k(k-1)}\frac{n}{\log^{\delta} n})^2 = \frac{n^2}{k} - \frac{2n^2}{k^2 (k-1)\log^\delta n}(1+o(1))$.
	As a result, $A_{\bar{\sigma}} \geq \frac{2n^2}{k^2 (k-1)\log^\delta n}(1+o(1))$.
	
	Under condition 2, we can get a lower bound on $N_{\bar{\sigma}}$ from \eqref{eq:N_w}: $N_{\bar{\sigma}} \geq \frac{1}{2}(\frac{n}{k} - \frac{2}{k(k-1)}\frac{n}{\log^{\delta} n})^2 = \frac{n^2}{k^2}(1+o(1))$.
	
	Now we use Chernoff bound to process inequality \eqref{eq:BwA}, we can omit $\frac{\alpha \log n}{n}$ on the left hand since it is far smaller than $\beta$:
	\begin{align*}
	&\Pr([B_{\bar{\sigma}} - A_{\bar{\sigma}}] >  \frac{\alpha \log n}{\beta n} N_{\bar{\sigma}}  - \frac{C}{\beta}n)
	\leq (E[\exp(sZ_{ij})])^{|B_{\bar{\sigma}}|}(E[\exp(-sZ_{ij})])^{|A_{\bar{\sigma}}|} \exp(-s(\frac{\alpha \log n}{\beta n} N_{\bar{\sigma}}  - \frac{C}{\beta}n)) \\
	& =\exp(|B_{\bar{\sigma}}|(\frac{b\log n}{n})(e^s -1)) \exp(|A_{\bar{\sigma}}|)[\frac{a\log n}{n}] (e^{-s} - 1))\exp(-s(\frac{\alpha \log n}{\beta n} N_{\bar{\sigma}}  - \frac{C}{\beta}n)) \\
	\end{align*}
	Using $|B_{\bar{\sigma}}| = N_{\bar{\sigma}} + |A_{\bar{\sigma}}|$ we can further simplify the exponential term as
	$$
	\frac{\log n}{n} [|A_{\bar{\sigma}}|(b(e^s -1)+ a(e^{-s} - 1)) +
	N_{\bar{\sigma}} (b(e^s - 1)-s\frac{\alpha}{\beta})]  + s \frac{C}{\beta}n
	$$
	Now we investigate the function $g_1(s) = b(e^s -1)+ a(e^{-s} - 1)$ and $g_2(s) = b(e^s - 1)-s\frac{\alpha}{\beta}$,
	both takes zero values at $0$ and $g_1'(s) = (be^s - ae^{-s}), g_2'(s) = be^s -\frac{\alpha}{\beta}$.
	Therefore $g_1'(0) = b-a<0, g_2'(0) = b - \frac{\alpha}{\beta} < 0$ and we can choose $0<s^*$ such that $g_1(s^*) < 0,g_2(s^*) < 0$.
	To compensate the influence of the term $sCn/\beta$ we only need to make sure that the order of $\frac{\log n}{n} \min\{|A_{\bar{\sigma}}|, N_{\bar{\sigma}}\}$ is larger than $n$.
	This corresponds to the two conditions we have used.
\end{proof}

\begin{proposition}  \label{prop:ab}
Let $a>b>0$ and $\alpha,\beta>0$ be constants. Let $m$ be a positive integer that is upper bounded by some polynomial of $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
$$
If $\alpha<b\beta$, then it is not possible to recover $X$ from the samples when $m=O(\log^{\delta}(n))$ for $0 < \delta < 1$.
\end{proposition}

\begin{proof}
First observe that there are $\frac{n!}{(n/k)^k}$ balanced partitions, so one needs at least $\log_k \frac{n!}{(n/k)^k}=\Theta(n)$ bits to recover $X$.
By Proposition~\ref{prop:1}, with probability $1-o(n^{-4})$, $\dist(\sigma^{(i)}, \Theta)< n/\log^{\delta}(n)$
for all $i\in[m]$. Therefore, each $\sigma^{(i)}$ takes at most
$$
T:=\sum_{j=0}^{n/\log^{\delta}(n)} \binom{n}{j}(k-1)^j
$$
values, so each $\sigma^{(i)}$ contains at most $\log_k T$ bits of information. Next we prove that $\log_k T=O(\frac{\log\log(n)}{\log^{\delta}(n)} n)$, so we need at least $\Omega(\frac{\log^{\delta}(n)}{\log\log(n)})$ samples to recover $X$, which proves the proposition.

In order to upper bound $T$, we define a binomial random variable $Y\sim\Binom(n,\frac{k-1}{k})$. Then
$$
T=k^n P(Y\le n/\log^{\delta}(n))
= k^n P(-Y\ge -n/\log^{\delta}(n)).
$$
The moment generating function of $-Y$ is $(\frac{1}{k}+\frac{k-1}{k}e^{-s})^n$. By Chernoff bound, for any $s>0$,
$$
P(-Y\ge - n/\log^{\delta}(n)) \le
(\frac{1}{k}+\frac{k-1}{k}e^{-s})^n
e^{ksn/\log^{\delta}(n)}
= k^{-n} (1+(k-1)e^{-s})^n e^{ksn/\log^{\delta}(n)} .
$$
As a consequence, for any $s>0$,
$$
\log_k T\le n\Big(\log_k(1+(k-1)e^{-s})
+\frac{ks}{\log^{\delta}(n)}\log_k e \Big) .
$$
Taking $s=\log\log(n)$ into this bound, we obtain that $\log_k T=O(\frac{\log\log(n)}{\log^{\delta}(n)} n)$.
\end{proof}

\section{$\sigma \in S_k(X)$ with probability $1-o(1)$ when $\beta>\beta^\ast$} \label{sect:equal}

Recall the definition of $\beta^\ast$ in \eqref{eq:defstar}.
\begin{proposition} \label{prop:tt}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\beta>\beta^\ast$ and $\alpha>b\beta$. 
Let 
$
(X,G,\sigma) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, 1) .
$
Then
$$
P_{\SIBM}(\sigma \in \Gamma)=1-o(1) .
$$
\end{proposition}

We have proved in Proposition~\ref{prop:1} that if $\alpha>b\beta$, then $\dist(\sigma \in \Gamma) \le n/\log^{\delta}(n)$
 with probability $1-o(1)$.
Then Proposition~\ref{prop:1} tells us that
$$
\sum_{\cI\subseteq[n],~
n/\log^{\delta}(n)<|\cI|<n-n/\log^{\delta}(n)} P_{\SIBM}(\sigma=X^{(\sim\cI)})  = o(1) .
$$
By definition, $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$. Therefore, 
$$
\sum_{\cI\subseteq[n],1\le |\cI|\le n/\log^{\delta}(n)} P_{\SIBM}(\sigma=f(X^{(\sim\cI)})) =\sum_{\cI\subseteq[n],1\le |\cI|\le n/\log^{\delta}(n)} P_{\SIBM}(\sigma=X^{(\sim\cI)}).
$$
As a consequence, to prove Proposition~\ref{prop:tt}, we only need to show that
$$
\sum_{\cI\subseteq[n],1\le |\cI|\le n/\log^{\delta}(n)} P_{\SIBM}(\sigma=X^{(\sim\cI)}) 
= o(1) .
$$
This is further equivalent to proving that there exists a set $\cG$ such that

\noindent (i)
$P(G\in\cG)=1-o(1)$, where the probability is calculated according to the $\SSBM(n, k,a\log(n)/n, \linebreak[4] b\log(n)/n)$.

\noindent (ii)
For every $G\in\cG$,
$$
\sum_{\cI\subseteq[n],1\le |\cI|\le n/\log^{\delta}(n)}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} =o(1) .
$$

The existence of $\cG$ is guaranteed by the following proposition:
\begin{proposition} \label{prop:big}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\beta>\beta^\ast$ and $\alpha>b\beta$. 
Let 
$
(X,G,\sigma) \sim \SIBM(n,k,a\log(n)/n, b\log(n)/n,\alpha,\beta, 1) .
$
There is an integer $n_0$ such that for every even integer $n>n_0$ and  every integer $1\le r \le n/\log^{\delta}(n)$,
there is a set $\cG^{(r)}$ for which

\noindent (i)
$P(G\in\cG^{(r)}) \ge 1- 2(k-1)^r n^{r\tilde{g}(\beta)/4}$ ,

\noindent (ii) For every $G\in\cG^{(r)}$,
$$
\frac{P_{\sigma|G}(\dist(\sigma, X)=r)}
{P_{\sigma|G}(\sigma=X)} <
n^{r \tilde{g}(\beta) /2} .
$$
\end{proposition}
With the $\cG^{(r)}$'s given by Proposition~\ref{prop:big}, we
define 
$$
\cG:=\bigcap_{r=1}^{n/\log^{\delta}(n)} \cG^{(r)} .
$$
By the union bound,
$$
P(G\in\cG)\ge 1-2 \sum_{r=1}^{n/\log^{\delta}(n)} (k-1)^rn^{r\tilde{g}(\beta)/4}
> 1- \frac{2 (k-1)n^{\tilde{g}(\beta)/4}}{1-(k-1)n^{\tilde{g}(\beta)/4}}
=1-o(1),
$$
where the last equality follows from $\tilde{g}(\beta)<0$. Moreover, for every $G\in\cG$,
\begin{align*}
\sum_{r=1}^{n/\log^{\delta}(n)}
\frac{P_{\sigma|G}(\dist(\sigma, X) = r )}
{P_{\sigma|G}(\sigma=X)}
< \sum_{r=1}^{n/\log^{\delta}(n)}
n^{r \tilde{g}(\beta) /2}
< \frac{n^{\tilde{g}(\beta)/2}}{1-n^{\tilde{g}(\beta)/2}} =o(1) .
\end{align*}
Thus we have shown that Proposition~\ref{prop:tt} is implied by Proposition~\ref{prop:big}. In the rest of this section, we will prove the latter proposition.
\begin{proof}
Using \eqref{eq:general_expansion}, we write
$$
\frac{P_{\sigma|G}(\dist(\sigma, X)=r)}
{P_{\sigma|G}(\sigma=X)} = \sum_{\dist(\bar{\sigma}, X)=r} 
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}
{P_{\sigma|G}(\sigma=X)}=
\sum_{\dist(\bar{\sigma}, X)=r} \exp((\beta + \frac{\alpha \log n}{n})[B_{\bar{\sigma}} - A_{\bar{\sigma}}] - \frac{\alpha \log n}{n} N_{\bar{\sigma}})
$$
Let $\bar{W} = \{w, \dots, w^{k-1}\}$, $\Xi(v) = \{ \sigma | \dist(\sigma, X) = dim(v),  \sigma_{i_j} = v_j \cdot X_{i_j}, j \in [r] \}$, and we consider the event
$$
D(v, s): \sum_{\bar{\sigma} \in \Xi(v)} \exp((\beta + \frac{\alpha \log n}{n}) [B_{\bar{\sigma}} - A_{\bar{\sigma}}]) > s
$$
where $v \in \bar{W}^r$ and $|\Xi(v)|=\binom{n}{r}$.

Following the main idea in Section \ref{sect:why}, we consider an enhanced conclusion of Lemma \ref{lem:fb}.
\begin{lemma}\label{lem:enhanced_fb}
	For $t\in [\frac{1}{k}(b-a), 0]$
	and $ |\cI| \le n/\log^{\delta}(n)$
	\begin{equation} \label{eq:upmpt}
	\begin{aligned}
	& P(B_{\bar{\sigma}}-A_{\bar{\sigma}}\ge t |\cI| \log(n))  \\
	\le & \exp\Big(I\log(n)
	\Big(f_{\beta}(t) - \beta t -1	+ O(\log^{-\delta}(n)) \Big)\Big) .
	\end{aligned}
	\end{equation}
\end{lemma}
\begin{proof}
	By definition, $A_{\bar{\sigma}}\sim\Binom(|A_{\bar{\sigma}}|,\frac{a\log(n)}{n})$ and
	$B_{\bar{\sigma}}\sim\Binom(|B_{\bar{\sigma}}|,\frac{b\log(n)}{n})$, and they are independent. For $s>0$, the moment generating function of $B_{\bar{\sigma}}-A_{\bar{\sigma}}$ for $ |\cI| \le n/\log^{\delta}(n)$ can be bounded from above as follows:
	\begin{align*}
	 E[e^{s(B_{\bar{\sigma}}-A_{\bar{\sigma}})}] 
	& =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^s \Big)^{|B_{\bar{\sigma}}|}
	\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-s} \Big)^{|A_{\bar{\sigma}}|}  \\
	& = \exp(\frac{\log n}{n}(|B_{\bar{\sigma}}|(be^s - b) + |A_{\bar{\sigma}}|(ae^{-s}-a) + |B_{\bar{\sigma}}|O(\frac{\log n}{n}) ))
	\end{align*}
	Using the conclusion that $|B_{\bar{\sigma}}|=\frac{n}{k}|\cI| + \frac{1}{2}\sum_{i=0}^{k-1}  (-2 I'_i I_i  + I'^2_i - \sum_{j=0}^{k-1} I^2_{ji})$ and
	$|A_{\bar{\sigma}}| = \frac{n}{k}|\cI| - \frac{1}{2}\sum_{i=0}^{k-1}  (I^2_i + \sum_{j=0}^{k-1} I^2_{ij})$.
	By estimating the order of residual term, $|A_{\bar{\sigma}}| = \frac{n}{k}|\cI| + O(I^2)$,
	 $|B_{\bar{\sigma}}| = \frac{n}{k}|\cI| + O(I^2)$.
	 
	\begin{align*}
	E[e^{s(B_{\bar{\sigma}}-A_{\bar{\sigma}})}] & \le
	\exp(\frac{\log n}{n}((\frac{n}{k}|\cI| + O(|\cI|^2))(be^s - b) +(\frac{n}{k} |\cI| + O(|\cI|^2))(ae^{-s}-a) + \frac{n}{k}|\cI|O(\frac{\log n}{n}) ))
	 \\
	& =
	\exp\Big(\frac{|\cI| \log(n)}{k}(a e^{-s}+b e^s-a-b +
	O(\log^{-\delta}(n))) \Big),
	\end{align*}
	The last equality follows from the assumption that $ |\cI| \le n/\log^{\delta}(n)$.
	By Chernoff bound, for $s>0$, we have
	\begin{align*} 
	& P(B_{\bar{\sigma}}-A_{\bar{\sigma}}\ge t |\cI| \log(n))\le
	\frac{E[e^{s(B_{\bar{\sigma}}-A_{\bar{\sigma}})}]}{e^{st \log(n)}}  \\
	\le & \exp\Big(\frac{|\cI|\log(n)}{k} \big(a e^{-s}+b e^s -kst -a-b
	+ O(\log^{-\delta}(n)) \big)\Big)  .
	\end{align*}
	The rest of the proof is to find $s^\ast$ to minimize $a e^{-s}+b e^s -kst$ and take $s^\ast$ into the above bound. 
	\end{proof}
\begin{align*}
&\Pr(D(v,s)) = 
\Pr(D(v,s)| B_{\bar{\sigma}} - A_{\bar{\sigma}}   \geq 0, \exists \bar{\sigma} \in \Xi(v))
\cdot I_1 \\
&+ \Pr(D(v,s) , B_{\bar{\sigma}} - A_{\bar{\sigma}}  < 0, \forall   \bar{\sigma} \in \Xi(v))
 \\
& \leq I_1
+ \Pr(D(v,s), B_{\bar{\sigma}} - A_{\bar{\sigma}}    < 0, \forall \bar{\sigma} \in \Xi(v))
\end{align*}
where $I_1 = \Pr( B_{\bar{\sigma}} - A_{\bar{\sigma}}  \geq 0, \exists   \bar{\sigma} \in \Xi(v) )$.

We can bound $I_1$ by choosing $t=0$ in Lemma \ref{lem:enhanced_fb}, we have
$\Pr( B_{\bar{\sigma}} - A_{\bar{\sigma}} \ge 0 ) \leq \exp(-r\log n \frac{(\sqrt{a}-\sqrt{b})^2}{k})$.
Then
\begin{align*}
I_1 \leq \sum_{\bar{\sigma} \in \Xi(v)} \Pr( B_{\bar{\sigma}} - A_{\bar{\sigma}} \geq 0) \leq \binom{n}{r} n^{-r\frac{(\sqrt{a}-\sqrt{b})^2}{k}}
\leq n^{r(1-\frac{(\sqrt{a}-\sqrt{b})^2}{k})}
\end{align*}
For the second term,
conditioned on $B_{\bar{\sigma}} - A_{\bar{\sigma}}    < 0, \forall \bar{\sigma} \in \Xi(v)$ we have
\begin{align*}
&\sum_{\bar{\sigma} \in \Xi(v)} \exp ( (\beta + \alpha \frac{\log n}{n})(B_{\bar{\sigma}} - A_{\bar{\sigma}} ) ) \\
& = \sum_{tr\log n = -A_{\bar{\sigma}} }^{-1} \sum_{\bar{\sigma} \in \Xi(v)} \mathbbm{1}[B_{\bar{\sigma}} - A_{\bar{\sigma}}  = tr \log n] \exp ( \beta  tr\log n)\leq \\ 
&
\sum_{tr\log n =\tau}^{-1} \sum_{\bar{\sigma} \in \Xi(v)}  \mathbbm{1}[B_{\bar{\sigma}} - A_{\bar{\sigma}}  = tr \log n]
\exp ( \beta  tr\log n) + n^{r+r\beta(b-a)/k}
\end{align*}
where $\tau =\frac{b-a}{k}r\log n$ in short. Therefore,
\begin{align*}
&\Pr(D(v,s), B_{\bar{\sigma}} - A_{\bar{\sigma}}  < 0, \forall \bar{\sigma} \in \Xi(v))  \\
&\leq\Pr(\sum_{t\log n =\tau}^{-1}\sum_{\bar{\sigma} \in \Xi(v)} \mathbbm{1}[B_{\bar{\sigma}} - A_{\bar{\sigma}} = tr\log n]\exp ( \beta  t\log n)  > \tilde{s} ) \\
& \leq \mathbb{E}[\sum_{t\log n =\tau}^{-1}\sum_{\bar{\sigma} \in \Xi(v)} \mathbbm{1}[B_{\bar{\sigma}} - A_{\bar{\sigma}} = tr\log n]\exp ( \beta  t\log n)] /  \tilde{s} \\
& \leq \frac{a-b}{k}r\log n \cdot n^{rf_{\beta}(t) + o(1)} / \tilde{s} \textrm{ using Lemma \ref{lem:fb} }
\end{align*}
where $\tilde{s} = s - n^{r+r\beta(b-a)/k}$. 
Choosing $s = O(n^{r\tilde{g}(\beta)/2})$ and using the following conclusion, which is an ehanced version of Lemma
\ref{lem:tilde_g}.
\begin{lemma}[Elementary properties of function $g(\beta), \tilde{g}(\beta), f_{\beta}(t)$] \label{lm:ele}
Let $g(\beta)$ be defined in \eqref{eq:gbeta}, $\tilde{g}(\beta)$ be defined in \eqref{eq:gbt} and $f_{\beta}(t)$ be defined in \eqref{eq:fbetat}. Assume that $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
Then,

\begin{enumerate}[label=(\roman*)]
\item The equation $g(\beta) = 0$ has two roots, and the smaller one of them is $\beta^\ast$, defined in \eqref{eq:beta_star}.

\item Denote the other root as $\beta'$. Then
$\beta^\ast< \frac{1}{2}\log\frac{a}{b} <\beta'$.

\item $g(\beta)<0$ for all $\beta^\ast< \beta \le \frac{1}{2}\log\frac{a}{b}$.

\item $\tilde{g}(\beta)<0$ for all $\beta>\beta^\ast$.

\item $\tilde{g}(\beta)$ is a decreasing function in $[0,+\infty)$. 

\item $\tilde{g}(\beta)<1$ for all $\beta>0$.

\item $f_{\beta}(t)\le \tilde{g}(\beta)$ for all $t\le 0$.

\item If $\beta>\beta^\ast$, $f_{\beta}(t)\le \tilde{g}(\beta)<0$ for all $t\le 0$.

\end{enumerate}
\end{lemma}

\begin{align*}
\Pr( D_r) \leq  n^{r-r\frac{(\sqrt{a}-\sqrt{b})^2}{k}} + O(\log n)  \cdot n^{r\tilde{g}(\beta)/2 + o(1)} \leq 2n^{r\tilde{g}(\beta)/4}
\end{align*}
Therefore, by union bound
\begin{align*}
&P(\sum_{\dist(\bar{\sigma}, X)=r} \exp((\beta + \frac{\alpha \log n}{n})[B_{\bar{\sigma}} - A_{\bar{\sigma}}] - \frac{\alpha \log n}{n} N_{\bar{\sigma}}) > n^{r\tilde{g}(\beta)/2}) \\
&\leq P(\sum_{\dist(\bar{\sigma}, X)=r} \exp((\beta + \frac{\alpha \log n}{n})[B_{\bar{\sigma}} - A_{\bar{\sigma}}]) > n^{r\tilde{g}(\beta)/2}) \\
&\leq \sum_{v \in \bar{W}^r} P(D(v, \frac{1}{(k-1)^r} n^{r\tilde{g}(\beta)/2})) \\
& \leq 2(k-1)^r n^{r\tilde{g}(\beta)/4}
\end{align*}
\end{proof}

\section{Samples differ from $S_k(X)$ in $O(n^{\theta})$ coordinates for some $\theta<1$ when $\beta\le\beta^\ast$}
\label{sect:theta}



\begin{proposition}[Refinement of Proposition~\ref{prop:1}] \label{prop:43}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\alpha>b\beta$ and $\beta\le\beta^\ast$. Let $m$ be a constant integer that is independent of $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
$$
Then for any $g(\beta) < \theta < 1$ and any (arbitrarily large) $r>0$, there exists $n_0(\theta, r)$ such that for all even integers $n>n_0(\theta, r)$,
$$
P_{\SIBM} \Big(\dist(\sigma^{(i)},S_k(X))<n^{\theta}
\text{~for all~} i\in[m] \Big) \ge 1- n^{-r} .
$$
\end{proposition}

By Lemma~\ref{lm:ele} (vi), we know that $g(\beta)<1$ for all $0<\beta\le\beta^\ast$, so we can always choose a $\theta$ such that $g(\beta)<\theta<1$.
Then Proposition~\ref{prop:43} implies that when $\beta\le\beta^\ast$,
with probability $1-o(1)$ all samples differ from $S_k(X)$ in $O(n^\theta)$ coordinates for some $\theta<1$.


Since we assume that $m$ is a constant that is independent of $n$,
we only need to prove Proposition~\ref{prop:43} for the special case of $m=1$, and the case of general values of $m$ follows immediately from this special case.
Proposition~\ref{prop:1} tells us that if $\alpha>b\beta$, then $\dist(\sigma,S_k(X)) \le n/\log^{\delta}(n)$
with probability $1-n^{-r}$ for any given $r>0$ and large enough $n$.
To prove Proposition~\ref{prop:43}, we only need to show that given $r>0$, there exists a set $\cG_{\delta}$ such that the following two conditions hold for large enough $n$:  (i)
$P(G\in\cG_{\theta})\ge 1-n^{-r}$, and (ii)
For every $G\in\cG_{\theta}$,
$$
\sum_{s=n^{\theta}}^{n/\log^{\delta}(n)}
\frac{P_{\sigma|G}(\dist(\sigma, X)=s)}{P_{\sigma|G}(\sigma=X)} \le n^{-r} .
$$
The existence of $\cG_{\theta}$ is guaranteed by the following proposition:
\begin{proposition} \label{prop:xz}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$.
Let 
$$
(X,G,\sigma) \sim \SIBM(n, k, a\log(n)/n, b\log(n)/n,\alpha,\beta, 1) .
$$
For any $g(\beta)<\theta<1$, there exists $n_0(\theta)$ such that
for every even integer $n>n_0(\theta)$ and
every integer $n^{\theta} \le s \le n/\log^{\delta}(n)$,
there is a set $\cG_{\theta}^{(s)}$ for which

\noindent (i)
$P(G\in\cG_\theta^{(s)}) \ge 1- 2(k-1)^s n^{-s\delta'}$,
where $\delta':=\min(\frac{\delta}{8},\frac{(\sqrt{a}-\sqrt{b})^2}{2k} - \frac{1}{2}) >0$.

\noindent (ii) For every $G\in\cG_\theta^{(s)}$,
$$
\frac{P_{\sigma|G}(\dist(\sigma,X) = s )}
{P_{\sigma|G}(\sigma=X)} <
n^{-s \delta /4} .
$$
\end{proposition}
With the $\cG_\theta^{(s)}$'s given by Proposition~\ref{prop:xz}, we
define 
$$
\cG_\theta:=\bigcap_{s=n^{\theta}}^{n/\log^{\delta}(n)} \cG_\theta^{(s)} .
$$
By the union bound,
$$
P(G\in\cG_\theta)\ge 1-2 \sum_{s=n^{\theta}}^{n/\log^{\delta}(n)} (k-1)^sn^{-s\delta'}
> 1-2 \sum_{s=\lceil 4r/\delta' \rceil}^{+\infty} (k-1)^sn^{-s\delta'}
\ge 1- (k-1)^{\lceil 4r/\delta' \rceil}\frac{2 n^{-4r}}{1-n^{-\delta'}}
>1-n^{-2r}
$$
for large enough $n$.
 Moreover, for every $G\in\cG_\theta$ and large enough $n$,
\begin{align*}
& \sum_{\cI\subseteq[n],~~
n^{\theta}\le |\cI|\le n/\log^{\delta}(n)}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} =
\sum_{s=n^{\theta}}^{2n/\log^{1/3}(n)}
\hspace*{0.05in}
\sum_{\cI\subseteq[n],|\cI|=s}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)}  \\
& < \sum_{s=n^{\theta}}^{2n/\log^{1/3}(n)}
n^{-s \delta /4}
< \sum_{s=\lceil 16r/\delta \rceil}^{+\infty}
n^{-s \delta /4}
\le \frac{n^{-4r}}{1-n^{-\delta /4}} < n^{-2r} .
\end{align*}
Thus we have shown that Proposition~\ref{prop:43} is implied by Proposition~\ref{prop:xz}.
\begin{proof}
It turns out that all we need for the proof of Proposition~\ref{prop:xz} is a tighter inequality than \eqref{eq:wuh}.
Recall that we obtain \eqref{eq:wuh} from \eqref{eq:3l} by using a coarse upper bound $\binom{n}{s}<n^s$.
Here we use a tighter upper bound $\binom{n}{s}<n^s/(s!)$ in \eqref{eq:3l} and obtain that for
$G\in\cG_1^{(s)}\cap\cG^{(s)}(\epsilon)$,
(see the definitions of $\cG_1^{(s)}$ and $\cG^{(s)}(\epsilon)$
in \eqref{eq:g1k} and \eqref{eq:gep})
\begin{equation} \label{eq:zm}
\begin{aligned}
& \sum_{\cI\subseteq[n],|\cI|=s}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} 
< (k-1)^s(s!)^{-1} n^{sf_{\beta}((b-a)/k)} +
(k-1)^s\sum_{ts\log(n)=\lceil\frac{b-a}{k}s\log(n) \rceil}^{-1}
(r!)^{-1} n^{r(f_{\beta}(t)+\epsilon)} \\
& \le (k-1)^s(s!)^{-1} n^{sg(\beta)}
+ (k-1)^s\sum_{ts\log(n)=\lceil\frac{b-a}{k}s\log(n) \rceil}^{-1}  (s!)^{-1} n^{s(g(\beta) + \epsilon)}  \\
& \le (s!)^{-1} (k-1)^s n^{s(g(\beta) + \epsilon)}
\Big(\frac{a-b}{k}\log(n^s)+1 \Big) \\
& < (s!)^{-1} n^{s(g(\beta) + 2\epsilon)}  ,
\end{aligned}
\end{equation}
where the second inequality holds because
$f_{\beta}(t)\le \tilde{g}(\beta) \le g(\beta)$ for all $t\le 0$
(see  Lemma~\ref{lm:tus}), and the last inequality holds for positive $\epsilon$ and large $n$.
It is well known that\footnote{We know from the Taylor expansion that $e^x>x^k/(k!)$ for any $x>0$. Taking $x=k$ gives us $k!>(k/e)^k$.}
$s!>(s/e)^s$
for all positive integer $s$.
Therefore, for $s>n^{\theta}$, we have
$$
s!>(s/e)^s
=\exp(s\log(s)-s)
>\exp(s(\theta)\log(n)-s)
=n^{s(\theta-o(1))}
$$
Taking this into \eqref{eq:zm}, we obtain that $G\in\cG_1^{(s)}\cap\cG^{(s)}(\epsilon)$,
$$
\sum_{\cI\subseteq[n],|\cI|=s}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} 
< n^{s(2\epsilon-\delta+o(1))}
< n^{s(3\epsilon-\delta)}
$$
for positive $\epsilon$ and large $n$.
Let $\epsilon=\delta/4$ and define 
$$
\cG_{\delta}^{(s)}
:=\cG_1^{(s)}\cap\cG^{(s)}(\delta/4) .
$$
Then for $G\in\cG_{\delta}^{(s)}$ we have
$$
\sum_{\cI\subseteq[n],|\cI|=s}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} <
n^{-s \delta /4} .
$$
By \eqref{eq:Dk}, \eqref{eq:g1k} and \eqref{eq:Gk}, 
\begin{align*}
P(G\in\cG_{\delta}^{(k)})
& \ge 1-(k-1)^s n^{-s\delta/8}- (k-1)^s n^{s (1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) )} \\
& \ge 1-(k-1)^s n^{-s\delta/8}- (k-1)^sn^{s(\frac{1}{2}-\frac{(\sqrt{a}-\sqrt{b})^k}{2k} )}
> 1- 2 (k-1)^s n^{-s\delta'},
\end{align*}
where the second inequality follows from  $1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}< 0$, and the last inequality follows from the definition $\delta'=\min(\frac{\delta}{8},\frac{(\sqrt{a}-\sqrt{b})^2}{2k} - \frac{1}{2})$.
\end{proof}

\section{Samples differ from $\Gamma$ in $\Theta(n^{g(\beta)})$ coordinates when $\beta\le\beta^\ast$}  \label{sect:struct}
Recall the definitions of $A^r_i$ in previous sections; see the beginning of Section~\ref{sect:k=1}. 
By definition,
$A^0_i\sim \Binom(\frac{n}{k}-1,\frac{a\log(n)}{n})$ and $A^r_i\sim \Binom(\frac{n}{k}, \frac{b\log(n)}{n})$ for $r>0$, and they are independent. 
Also note that $A^r_i$ are functions of the underlying graph $G$.

\begin{proposition}  \label{prop:con}
Let $(X,G)\sim \SSBM(n,k, a\log(n)/n, b\log(n)/n)$, where $\sqrt{a}-\sqrt{b} > \sqrt{k}$.
Suppose that $0< \beta\le \beta^\ast$.
Then there is a set $\cG_{\con}$ such that (i) $P(G \in \cG_{\con}) = 1-o(1)$ and (ii) for every $G\in \cG_{\con}$, 
$$
\sum_{i=1}^n \exp\big(k\beta (A^r_i-A^0_i) \big)
=(1+o(1)) n^{g(\beta)} .
$$
\end{proposition}

\begin{proof}
Let $\cG_1:=\{G:A^r_i-A^0_i<0\text{~for all~}i\in[n] \text{ and } 1\leq r \leq k-1 \}$. By \eqref{eq:tD}, we have $P(G\in\cG_1)=1-o(1)$. We will prove that
\begin{align}
& E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big]
= (1+o(1)) n^{g(\beta)}  , \label{eq:cg} \\
& \Var \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big]
= o (n^{g(\beta)} ) .  \label{eq:sw}
\end{align}
Then the proposition follows immediately from  Chebyshev's inequality and the fact that $g(\beta)\ge 0$ when $0<\beta\le\beta^\ast$ (see Lemma~\ref{lm:ele}).


In Proposition~\ref{prop:df} (see Appendix~\ref{ap:um}), we prove \eqref{eq:cg} for $0<\beta<\frac{1}{2k}\log\frac{a}{b}$. By Lemma~\ref{lm:ele}, $\beta^\ast<\frac{1}{2k}\log\frac{a}{b}$, so \eqref{eq:cg} holds for $0< \beta\le \beta^\ast$.
Now we are left to prove \eqref{eq:sw}. Observe that
\begin{equation} \label{eq:mh1}
\begin{aligned}
& \Var \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big] \\
= & \sum_{i=1}^n  \Var \big[\exp\big(k\beta (A^r_i-A^0_i) \big) ~\big|~ G\in\cG_1 \big] \\
& \hspace*{1.2in} + \sum_{i,j\in[n],i\neq j}
\Cov(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j) \big) ~\big|~ G\in\cG_1 ) \\
\le & \sum_{i=1}^n E \big[ \exp\big(2k\beta (A^r_i-A^0_i) \big) ~\big|~ G\in\cG_1 \big] \\
& \hspace*{1.2in} + \sum_{i,j\in[n],i\neq j}
\Cov(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j) \big) ~\big|~ G\in\cG_1) .
\end{aligned}
\end{equation}
By Corollary~\ref{cr:yy} in Appendix~\ref{ap:um}, for all $\beta>0$ we have
$$
\sum_{i=1}^n E[ \exp\big(2k\beta (A^r_i-A^0_i) \big) \big| G\in\cG_1]
= O ( n^{\tilde{g}(2\beta)} ) .
$$
By Lemma~\ref{lm:ele}, $\tilde{g}(\beta)$ is a decreasing function, and it is strictly decreasing when $\beta\le\beta^\ast<\frac{1}{2k}\log\frac{a}{b}$. Therefore, $g(\beta)=\tilde{g}(\beta)>\tilde{g}(2\beta)$ whenever $\beta\le\beta^\ast$. As a consequence, 
\begin{equation} \label{eq:mh2}
\sum_{i=1}^n E[ \exp\big(2k\beta (A^r_i-A^0_i) \big) \big| G\in\cG_1]
< o ( n^{g(\beta)} ) .
\end{equation}



Now we are left to bound the covariance of $\exp\big(k\beta (A^r_i-A^0_i) \big)$ and $\exp\big(k\beta (A^r_j-A^0_j) \big)$ for $i\neq j$.
Define $\xi_{ij}=\xi_{ij}(G):=\mathbbm{1}[\{i,j\}\in E(G)]$ as the indicator function of the edge $\{i,j\}$ connected in graph $G$.
Now suppose that\footnote{The case of $X_i=X_j$ can be handled in the same way.} $X_i\neq X_j$. Then we can decompose $A'^r_i$ and $A'^r_j$ as $A^r_i=A'^r_i+\xi_{ij}$ and $A^r_j=A'^r_j+\xi_{ij}$, where both $A'^r_i$ and $A'^r_j$ have distribution $\Binom(\frac{n}{k} - 1 , \frac{b\log(n)}{n})$,
and the five random variables $A^0_i, A^0_j, A'^r_i,A'^r_j$ and $\xi_{ij}$ are independent.
Therefore,
\begin{align*}
& \Cov(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j) \big) ) \\
= & E[\exp\big(k\beta (A^r_i-A^0_i+A^r_j-A^0_j) \big)] 
-E[\exp\big(k\beta (A^r_i-A^0_i) \big)] E[\exp\big(k\beta (A^r_j-A^0_j) \big)]   \\
= & E[\exp\big(k\beta (A'^r_i-A^0_i) \big)] E[\exp\big(k\beta (A'^r_j-A^0_j) \big)]  \Big( E[\exp(2k\beta \xi_{ij})] -
\big(E[\exp(k\beta \xi_{ij})]\big)^2 \Big) \\
= & E[\exp\big(k\beta (A'^r_i-A^0_i) \big)] E[\exp\big(k\beta (A'^r_j-A^0_j) \big)] \\
& \hspace*{1.2in}
\Big( 1-\frac{b\log(n)}{n} + \frac{b\log(n)}{n} e^{2k\beta} -
\Big(1-\frac{b\log(n)}{n} + \frac{b\log(n)}{n} e^{k\beta} \Big)^2 \Big) \\
= & \Theta\Big( \frac{\log(n)}{n} \Big) E[\exp\big(k\beta (A'^r_i-A^0_i) \big)] E[\exp\big(k\beta (A'^r_j-A^0_j) \big)] \\
= & \Theta\Big( \frac{\log(n)}{n} \Big) E[\exp\big(k\beta (A^r_i-A^0_i) \big)] E[\exp\big(k\beta (A^r_j-A^0_j) \big)]  ,
\end{align*} 
where the last equality holds because $\exp\big(k\beta (A'^r_i-A^0_i) \big)$ differs from $\exp\big(k\beta (A^r_i-A^0_i) \big)$  by a factor of at most $e^{k\beta}$. 
By \eqref{eq:pl} in Appendix~\ref{ap:um}, $E \big[  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\big|~ G\in\cG_1 \big] 
= (1+o(1)) E \big[  \exp\big(k\beta (A^r_i-A^0_i) \big) \big]$ when $0<\beta\le\beta^\ast$. By Lemma \ref{lem:BijG} in Appendix~\ref{ap:um} we have $E[\exp\big(k\beta (A^r_i-A^0_i+A^r_j-A^0_j) ~\big|~ G\in\cG_1 \big)] = (1+o(1)) E[\exp\big(k\beta (A^r_i-A^0_i+A^r_j-A^0_j) \big)]$ when $0<\beta\le\beta^\ast$.
Therefore,
\begin{align*}
& \Cov \big(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j)  \big) ~\big|~ G\in\cG_1 \big) \\
= & (1+o(1)) \Cov \big(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j)  \big) \big) \\
= & \Theta\Big( \frac{\log(n)}{n} \Big) E[\exp\big(k\beta (A^r_i-A^0_i) \big)] E[\exp\big(k\beta (A^r_j-A^0_j) \big)] .
\end{align*}
As a consequence,
\begin{align*}
& \sum_{i,j\in[n],i\neq j}
\Cov \big(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j) \big) ~\big|~ G\in\cG_1 \big) \\
= & \Theta\Big( \frac{\log(n)}{n} \Big) \sum_{i,j\in[n],i\neq j} \Big( E[\exp\big(k\beta (A^r_i-A^0_i) \big)] E[\exp\big(k\beta (A^r_j-A^0_j) \big)] \Big) \\
\le & \Theta\Big( \frac{\log(n)}{n} \Big)
\Big(\sum_{i=1}^n  E[\exp\big(k\beta (A^r_i-A^0_i) \big)] \Big)^2 \\
\overset{(a)}{=} & \Theta(n^{2g(\beta)-1} \log(n)) \\
\overset{(b)}{=} & o(n^{g(\beta)})
\end{align*}
where equality $(a)$ follows from \eqref{eq:lb}, and $(b)$ follows from $g(\beta)<1$ when $0<\beta\le\beta^\ast$; see Lemma~\ref{lm:ele} (vi).
Finally, \eqref{eq:sw} follows immediately from this bound and \eqref{eq:mh1}--\eqref{eq:mh2}.
\end{proof}

\begin{remark}
One might wonder why we use the conditional expectation and variance to prove Proposition~\ref{prop:con} instead of using the unconditional ones. By \eqref{eq:lb} in Appendix~\ref{ap:um},
$$
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big]
= (1+o(1)) n^{g(\beta)}  
$$
for all $\beta$. This gives us the same estimate as the conditional expectation in \eqref{eq:cg}. However, the unconditional variance can be much larger than the conditional one in \eqref{eq:sw}. Recall from \eqref{eq:mh1} that we use $\sum_{i=1}^n E[ \exp\big(2k\beta (A^r_i-A^0_i) \big)]$ to bound the variance. 
By Corollary~\ref{cr:yy} in Appendix~\ref{ap:um},
for the unconditional case this sum is of order $\Theta(n^{g(2\beta)})$ while for the conditional case it is of order $O(n^{\tilde{g}(2\beta)})$. One can show that if $\beta\le\beta^\ast$, then we always have $g(2\beta)>\tilde{g}(2\beta)$. Therefore the order of the unconditional variance is much larger than the conditonal one.
\end{remark}

\begin{theorem}  \label{thm:dist}
	Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\alpha>b\beta$ and $0<\beta\le \beta^\ast$. Let 
	$
	(X,G,\sigma) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, 1) .
	$
	Then
	$$
	P_{\SIBM}(\dist(\sigma, \Gamma) = \Theta(n^{g(\beta)}) ) = 1-o(1) .
	$$
\end{theorem}
\begin{proof}
	We prove the following equivalent form:
	$$
	P_{\SIBM}(\dist(\sigma, X) = \Theta(n^{g(\beta)}) ~\big|~ \dist(\sigma, X) \le n/k) = 1-o(1) .
	$$
	Corollary~\ref{cr:1} together with Proposition~\ref{prop:43} implies that there is an integer $z>0$ and a set $\cG_{\good}$ such that
	
	\noindent (i)
	$P(G\in\cG_{\good}) \ge 1- O(n^{-4})$.
	
	\noindent (ii) For every $G\in\cG_{\good}$, 
	\begin{equation}  \label{eq:hs}
	P_{\sigma|G} \big( \sigma \in  \Lambda(G, z)
	\text{~and~} \dist(\sigma, X) \le n^\theta ~\big|~ \dist(\sigma, X) \le n/k \big) 
	=1- O(n^{-4}) ,
	\end{equation}
	where we can choose $\theta$ to be any constant in the open interval $(\tilde{g}(\beta), 1)$.
	
	We now define 
	$\phi_i := \mathbbm{1}[\sigma_i \neq X_i]$ for $i\in[n]$, and
	we want to estimate $\dist(\sigma,X)=\sum_{i=1}^n \phi_i$. 
	Given a fixed graph $G$ and a random sample $\sigma$, we will define {\bf Bernoulli} random variables $\underline{S}_1,\dots, \underline{S}_n$ and $\overline{S}_1,\dots,\overline{S}_n$ satisfying the following conditions:
	\begin{enumerate}
		\item $\underline{S}_1,\dots, \underline{S}_n$ are conditionally independent given the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$. $\overline{S}_1,\dots,\overline{S}_n$ are also conditionally independent given the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$.
		\item $\underline{S}_i\le \phi_i\le \overline{S}_i$ for all $i\in[n]$.
		\item Conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$, $P(\underline{S}_i=1)=\underline{C}
		\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)$ and $P(\overline{S}_i=1)=\overline{C}
		\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)$ for all $i\in[n]$. The constants $\bar{C}$ and $\underline{C}$ are defined in Equation \eqref{eq:qke}.
	\end{enumerate}
	We will postpone the definition of random variables $\underline{S}_1,\dots, \underline{S}_n$ and $\overline{S}_1,\dots,\overline{S}_n$ to the end of this proof. For now let us assume the existence of these random variables and prove the theorem.
	By property 3), conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$, we have
	\begin{align*}
	& E[\underline{S}_1 + \dots + \underline{S}_n] = \underline{C}
	\sum_{r=1}^{k-1}\sum_{i=1}^n \exp\big(2\beta (A^r_i-A^0_i) \big) , \quad
	\Var(\underline{S}_1 + \dots + \underline{S}_n) \le \underline{C}
	\sum_{r=1}^{k-1}\sum_{i=1}^n \exp\big(2\beta (A^r_i-A^0_i) \big)
 	, \\
	& E[\overline{S}_1 + \dots + \overline{S}_n] = \overline{C}
	\sum_{r=1}^{k-1}\sum_{i=1}^n \exp\big(2\beta (A^r_i-A^0_i) \big) , \quad
	\Var(\overline{S}_1 + \dots + \overline{S}_n) \le \overline{C}
	\sum_{r=1}^{k-1}\sum_{i=1}^n \exp\big(2\beta (A^r_i-A^0_i) \big) ,
	\end{align*}
	where we use the fact that the variance of a Bernoulli random variable is always upper bounded by its expectation.
	By Proposition~\ref{prop:con}, for all $G\in\cG_{\good}\cap\cG_{\con}$, we have 
	\begin{align*}
	& E[\underline{S}_1 + \dots + \underline{S}_n] = \Theta(n^{g(\beta)}) , \quad
	\Var(\underline{S}_1 + \dots + \underline{S}_n) = O(n^{g(\beta)}) , \\
	& E[\overline{S}_1 + \dots + \overline{S}_n] = \Theta(n^{g(\beta)}) , \quad
	\Var(\overline{S}_1 + \dots + \overline{S}_n) = O(n^{g(\beta)}) 
	\end{align*}
	conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$. Since $g(\beta)\ge 0$ for all $0<\beta\le \beta^\ast$, by Chebyshev's inequality we know that both $\underline{S}_1 + \dots + \underline{S}_n=\Theta(n^{g(\beta)})$ and $\overline{S}_1 + \dots + \overline{S}_n=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$.
	Since $\underline{S}_i\le \phi_i\le \overline{S}_i$ for all $i\in[n]$, we also have $\dist(\sigma,X)=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$. Combining this with \eqref{eq:hs}, we obtain that for every $G\in\cG_{\good}\cap\cG_{\con}$, $\dist(\sigma,X)=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ conditioning on $\{\dist(\sigma, X) \le n/k\}$. Finally, the theorem follows from $P(G\in\cG_{\good}\cap\cG_{\con})=1-o(1)$.
	
	Now we are left to define {\bf Bernoulli} random variables $\underline{S}_1,\dots, \underline{S}_n$ and $\overline{S}_1,\dots,\overline{S}_n$.
	First we define some auxiliary random variables.
	For $i\in[n]$, let 
	$\underline{R}_i=\underline{R}_i(\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n,G)$ be a Bernoulli random variable with parameter 
	$$
	\min \Big( \frac{\underline{C}
		\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)}{P_{\sigma|G}(\phi_i=1|\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n)} , 1 \Big) ,
	$$
	and let 
	$\overline{R}_i=\overline{R}_i(\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n,G)$ be a Bernoulli random variable with parameter 
	$$
	\min \Big(\frac{1- \overline{C}
		\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)}{P_{\sigma|G}(\phi_i=0|\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n)} , 1 \Big) .
	$$
	Furthermore, let both $\underline{R}_i$ and $\overline{R}_i$ be independent of $\phi_i$.
	For $i\in[n]$, define
	$$
	\underline{S}_i=\phi_i \underline{R}_i \quad
	\text{and} \quad
	\overline{S}_i = 1- (1-\phi_i) \overline{R}_i .
	$$
	Property 2) above directly follows from this definition.
	Now condition on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$.
	By \eqref{eq:soon} we have $\underline{R}_i =\frac{\underline{C}
		\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)}{P_{\sigma|G}(\phi_i=1|\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n)} $, one can see that no matter what values $\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n$ take, the conditional probability of $\underline{S}_i=1$ given these variables is always $\underline{C}
	\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)$. Therefore, conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$, $\underline{S}_i$ is independent of $\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n$ and it only depends on $\phi_i$. As a consequence, $\underline{S}_1,\dots, \underline{S}_n$ are conditionally independent given the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$. Thus, we have verified the three properties above for $\underline{S}_1,\dots, \underline{S}_n$. The arguments for $\overline{S}_1,\dots, \overline{S}_n$ are the same.
\end{proof}
For $0<\beta<\beta^\ast$, we have $g(\beta)>0$, so Theorem~\ref{thm:dist} immediately implies that $P_{\SIBM}(\sigma \in \Gamma)=o(1)$. However, when $\beta=\beta^\ast$, we have $g(\beta^\ast)=0$. In this case, Theorem~\ref{thm:dist} tells us that
$
P_{\SIBM}(\dist(\sigma,\Gamma) = \Theta(1) ) = 1-o(1) ,
$
but this is not sufficient for us to draw any conclusion on $P_{\SIBM}(\sigma \in \Gamma)$.
Below we use Proposition~\ref{prop:zj} to prove that when $\beta=\beta^\ast$, $P_{\SIBM}(\sigma \in \Gamma)$ is bounded away from $1$.

\begin{proposition}  \label{prop:zj}
	Let $a,b,\alpha> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta^\ast$. Let 
	$
	(X,G,\sigma) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta^\ast, 1) .
	$
	Then 
	$$
	P_{\SIBM}(\sigma \in \Gamma) \le \frac{1}{k}(1+o(1)) .
	$$
\end{proposition}
\begin{proof}
	We prove an equivalent form
	$$
	P_{\SIBM}(\sigma = X | \dist(\sigma, X) \le n/k) \le \frac{1}{k}(1+o(1)) .
	$$
	By \eqref{eq:isingma}, we have
	\begin{align*}
	\frac{P_{\sigma|G}(\sigma=X^{(\sim i)} )}
	{P_{\sigma|G}(\sigma=X)}
	& = \sum_{r=1}^{k-1}\exp\Big(k\big(\beta^\ast+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i)
	-\frac{2(k-1)\alpha\log(n)}{n} \Big) \\
	& = (1+o(1))\sum_{r=1}^{k-1} \exp\big(k\beta^\ast (A^r_i-A^0_i) \big)  ,
	\end{align*}
	where the last equality holds for almost all $G$ since $|A^r_i-A^0_i|=O(\log(n))$ with probability $1-o(1)$; see Lemma~\ref{lm:bmd} in Appendix~\ref{ap:6}.
	Take $\cG_{\con}$ from Proposition~\ref{prop:con}. Then for every $G\in\cG_{\con}$,
	\begin{align*}
	\frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
	{P_{\sigma|G}(\sigma=X)}  = (1+o(1)) \sum_{i=1}^n \sum_{r=1}^{k-1}\exp\big(k\beta^\ast (A^r_i-A^0_i) \big)  =k+o(1) .
	\end{align*}
	As a consequence, for every $G\in\cG_{\con}$,
	\begin{align*}
	P_{\sigma|G} (\sigma= X | \dist(\sigma, X) \le n/k ) <
	\frac {P_{\sigma|G}(\sigma=X)}
	{P_{\sigma|G}(\sigma=X)+\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
	=\frac{1}{k} (1+o(1)) .
	\end{align*}
	By Proposition~\ref{prop:con}, $P(G\in\cG_{\con})=1-o(1)$, so
	$$
	P_{\SIBM}(\sigma= X | \dist(\sigma, X) \le n/k ) \le \frac{1}{k}(1+o(1)) .
	$$
\end{proof}
\section{Exact recovery in $O(n)$ time when $\lfloor \frac{m+1}{2} \rfloor \beta>\beta^\ast$}
\label{sect:direct}
In this section, we prove that Algorithm~\ref{alg:ez} in Section~\ref{sect:multi} is able to learn $\SIBM(n,k,a\log(n)/n, \linebreak[4] b\log(n)/n,\alpha,\beta, m)$ as long as $\sqrt{a}-\sqrt{b} > \sqrt{k}$,  $\alpha>b\beta$ and $\lfloor \frac{m+1}{2} \rfloor \beta>\beta^\ast$, where $\beta^\ast$ is defined in \eqref{eq:defstar}.

\begin{proposition} \label{prop:nr}
	Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$. Let $m$ be an integer such that $\lfloor \frac{m+k-1}{k} \rfloor \beta>\beta^\ast$.
	Let 
	$$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
	$$
	Let $\hat{X}=\texttt{LearnSIBM}(\sigma^{(1)},\dots,\sigma^{(m)})$ be the output of Algorithm~\ref{alg:ez}. Then
	$$
	P(\hat{X} \in \Gamma) = 1-o(1) .
	$$
\end{proposition}

By Proposition~\ref{prop:tt},
if $\beta>\beta^\ast$, then $\sigma \in \Gamma $ with probability $1-o(1)$, so $m=1$ sample suffices for recovery. In the rest of this section, we will focus on the case $\beta\le \beta^\ast$.

In Proposition~\ref{prop:43} (or Proposition~\ref{prop:1}), we have shown that $\dist(\sigma^{(i)}, \Gamma) =o(n)$ for all $i\in[m]$ with probability $1-O(n^{-r})$ for any constant $r>0$, but it is possible that $\sigma^{(1)}$ is close to $X$ while $\sigma^{(2)}$ is close to some $f(X)$. Step 1 (the alignment step) in the above algorithm eliminates such possibility: After the alignment step, with probability $1-O(n^{-r})$ only the following two scenarios will happen: Either
$\dist(\sigma^{(i)}, X)=o(n)$ for all $i\in[m]$ or $\dist(\sigma^{(i)}, f(X))=o(n)$ for all $i\in[m]$ where $f \in \Gamma \backslash \{\textrm{id}\}$. Without loss of generality, we assume the former case, i.e., we assume that 
$\dist(\sigma^{(i)}, X) \le n/k$ for all $i\in[m]$,
and in the rest of this section we will prove that Algorithm~\ref{alg:ez} outputs $\hat{X}=X$ with probability $1-o(1)$.


Given the ground truth $X$, the graph $G$ and a vertex $i\in[n]$, define the neighbors of $i$ in $G$ as 
\begin{equation} \label{eq:ngbi}
\cN_i(G) := \{j\in[n]\setminus\{i\}:
\{i,j\}\in E(G) \} .
\end{equation}
Given a sample $\sigma\in W^n$ and a graph $G$,  we define the set of ``bad" neighbors of the vertex $i$ in $G$ as
$$
\Omega_i(\sigma,G):=\{j\in \cN_i(G): 
\sigma_j \neq X_j \} .
$$
Given a vertex $i\in[n]$, a graph $G$ and an integer $z>0$, we also define
$$
\Lambda_i(G, z):=\{ \sigma\in W^n: |\Omega_i(\sigma,G)| < z \} ,
$$
i.e., $\Lambda_i(G, z)$ consists of all the samples for which the number of ``bad" neighbors of the vertex $i$ in $G$ is less than $z$.

\begin{lemma} \label{lm:us}
	Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\alpha>b\beta$ and $\beta\le \beta^\ast$.
	Given any $r>0$, there exists an integer $z>0$ such that for large enough $n$ and every $i\in[n]$, 
	\begin{equation} \label{eq:zr}
	P_{\SIBM} (\sigma\in \Lambda_i(G, z)
	| \dist(\sigma, X) \le n/k)
	> 1 - n^{-r} .
	\end{equation}
\end{lemma}
\begin{proof}
	The event $\{\sigma\notin \Lambda_i(G, z) \}$ can be written as
	\begin{equation} \label{eq:lei}
	\{\sigma\notin \Lambda_i(G, z) \}
	=\{|\Omega_i(\sigma,G)| \ge z \}
	=\bigcup_{\tilde{\cI}\subseteq[n]\setminus\{i\}, |\tilde{\cI}| = z} \{\tilde{\cI} \subseteq \Omega_i(\sigma,G)\} .
	\end{equation}
	Given a subset $\tilde{\cI}\subseteq[n]\setminus\{i\}$,
	the event $\{\tilde{\cI} \subseteq \Omega_i(\sigma,G) \}$ is the intersection of two events $\{\sigma_j \neq X_j \text{~for all~} j\in \tilde{\cI}\}$ and $\{i\in\cN_j(G) \text{~for all~} j\in \tilde{\cI}\}$.
	
	We start with the analysis of the first event.
	In Proposition~\ref{prop:43}, we have shown that
	for any $\delta>0$ and any $r'>0$, there exists $n_0(\delta, r')$ such that for all even integers $n>n_0(\delta, r')$,
	$$ 
	P_{\SIBM} \Big(\dist(\sigma, \Gamma) \le n^{\theta}
	\Big) \ge 1- n^{-r'} .
	$$
	By Lemma~\ref{lm:ele} (vi), we know that $g(\beta)<1$ for all $0<\beta\le\beta^\ast$, so we can always choose a $\delta>0$ such that $\theta<1$.
	Let $\theta:=\theta<1$. Then for large enough $n$,
	$$
	P_{\SIBM} \Big(\dist(\sigma, \Gamma) \le n^{\theta} \Big) \ge 1- n^{-r'} .
	$$
	This in particular implies that
	$$
	P_{\SIBM} \Big(\dist(\sigma, X) \le n^{\theta} | \dist(\sigma, X) \le n/k
	\Big) \ge 1- n^{-r'} .
	$$
	Moreover, Lemma~\ref{lm:bq} in Appendix~\ref{ap:6} (see inequality \eqref{eq:l2}) tells us that
	$$
	P_{\SIBM}(\sigma_j \neq X_j \text{~for all~}  j\in\tilde{\cI}
	~\big| \dist(\sigma,X) \le n^\theta) \le \Big(\frac{n^\theta}{n/k-|\tilde{\cI}|}
	\Big)^{|\tilde{\cI}|}
	\quad \text{for all~} \tilde{\cI}\subseteq [n] .
	$$
	Therefore, for any subset $\tilde{\cI}\subseteq[n]\setminus\{i\}$ with size $|\tilde{\cI}|=z$, we have
	$$
	P_{\SIBM}(\sigma_j \neq X_j \text{~for all~} j\in \tilde{\cI}
	| \dist(\sigma, X) \le n/k )
	\le O(n^{-(1-\theta) z}) + O(n^{-r'}) . 
	$$
	Taking $r'>(1-\theta) z$ gives us
	\begin{equation} \label{eq:gr}
	P_{\SIBM}(\sigma_j \neq X_j \text{~for all~} j\in \tilde{\cI}
	| \dist(\sigma, X) \le n/k )
	\le O(n^{-(1-\theta) z}) . 
	\end{equation}
	
	Given $r>0$, we will prove \eqref{eq:zr} for any integer $z\ge \frac{r+1}{1-\theta}$. 
	Now condition on the event $\{\sigma_j \neq X_j \text{~for all~} j\in \tilde{\cI}\}$.
	Taking a specific $\sigma'$ which satistifies  $\{\sigma_j \neq X_j \text{~for all~} j \in \tilde{\cI}, \dist(\sigma, X) \le n/k\}$.
	Using Lemma \ref{lem:post_independent}, we can show that
	\begin{align*}
	P(i\in\cN_j(G) \text{~for all~} j\in \tilde{\cI} | \sigma_j \neq X_j \text{~for all~} j \in \tilde{\cI}, \dist(\sigma, X) \le n/k) &\leq P(i\in\cN_j(G) \text{~for all~} j\in \tilde{\cI} | \sigma =\sigma')\\
	&=\prod_{j\in \tilde{\cI}}\Pr((i,j)\in E(G) | \sigma = \sigma') =  O\Big( \frac{\log^z(n)}{n^z} \Big) .
	\end{align*}
	Combining this with \eqref{eq:gr}, we have
	$$
	P_{\SIBM} (\tilde{\cI} \subseteq \Omega_i(\sigma,G))
	= O (n^{-(2-\theta)z}\log^z(n)) .
	$$
	Finally, combining this with \eqref{eq:lei} and the union bound, we have
	\begin{align*}
	P_{\SIBM} \big(\sigma\notin \Lambda_i(G, z)
	| \dist(\sigma, X) \le n/k \big)
	& \le n^z O (n^{-(2-\theta)z}\log^z(n))
	= O (n^{-(1-\theta)z}\log^z(n))  \\
	& \le O (n^{-r-1}\log^z(n)) <n^{-r}
	\end{align*}
	for large enough $n$. This completes the proof of the lemma.
\end{proof}
We further define 
\begin{align*}
\Lambda(G, z) :=
\bigcap_{i=1}^n
\Lambda_i(G, z) .
\end{align*}
The following corollary follows immediately from Lemma~\ref{lm:us} and the union bound.
\begin{corollary} \label{cr:1}
	Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$.
	Given any $r>0$, there exists an integer $z>0$ such that for large enough $n$, 
	$$
	P_{\SIBM} (\sigma\in \Lambda(G, z)
	| \dist(\sigma, X) \le n/k)
	> 1 - n^{-2r} .
	$$
	Equivalently, for any $r>0$, there is an integer $z>0$ and a set $\cG_{\good}$ such that
	
	\noindent (i)
	$P(G\in\cG_{\good}) \ge 1- O(n^{-r})$.
	
	\noindent (ii) For every $G\in\cG_{\good}$,
	$$
	P_{\sigma|G} \big( \sigma \in  \Lambda(G, z)  \big| \dist(\sigma, X) \le n/2 \big) =1- O(n^{-r}).
	$$
\end{corollary}
\begin{proof}
	We only explain why the first statement implies the second one.
	Define a set 
	$$
	\cG_{\bad}:= \big\{ G: P_{\sigma|G} \big( \sigma \notin  \Lambda(G, z)  \big| \dist(\sigma, X) \le n/k \big)
	>n^{-r} \big\} .
	$$
	Then by the union bound and Lemma~\ref{lm:us}, $P(G\in\cG_{\bad})\le n^{-r}$. Therefore, the second statement follows by taking $\cG_{\good}$ to be the complement of $\cG_{\bad}$.
\end{proof}
We define subsets of $\cN_i(G)$ with the same labeling and the opposite labeling as
$$
\cN_{i,r}(G):=\{j\in \cN_i(G): X_j=\omega^r \cdot X_i \}
$$
respectively.
Recall that in \eqref{eq:defAB}, we defined $A^r_i=A^r_i(G):=|\cN_{i,r}(G)|$.
By definition, $\sigma = X^{(\sim\cI)}\in  \Lambda_i(G,z)$ if and only if $|\cI\cap \cN_i(G)| < z$.


\begin{lemma} \label{lm:et}
	Let $0<\theta<1$ be some constant and $1\leq r \leq k-1$. Then for large enough $n$ we have
	\begin{align*}
	\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i - 2z) \Big) & \le 
	\frac{P_{\sigma|G}(\sigma_i= \omega^r \cdot X_i, \sigma\in \Lambda_i(G,z) ,\dist(\sigma, X) \le n^{\theta} ) } 
	{ P_{\sigma|G}(\sigma_i=X_i, \sigma\in \Lambda_i(G,z) ,\dist(\sigma, X) \le n^{\theta} ) } \\
	& \le \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i + 2z) \Big)
	\end{align*}
\end{lemma}

\begin{proof}
	We only need to show that for every $\cI\in [n]\setminus\{i\},v$ with size $|\cI|\le n^\theta$ such that $X^{(\sim\cI,v)}\in \Lambda_i(G,z)$,
	and $v'$ be an extension of $v$ such that $v'=[v, \omega^r]$:
	\begin{equation} \label{eq:qk}
	\begin{aligned}
	\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i - kz) \Big) & \le 
	\frac{P_{\sigma|G}(\sigma= X^{(\sim(\cI\cup\{i\}), v')} ) } { P_{\sigma|G}(\sigma= X^{(\sim\cI,v)} ) } \\
	& \le \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i + kz) \Big) .
	\end{aligned}
	\end{equation}
	Then using 
	$$
	\frac{P_{\sigma|G}(\sigma_i= \omega^r \cdot X_i, \sigma\in \Lambda_i(G,z) ,\dist(\sigma, X) \le n^{\theta} ) } 
	{ P_{\sigma|G}(\sigma_i=X_i, \sigma\in \Lambda_i(G,z) ,\dist(\sigma, X) \le n^{\theta} ) } 
	= \frac{\sum_{\cI, v} P_{\sigma|G}(\sigma= X^{(\sim(\cI\cup\{i\}), v')} ) } {\sum_{\cI, v} P_{\sigma|G}(\sigma= X^{(\sim\cI,v)} ) } 
	$$
	we can get the conclusion.
	
	Define $v^{-r} = |\{i | v_i = \omega^{k-r}\}|$ and
	$$
	A'^r_i :=A^r_i-|\cI\cap \cN_{i,r}(G)| + \sum_{s=0, s\neq r}^{k-1}|\cI\cap \cN_{i,s}(G) \cap v^{-(s-r)}| .
	$$
	Since $X^{(\sim\cI, v)}\in \Lambda_i(G,z)$, we have $|\cI\cap \cN_{i,r}(G)| \le z-1$. Therefore,
	\begin{equation} \label{eq:oo}
	A^r_i-A^0_i-k(z-1) \le A'^r_i-A'^0_i\le A^r_i-A^0_i+k(z-1) .
	\end{equation}
	By \eqref{eq:isingma}, we have
	\begin{align*}
	& \frac{P_{\sigma|G}(\sigma= X^{(\sim(\cI\cup\{i\}))} ) } { P_{\sigma|G}(\sigma= X^{(\sim\cI)} ) } \\
	= & \exp\Big(k(\beta+\frac{\alpha\log n }{n})(A'^r_i-A'^0_i)
	-2\frac{(k-1)\alpha\log(n)}{n}
	\Big)
	\\
	= & \exp\Big(k(\beta+\frac{\alpha\log n }{n})(A'^r_i-A'^0_i)+o(1) \Big)
	\end{align*}
	Taking \eqref{eq:oo} into this equation gives us \eqref{eq:qk} and completes the proof.
\end{proof}
\begin{remark}
	From the proof we can see that $\Gamma_i$ can be replaced with $\Gamma$. Since $A_i^r - A_i^0 < 0$ with probability one when
	$n$ is large, we can get there exists $\underline{C}, \bar{C}$ such that
	\begin{equation} \label{eq:qke}
	\underline{C}\sum_{r=1}^{k-1}\exp\Big(k\beta(A^r_i-A^0_i) \Big)  \le 
	P_{\sigma|G}(\sigma \neq X_i | \sigma \in \Lambda(G, z),
	\dist(\sigma, X) \leq n^{\theta})
	\le \bar{C} \sum_{r=1}^{k-1}\exp\Big(k\beta (A^r_i-A^0_i ) \Big) 
	\end{equation}
	where $\underline{C}$ and $\overline{C}$ are constants that are independent of $n$.
	Under the condition that $\bar{\sigma} \in \Lambda(G, z)$ and
	$\dist(\bar{\sigma}, X) \leq n^{\theta}$,
	we can also reformulate Equation \eqref{eq:qk} as follows:
	\begin{equation}\label{eq:soon}
	\underline{C} \sum_{r=1}^{k-1} \exp(k\beta(A^r_i - A^0_i)) \leq P_{\sigma | G}(\sigma_i \neq X_i | \sigma_j = \bar{\sigma}_j \text{ for all } j\neq i ) \leq
	\bar{C} \sum_{r=1}^{k-1} \exp(k\beta(A^r_i - A^0_i))
	\end{equation}
\end{remark}

\noindent
{\em Proof of Proposition~\ref{prop:nr}.}
Recall that $\sigma^{(1)},\dots,\sigma^{(m)}$ are the samples of SIBM.
At the beginning of this section, we have shown that after the alignment step in Algorithm~\ref{alg:ez}, with probability $1-O(n^{-r})$ for any constant $r>0$ $\dist(\sigma^{(j)}, f(X))=o(n)$ for all $j\in[m]$ and $f(X) \in \Gamma$. Without loss of generality, we assume that 
$\dist(\sigma^{(j)}, X) \le n/k$ for all $j\in[m]$. 

Corollary~\ref{cr:1} together with Proposition~\ref{prop:43} implies that there is an integer $z>0$ and a set $\cG_{\good}$ such that

\noindent (i)
$P(G\in\cG_{\good}) \ge 1- O(n^{-4})$.

\noindent (ii) For every $G\in\cG_{\good}$, conditioning on the event $\dist(\sigma^{(j)}, X) \le n/k$ for all $j\in[m]$,
\begin{equation}  \label{eq:chui}
P_{\sigma|G} \big( \sigma^{(j)}\in  \Lambda(G, z)
\text{~and~} \dist(\sigma^{(j)}, X) \le n^\theta
\text{~for all~} j\in[m]  \big) 
=1- O(n^{-4}) ,
\end{equation}
where we can choose $\theta$ to be any constant in the open interval $(g(\beta), 1)$.
By Lemma~\ref{lm:et},
$$
P_{\sigma|G}(\sigma_i^{(j)} = \omega^r \cdot X_i \big| \sigma^{(j)}\in \Lambda(G,z) ,\dist(\sigma^{(j)}, X) \le n^{\theta} ) 
\le  \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i + kz) \Big)
$$
for all $i\in[n]$ and all $j\in[m]$.
For $i\in[n]$, define 
$$
\Phi_i := |\{j\in[m]: \sigma_i^{(j)} = \omega^r \cdot X_i\}|
$$
as the number of samples for which $\sigma_i^{(j)} \neq X_i$.
For an integer $u\in[m]$, we have
$$
\{\Phi_i \ge u\} =
\bigcup_{\cJ\subseteq[m], |\cJ|=u}
\{\sigma_i^{(j)} = \omega^r \cdot X_i \text{~for all~} j\in \cJ\} .
$$
Therefore, by the union bound,
\begin{align*}
& P_{\sigma|G}( \Phi_i \ge u  ~\big|~  \sigma^{(j)}\in \Lambda(G,z) ,\dist(\sigma^{(j)}, X) \le n^{\theta} \text{~for all~} j\in[m] ) \\
\le &  \binom{m}{u} \exp\Big(ku\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i + kz) \Big)
\quad\quad\quad\quad\text{~for all~} i\in[n] .
\end{align*}
Combining this with \eqref{eq:chui}, we obtain that for every $G\in\cG_{\good}$, conditioning on the event $\dist(\sigma^{(j)}, X) \le n/k$ for all $j\in[m]$,
$$
P_{\sigma|G} \big( \Phi_i \ge u \big) \le  \binom{m}{u} \exp\Big(ku \big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i + kz) \Big) + O(n^{-4}).
$$
Using the union bound, we have
$$
P_{\sigma|G} \big(\exists i \text{~s.t.~} \Phi_i \ge u \big) \le 
C  \sum_{i=1}^n \exp\Big(ku \big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i ) \Big) + o(1) ,
$$
where $C:=\binom{m}{u}\exp(k^2u(\beta+\alpha)z)$ is a constant.
By Proposition \ref{prop:con}, if $\beta>\beta^*$, then there is a set $\cG^{(1)}$ such that (i) $P(G\in \cG^{(1)})=1-o(1)$; (ii) for every $G\in\cG^{(1)}$,
$$
\sum_{i=1}^n \exp\Big(k \big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i ) \Big) = o(1) .
$$
This immediately implies that if $u\beta>\beta^*$, then for every $G\in\cG^{(1)}\cap\cG_{\good}$, conditioning on the event $\dist(\sigma^{(j)}, X) \le n/k$ for all $j\in[m]$,
$$
P_{\sigma|G} \big(\exists i \text{~s.t.~} \Phi_i \ge u \big) \le 
C\sum_{i=1}^n \exp\Big(ku \big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i ) \Big) + o(1) =o(1).
$$
Since $P(G\in\cG^{(1)}\cap\cG_{\good})=1-o(1)$, we conclude that $P_{\SIBM} \big(\Phi_i \le u-1 \text{~for all~} i\in[n] \big)=1-o(1)$ conditioning on the event $\dist(\sigma^{(j)}, X) \le n/k$ for all $j\in[m]$.

By assumption we have $\lfloor \frac{m+k-1}{k} \rfloor \beta>\beta^\ast$. Therefore, conditioning on the event $\dist(\sigma^{(j)}, X) \le n/k$ for all $j\in[m]$, we have $\Phi_i\le \lfloor \frac{m-1}{k} \rfloor$ for all $i\in[n]$ with probability $1-o(1)$. As a consequence, after the majority voting step in Algorithm~\ref{alg:ez}, $\hat{X}_i=X_i$ for all $i\in[n]$ with probability $1-o(1)$. This completes the proof of Proposition~\ref{prop:nr}.
\hfill\qed
\section{Exact recovery is not solvable when $\lfloor (k-1)\frac{m+1}{k} \rfloor \beta < \beta^\ast$}\label{sect:converse}

\begin{lemma} \label{lm:qq}
	Let 
	$$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k,a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
	$$
	If there is $k$-pair $i_0,\dots, i_{k}\in[n]$ satisfying the following two conditions: (1) $\sigma_{i_1}^{(j)}=\sigma_{i_r}^{(j)}$ for all $j\in[m], r \in [k] $ and (2) $\{X_{i_0}, \dots, X_{i_{k}}\} = \{1,\omega, \dots, \omega^{k-1} \}$, then it is not possible to distinguish which permutation $(X_{i_1}, \dots, X_{i_k})$ takes. In other words, conditioning on the samples, the posterior probability of the ground truth being $X$ is the same as that of the ground truth being $X^{(\sim\{I\},v)}$ where $I=\{i_1, \dots, i_k\}$ and $v$ is taken such that $\{X^{(\sim I, v)}_{i_1}, \dots, X^{(\sim I,v)}_{i_k}\} = \{1,\omega, \dots, \omega^{k-1} \}$.
\end{lemma}
\begin{proof}
Taking a vector $\bar{X}$ which satisfies $\bar{X}_{i_r} = \omega^{r-1}$ for $r\in [k]$. For a vector $v$ such that $v_{1}\cdot \bar{X}_{i_1}, \dots, v_{r} \cdot \bar{X}_{i_k}$ is a permutation of $1, \omega, \dots, \omega^{k-1}$, we only need to show
\begin{equation}\label{eq:11}
P(X=\bar{X}, \sigma_{i_1}^{(j)}=\sigma_{i_r}^{(j)}, j\in[m], r\in[k]) = P(X=\bar{X}^{(\sim I, v)}, \sigma_{i_1}^{(j)}=\sigma_{i_r}^{(j)}, j\in[m], r\in[k])
\end{equation}
We can also find a 1-1 mapping $\pi$ which satisfies $ \bar{X}_{i_r} = \bar{X}^{(\sim I, v)}_{\pi(i_r)}$. $\pi$ is a permutation restricted on $\{i_1, \dots, i_k\}$ and is identify mapping on other indices.
Similar to the analysis in the proof of Lemma \ref{lm:cc}, $\pi$ satisfies properties such as
$\{pi(i), \pi(j)\} \in E(\pi(G)) \iff \{i,j\} \in E(G)$, $Z_G(\alpha, \beta) = Z_{\pi(G)}(\alpha, \beta)$ and
$$
P_{\SSBM}(G  | X = \bar{X})=P_{\SSBM}(\pi(G) | X = \bar{X}^{(\sim I,v)}),
$$

Using the Markov property of $X \to G \to \sigma$, we have
\begin{align*}
P(X=\bar{X}, \sigma_{i_1}^{(j)}=\sigma_{i_r}^{(j)}, j\in[m], r\in[k])& = \sum_{G \in \cG_{[n]}} P(X=\bar{X}) P_{\SSBM}(G  | X = \bar{X})\prod_{j=1}^m P_{\sigma | G}(\sigma =\sigma^{(j)})\\
\end{align*}
If we can show that 
\begin{equation}\label{eq:sigmaEqual}
P_{\sigma | G}(\sigma =\sigma^{(j)}) = P_{\sigma | \pi(G)}(\sigma =\sigma^{(j)})
\end{equation}
Then 
\begin{align*}
P(X=\bar{X}, \sigma_{i_1}^{(j)}=\sigma_{i_r}^{(j)}, j\in[m], r\in[k]) &
= \sum_{G \in \cG_{[n]}} P(X=\bar{X}^{(\sim I,v)})
P_{\SSBM}(\pi(G)  | X = \bar{X}^{(\sim I,v)})\prod_{j=1}^m P_{\sigma | \pi(G)}(\sigma =\sigma^{(j)})\\
& = P(\bar{X}^{(\sim I,v)}, \sigma_{i_1}^{(j)}=\sigma_{i_r}^{(j)}, j\in[m], r\in[k])
\end{align*}
The key to prove Equation \eqref{eq:sigmaEqual} lies at the property $\sigma^{(j)}_{i_1} = \sigma^{(j)}_{i_r}, r\in [k]$.
Let $c_G(\sigma, i,j) = (\beta + \frac{\alpha \log n}{n})I(\sigma_i,\sigma_j) \mathbbm{1}[\{i,j\}\in E(G)]  - \frac{\alpha \log n}{n}$
and $C_G(\sigma) = \frac{1}{Z_G(\alpha, \beta)}\exp(\sum_{j_1, j_2 \not\in \{i_1, \dots, i_k\}} c_G(\sigma, j_1, j_2) - (k-1)c_G(\sigma, i,i'))$
\begin{align*}
P_{\sigma | G}(\sigma =\sigma^{(j)}) & = C_G(\sigma^{(j)})\exp(\sum_{r=1}^k\sum_{s=1}^n c_G(\sigma^{(j)}, i_r, s)) \\
& = C_{\pi(G)}(\sigma^{(j)})\exp(\sum_{r=1}^k\sum_{s=1}^n c_G(\sigma^{(j)}, i_r, s) ) \\
& = C_{\pi(G)}(\sigma^{(j)})\exp(\sum_{r=1}^k\sum_{s=1}^n c_{\pi(G)}(\sigma^{(j)}, \pi(i_r), s) ) \\
& = C_{\pi(G)}(\sigma^{(j)})\exp(\sum_{r=1}^k\sum_{s=1}^n c_{\pi(G)}(\sigma^{(j)}, i_r, s) ) \\
& = P_{\sigma | \pi(G)}(\sigma =\sigma^{(j)})
\end{align*}
\end{proof}
\begin{lemma}  \label{lm:cvs}
	Let $v=(v^{(1)},v^{(2)},\dots,v^{(m)})\in W^m$ be a vector of length $m$ and let $u_r:=|\{i\in[m]:v^{(i)}=\omega^r\}|$ be the number of $\omega^r$'s in $v$ for $r=0,1,\dots, k-1$.
	Let $\sigma^{(1)},\dots,\sigma^{(m)}$ be the aligned samples of SIBM (see the alignment step in Algorithm~\ref{alg:ez}).
	Without loss of generality we assume that the aligned samples satisfy  $\dist(\sigma^{(j)}, X) \le n/k$ for all $j\in[m]$.
	Define 
	\begin{equation}  \label{eq:rl}
	T_r:= | \{i\in[n]:(\sigma_i^{(1)},\dots,\sigma_i^{(m)})=v, X_i=\omega^r \} | ,  \\
	\end{equation}
	If $u_r\beta<\beta^\ast$ for $r\in \{0,1, \dots, k-1 \} \backslash \{s\}$, then $P_{\SIBM}\big(T_s = \Theta(\sum_{r=0, r\neq s}^{k-1}n^{g(u_r\beta)}) \big) = 1-o(1)$.
\end{lemma}
\begin{proof}

	We only prove the claims about $T_0$ since the proof for $T_r (r>0)$ is virtually identical.
	The proof follows the same steps as the proof of Theorem~\ref{thm:dist}. All we need to do is to replace $\beta$ with $u\beta$ in the proof of Theorem~\ref{thm:dist}. For the sake of completeness, we provide the proof here.
	
	
	Corollary~\ref{cr:1} together with Proposition~\ref{prop:43} implies that there is an integer $z>0$ and a set $\cG_{\good}$ such that
	
	\noindent (i)
	$P(G\in\cG_{\good}) \ge 1- O(n^{-4})$.
	
	\noindent (ii) For every $G\in\cG_{\good}$, 
	\begin{equation}  \label{eq:ljrc}
	\begin{aligned}
	& P_{\sigma|G} \big( \sigma^{(j)}\in  \Lambda(G, z)
	\text{~and~} \dist(\sigma^{(j)}, X) \le n^\theta
	\text{~for all~} j\in[m] ~\big| \dist(\sigma^{(j)}, X) \le n/k
	\text{~for all~} j\in[m]  \big) \\
	& =1- O(n^{-4}) ,
	\end{aligned}
	\end{equation}
	where we can choose $\theta$ to be any constant in the open interval $(g(\beta), 1)$.
	We define $[n]_r:=\{v\in[n]:X_v= \omega^r\}$ for $r=0, \dots, k-1$.
	By Lemma~\ref{lm:et} we know that for all $i\in[n]_0$ and all $j\in[m]$,
	$P_{\sigma|G}(\sigma_i^{(j)} \neq 1 \big| \sigma^{(j)}\in \Lambda(G,z) ,\dist(\sigma^{(j)}, X) \le n^{\theta} )$ differ from
	$\sum_{r=1}^{k-1}\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i) \Big)$
	by at most a constant factor. Since $|A^r_i-A^0_i|=O(\log(n))$ with probability $1-o(1)$, the term $\frac{\alpha\log(n)}{n}(A^r_i-A^0_i)=o(1)$ and is negligible. Moreover, since the $m$ samples are independent given the graph $G$, there are $u_0$ samples which takes value $\sigma_i^{(j)} = 1$. Since the probability
	$P_{\sigma|G}(\sigma_i^{(j)} = 1 \big| \sigma^{(j)}\in \Lambda(G,z) ,\dist(\sigma^{(j)}, X) \le n^{\theta} )$ is near 1, we conclude that
	\begin{align*}
	& \underline{C}
	\sum_{r=1}^{k-1}\exp\big(k u_r \beta (A^r_i-A^0_i) \big) \\
	\le
	& P_{\sigma|G} \big( (\sigma_i^{(1)},\dots,\sigma_i^{(m)})=v ~\big|~ \sigma^{(j)}\in  \Lambda(G, z)
	\text{~and~} \dist(\sigma^{(j)}, X) \le n^\theta
	\text{~for all~} j\in[m] \big) \\
	\le & \overline{C}
	\sum_{r=1}^{k-1}\exp\big(k u_r \beta (A^r_i-A^0_i) \big)
	\end{align*}
	for all $i\in[n]$, where $\underline{C}$ and $\overline{C}$ are constants that are independent of $n$.
	Similar to Equation \eqref{eq:soon}. the above equation can be reformulated as follows:
	For every $\bar{\sigma}^{(1)},\dots,\bar{\sigma}^{(m)}\in W^n$ such that $\bar{\sigma}^{(j)}\in  \Lambda(G, z)$
	and $\dist(\bar{\sigma}^{(j)}, X) \le n^\theta$
	for all $j\in[m]$,
	\begin{equation} \label{eq:s56}
	\begin{aligned}
	& \underline{C}
	\sum_{r=1}^{k-1}\exp\big(k u_r  \beta (A^r_i-A^0_i) \big) \\
	\le
	& P_{\sigma|G} \big( (\sigma_i^{(1)},\dots,\sigma_i^{(m)})=v ~\big|~ (\sigma_{i'}^{(1)},\dots,\sigma_{i'}^{(m)})= (\bar{\sigma}_{i'}^{(1)},\dots,\bar{\sigma}_{i'}^{(m)}) \text{~for all~} i'\neq i \big) \\
	\le & \overline{C}
	\sum_{r=1}^{k-1}\exp\big(k u_r  \beta (A^r_i-A^0_i) \big).
	\end{aligned}
	\end{equation}
	Define 
	$\phi_i := \mathbbm{1}[(\sigma_i^{(1)},\dots,\sigma_i^{(m)})=v]$ for $i\in[n]$.

	Given a fixed graph $G$ and random samples $\sigma^{(1)},\dots,\sigma^{(m)}$, we will define {\bf Bernoulli} random variables $\underline{S}_1,\dots, \underline{S}_n$ and $\overline{S}_1,\dots,\overline{S}_n$ satisfying the following conditions:
	\begin{enumerate}
		\item $\underline{S}_1,\dots, \underline{S}_n$ are conditionally independent given the event $\{\sigma^{(j)}\in  \Lambda(G, z)
		\text{~and~} \dist(\sigma^{(j)}, X) \le n^\theta
		\text{~for all~} j\in[m]\}$. $\overline{S}_1,\dots,\overline{S}_n$ are also conditionally independent given the event $\{\sigma^{(j)}\in  \Lambda(G, z)
		\text{~and~} \dist(\sigma^{(j)}, X) \le n^\theta
		\text{~for all~} j\in[m]\}$.
		\item $\underline{S}_i\le \phi_i\le \overline{S}_i$ for all $i\in[n]$.
		\item Conditioning on the event $\{\sigma^{(j)}\in  \Lambda(G, z)
		\text{~and~} \dist(\sigma^{(j)}, X) \le n^\theta
		\text{~for all~} j\in[m]\}$,
		$P(\underline{S}_i=1)=\underline{C}
		\sum_{r=1}^{k-1}\exp\big(k u_r \beta (A^r_i-A^0_i) \big)$ and $P(\overline{S}_i=1)=\overline{C}
		\sum_{r=1}^{k-1}\exp\big(k u_r \beta (A^r_i-A^0_i) \big)$ for all $i\in[n]$.
	\end{enumerate}
	The construction of random variables $\underline{S}_1,\dots, \underline{S}_n$ and $\overline{S}_1,\dots,\overline{S}_n$ is rather similar to the one at the end of the proof of Theorem~\ref{thm:dist}, and we do not repeat it here. 
	Define a set $[n]_+:=\{i\in[n]:X_i=1\}$.
	Then $T_0=\sum_{i\in[n]_+} \phi_i$.
	By property 3), conditioning on the event $\{\sigma^{(j)}\in  \Lambda(G, z)
	\text{~and~} \dist(\sigma^{(j)}, X) \le n^\theta
	\text{~for all~} j\in[m]\}$, we have
	\begin{align*}
	& E\Big[ \sum_{i\in[n]_+}\underline{S}_i \Big] = \underline{C}
	\sum_{i\in[n]_+} \sum_{r=1}^{k-1}\exp\big(k u_r \beta (A^r_i-A^0_i) \big) , \quad
	\Var \Big(\sum_{i\in[n]_+}\underline{S}_i \Big) \le \underline{C}
	\sum_{i\in[n]_+} \sum_{r=1}^{k-1}\exp\big(k u_r \beta (A^r_i-A^0_i) \big), \\
	& E \Big[\sum_{i\in[n]_+} \overline{S}_i \Big] = \overline{C}
	\sum_{i\in[n]_+} \sum_{r=1}^{k-1}\exp\big(k u_r \beta (A^r_i-A^0_i) \big), \quad
	\Var \Big(\sum_{i\in[n]_+} \overline{S}_i \Big) \le \overline{C}
	\sum_{i\in[n]_+} \sum_{r=1}^{k-1}\exp\big(k u_r \beta (A^r_i-A^0_i) \big) ,
	\end{align*}
	where we use the fact that the variance of a Bernoulli random variable is always upper bounded by its expectation.
	In Proposition~\ref{prop:con}, we have shown that for $ u_r \beta\le \beta^\ast$, there is a set $\cG_{\con}$ such that (i) $P(G \in \cG_{\con}) = 1-o(1)$ and (ii) for every $G\in \cG_{\con}$, 
	$
	\sum_{i=1}^n \exp\big(k u_r \beta (A^r_i-A^0_i) \big)
	=(1+o(1)) n^{g( u_r \beta)} .
	$
    Since the size of $[n]_0$ is $1/k$ of $[n]$, for every $G\in \cG_{\con}$, 
	$
	\sum_{i\in[n]_0} \exp\big(ku_r \beta (A^r_i-A^0_i) \big)
	=\Theta (n^{g(u_r \beta)}) .
	$
	Therefore, for every $G\in\cG_{\good}\cap\cG_{\con}$, 
	\begin{align*}
	& E \Big[ \sum_{i\in[n]_+}\underline{S}_i \Big] = \Theta(\sum_{r=1}^{k-1}n^{g(u_r \beta)}) , \quad
	\Var \Big( \sum_{i\in[n]_+}\underline{S}_i \Big) = O(\sum_{r=1}^{k-1} n^{g(u_r\beta)}) , \\
	& E \Big[ \sum_{i\in[n]_+}\overline{S}_i \Big] = \Theta(\sum_{r=1}^{k-1}n^{g(u_r\beta)}) , \quad
	\Var \Big( \sum_{i\in[n]_+}\overline{S}_i \Big) = O(\sum_{r=1}^{k-1} n^{g(u_r\beta)}) 
	\end{align*}
	conditioning on the event $\{\sigma^{(j)}\in  \Lambda(G, z)
	\text{~and~} \dist(\sigma^{(j)}, X) \le n^\theta
	\text{~for all~} j\in[m]\}$. Since $g(u_r\beta)> 0$ for all $u_r\beta< \beta^\ast$, by Chebyshev's inequality we know that both $\sum_{i\in[n]_+}\underline{S}_i=\Theta(\sum_{r=1}^{k-1}n^{g(u_r\beta)})$ and
	$\sum_{i\in[n]_+}\overline{S}_i=\Theta(\sum_{r=1}^{k-1} n^{g(u_r\beta)})$ with probability $1-o(1)$ conditioning on the event $\{\sigma^{(j)}\in  \Lambda(G, z)
	\text{~and~} \dist(\sigma^{(j)}, X) \le n^\theta
	\text{~for all~} j\in[m]\}$.
	Since $\underline{S}_i\le \phi_i\le \overline{S}_i$ for all $i\in[n]$, we also have $T_0=\Theta(\sum_{r=1}^{k-1}n^{g(u_r\beta)})$ with probability $1-o(1)$ conditioning on the event $\{\sigma^{(j)}\in  \Lambda(G, z)
	\text{~and~} \dist(\sigma^{(j)}, X) \le n^\theta
	\text{~for all~} j\in[m]\}$. Combining this with \eqref{eq:ljrc}, we obtain that for every $G\in\cG_{\good}\cap\cG_{\con}$,
	$T_0=\Theta(\sum_{r=1}^{k-1} n^{g(u_r\beta)})$ with probability $1-o(1)$ conditioning on $\dist(\sigma^{(j)}, X) \le n/k$ for all $j\in[m]$. Finally, the lemma follows from $P(G\in\cG_{\good}\cap\cG_{\con})=1-o(1)$.
\end{proof}
\begin{lemma}\label{lem:mk}
	Let $m,k$ be integers and $k\geq 2, m\geq 1$, then
	\begin{equation}\label{eq:m1}
	m  \leq  k \lfloor {m + k -1 \over k} \rfloor 
	\end{equation}
\end{lemma}
\begin{proof}
   Let $m = kr + t$ where $0\leq t \leq k-1$.
   Then Equation \eqref{eq:m1} is equivalent with $ t \leq k\lfloor 1 +  \frac{t-1}{k} \rfloor $.
   When $ t = 0$, we have the equality. For $1\leq t \leq k-1$, we have $t < k \leq k\lfloor 1 +  \frac{t-1}{k} \rfloor$
   Therefore, Equation \eqref{eq:m1} holds.
\end{proof}
Using the above three lemmas, we obtain the following proposition
\begin{proposition}
	Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$. 
	Let 
	$$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k,a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
	$$
	If $\lfloor\frac{m+k-1}{k} \rfloor \beta<\beta^\ast$, then no algorithm can recover $X$ from the samples with constant success probability, i.e., the success probability of any algorithm is $o(1)$.
\end{proposition}
\begin{proof}
	From Lemma \ref{lem:mk} we have $m-(k-1)\lfloor \frac{m+k-1}{k}  \rfloor \le \lfloor \frac{m+k-1}{k}  \rfloor$. If $\lfloor \frac{m+k-1}{k}  \rfloor\beta<\beta^\ast$, then $(m-(k-1)\lfloor \frac{m+k-1}{k}  \rfloor) \beta<\beta^\ast$.
	Now pick a vector $v\in W^m$ such that it has $\lfloor \frac{m+k-1}{k} \rfloor$ coordinates being $\omega^r$ for $r=0, \dots, k-2$
	and $(m-(k-1)\lfloor \frac{m+k-1}{k}  \rfloor)$ coordinates being $\omega^{k-1}$.
	
	By Lemma~\ref{lm:cvs}, with probability $1-o(1)$, $T_r$ is $\omega(1)$ for $r=0, \dots, k-1$. Therefore, we can find $\omega(1)$ pairs of $i_1, \dots, i_k \in[n]$ satisfying the following two conditions: (1) $\sigma_{i_r}^{(j)}=\sigma_{i_1}^{(j)}$ for all $j\in[m], r\in[k]$ and (2) $\{X_{i_1}, \dots, X_{i_k} \} = \{1, \omega, \dots, \omega^{k-1} \}$. Then by Lemma~\ref{lm:qq}, it is not possible to distinguish $X$ from $X^{(\sim\{I\}, v)}$ for $\omega(1)$ pairs of $i_1, \dots,i_k$. Therefore, the success probability of any recovery algorithm is $o(1)$.
\end{proof}
\appendix

\section{Auxiliary propositions used in Section~\ref{sect:struct}}\label{ap:um}



We first prove a tight estimate of $P(A^1_i-A^0_i = t\log(n))$ for $t=\Theta(1)$. Note that \eqref{eq:upba} gives an upper bound on $P(A^1_i-A^0_i \ge t\log(n))$ for $t\in [\frac{1}{k}(b-a), 0]$, so it is also an upper bound of $P(A^1_i-A^0 = t\log(n))$. Below we prove that the upper bound in \eqref{eq:upba} is in fact tight up to a $\Theta(1/ \sqrt{\log(n)})$ factor for all $t=\Theta(1)$ such that $t\log(n)$ is an integer.

We use $f(n)=\Theta(g(n))$ and $f(n)\asymp g(n)$ interchangeably if there is a constant $C>0$ such that $C^{-1}g(n)\le f(n)\le C g(n)$ for large enough $n$.

\begin{proposition}  \label{prop:99}
For any $t$ such that $t\log(n)$ is an integer and $|t|<100a$,
\begin{equation} \label{eq:ly}
\begin{aligned}
& P(A^1_i-A^0_i = t\log(n))  \\
\asymp & \frac{1} {\sqrt{\log(n)}} \exp\Big(\log(n)
\Big(\sqrt{t^2+\frac{4ab}{k}} -t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big) -\frac{a+b}{k}  \Big)\Big) .
\end{aligned}
\end{equation}
\end{proposition}


\begin{proof}
Since
$$
P(A^1_i-A^0_i = t\log(n))
= \sum_{s\log(n)=0}^{n/k}
P(A^1_i = s\log(n)) P(A^0_i=(s-t)\log(n)) ,
$$
we first calculate tight estimates of $P(A^1_i = s\log(n))$ and $P(A^0_i=(s-t)\log(n))$. (The summation $\sum_{s\log(n)=0}^{n/k}$ in the above equation means that the quantity $s\log(n)$ ranges over all integer values from $0$ to $n/k$.)
By Lemma~\ref{lm:bmd} in Appendix~\ref{ap:6}, we only need to focus on the regime where both $|s|$ and $|t|$ are bounded from above by some (large) constants, e.g., $100a$.
Therefore,
\begin{align*}
& P(A^1_i = s\log(n)) \\
= & \binom{n/k}{s\log(n)}
\Big( \frac{b\log(n)}{n} \Big)^{s\log(n)}
\Big( 1- \frac{b\log(n)}{n} \Big)^{n/k-s\log(n)}  \\
= & (1+o(1))
\frac{(n/k)^{s\log(n)}}{(s\log(n))!} \Big( \frac{b\log(n)}{n} \Big)^{s\log(n)}
\exp \Big(-\frac{b\log(n)}{k} \Big)  \\
= & (1+o(1))
\frac{1} {(s\log(n))!} \Big( \frac{b\log(n)}{k} \Big)^{s\log(n)}
\exp \Big(-\frac{b\log(n)}{k} \Big)  \\
\overset{(a)}{=} & (1+o(1))
\frac{1} {\sqrt{2\pi s\log(n)}}
\Big(\frac{e}{s\log(n)} \Big)^{s\log(n)}
\Big( \frac{b\log(n)}{k} \Big)^{s\log(n)}
\exp \Big(-\frac{b\log(n)}{k} \Big)  \\
= & (1+o(1))
\frac{1} {\sqrt{2\pi s\log(n)}}
\Big( \frac{ e b }{ks} \Big)^{s\log(n)}
\exp \Big(-\frac{b\log(n)}{k} \Big) \\
= & (1+o(1))
\frac{1} {\sqrt{2\pi s\log(n)}}
\exp\Big( \log(n) \Big( 
s+ s\log(b)-s\log(k)-s\log(s)-\frac{b}{k}
\Big)\Big) ,
\end{align*}
where $(a)$ follows from Stirling's formula.
Similarly, when $s > t$,
\begin{align*}
& P(A^0_i=(s-t)\log(n)) \\
= & (1+o(1))
\frac{1} {\sqrt{2\pi (s-t)\log(n)}}
\exp\Big( \log(n) \Big( 
 (s-t)(1+\log(a)-\log(k)-\log(s-t))-\frac{a}{k}
\Big)\Big)
\end{align*}
Define a function
$$
h_t(s):=(2s-t)(1-\log(k))
+s\log\frac{ab}{s(s-t)}
+t\log\frac{s-t}{a} -\frac{a+b}{k} .
$$
Then for $s>\max(0,t)$,
$$
P(B_i = s\log(n)) P(A_i=(s-t)\log(n))
= (1+o(1)) \frac{1} {2\pi \log(n)\sqrt{s (s-t)}}
\exp(h_t(s) \log(n)) .
$$
Therefore, for $t$ such that $t\log(n)$ is an integer, we have
\begin{equation} \label{eq:gj}
\begin{aligned}
& P(A^1_i-A^0_i = t\log(n))  \\
= & (1+o(1)) \sum_{s\log(n)=\max(0,t\log(n))}^{n/k}
\frac{1} {2\pi \log(n)\sqrt{s (s-t)}}
\exp(h_t(s) \log(n)) .
\end{aligned}
\end{equation}
In order to estimate this sum, we need to analyze the function $h_t(s)$. Its first and second derivatives are
$h_t'(s)=\log\frac{ab}{k^2s(s-t)}$
and $h_t''(s)=-\frac{1}{s}-\frac{1}{s-t}<0$, so $h_t(s)$ is a concave function and takes maximum at $s^\ast$ such that $h_t'(s^\ast)=0$.
Simple calculations show that
$s^\ast=(t+\sqrt{t^2+4ab/k^2})/2>\max(0,t)$ and
$$
h_t(s^\ast)
= \sqrt{t^2+4ab/k^2} +t\big(\log(\sqrt{t^2+4ab/k^2}-t)-\log(2a) \big) -\frac{a+b}{k} .
$$
By Lemma~\ref{lm:bmd} in Appendix~\ref{ap:6}, both $|s|$ and $|t|$ are upper bounded by some (large) constants
with probability $1-o(n^{-10})$. Therefore, the sum on the right-hand side of \eqref{eq:gj} is concentrated around a small neighborhood of $s^\ast$. In this neighborhood, we have
$$
\frac{1} {2\pi \log(n)\sqrt{s (s-t)}} = \Theta(\frac{1} {\log(n)}) .
$$
Therefore, in order to prove this proposition, we only need to show that
\begin{equation} \label{eq:jh}
\sum \exp(h_t(s) \log(n))
= \Theta\Big(\sqrt{\log(n)} \exp(h_t(s^\ast) \log(n))\Big) ,
\end{equation}
where the summation is taken over this small neighborhood.
We will show that $\exp(h_t(s) \log(n))$ varies by a constant factor within a window of length $\Theta(1/\sqrt{\log(n)})$ around $s^\ast$, and
then drops off geometrically fast beyond that window. First observe that $h_t(s)\approx h_t(s^\ast) - h_t''(s^\ast) (s-s^\ast)^2$ in the neighborhood of $s^\ast$, so when $|s-s^\ast|=\Theta(1/\sqrt{\log(n)})$, we have $h_t(s^\ast) \log(n) - h_t(s) \log(n) = \Theta(1)$.
Also note that when $s$ is in the range $(s^\ast-\Theta(1/\sqrt{\log(n)}) , s^\ast+\Theta(1/\sqrt{\log(n)}))$, the quantity $s\log(n)$ takes $\Theta(\sqrt{\log(n)})$ integer values.
Now pick some constant $c>0$. By the above analysis we have
$$
\sum_{s^\ast\log(n) - c\sqrt{\log(n)} \le s\log(n) \le s^\ast\log(n) + c\sqrt{\log(n)}}
\exp(h_t(s) \log(n))
= \Theta\Big(\sqrt{\log(n)} \exp(h_t(s^\ast) \log(n))\Big) .
$$
For $s>s^\ast+c/\sqrt{\log(n)}$, we use the fact that concave functions are always bounded from above by its tangent lines. Therefore,
\begin{align*}
h_t(s) & \le h_t(s^\ast+c/\sqrt{\log(n)})
+ h_t'(s^\ast+c/\sqrt{\log(n)}) 
(s- s^\ast - c/\sqrt{\log(n)}) \\
& \le h_t(s^\ast) - \frac{c'}{\sqrt{\log(n)}}
(s- s^\ast - c/\sqrt{\log(n)}) ,
\end{align*}
where we use the fact that $h_t'(s^\ast+c/\sqrt{\log(n)}) = \Theta(1/\sqrt{\log(n)})$, and $c'$ is another constant that depends on $c$.
Therefore,
\begin{align*}
& \sum_{s\log(n) > s^\ast\log(n) + c\sqrt{\log(n)}}
\exp(h_t(s) \log(n))  \\
\le & \sum_{s\log(n) > s^\ast\log(n) + c\sqrt{\log(n)}}
\exp \Big( h_t(s^\ast) \log(n) 
-\frac{c'}{\sqrt{\log(n)}}
(s\log(n)- s^\ast \log(n)- c \sqrt{\log(n)}) \Big)   \\
= & \exp(h_t(s^\ast) \log(n))
\sum_{j>0} \exp \Big( 
-\frac{c'}{\sqrt{\log(n)}} j \Big) \\
\le & \exp(h_t(s^\ast) \log(n))
\frac{1}{1-\exp(-c'/ \sqrt{\log(n)})} \\
= & O\Big(\sqrt{\log(n)} \exp(h_t(s^\ast) \log(n))\Big) .
\end{align*}
The sum over $s\log(n) < s^\ast\log(n) - c\sqrt{\log(n)}$ can be bounded in the same way. Thus we have shown \eqref{eq:jh}, and this completes the proof of the proposition.
\end{proof}

Let $\cG_1:=\{G:A^r_i-A^0_i<0\text{~for all~}i\in[n]\text{~and~} 1\leq r \leq k-1\}$. By \eqref{eq:tD}, we have $P(G\in\cG_1)=1-o(1)$. In the proposition below, we will prove that if $0<\beta<\frac{1}{2k}\log\frac{a}{b}$,
then the conditional expectation $E \Big[ \sum_{i=1}^n  \exp\big(2\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big]$ is very close to the unconditional expectation $E \Big[ \sum_{i=1}^n  \exp\big(2\beta (A^r_i-A^0_i) \big) \Big]$. On the other hand, if $\beta\ge\frac{1}{2k}\log\frac{a}{b}$, then the conditional expectation is $O(n^{\tilde{g}(\beta)})$ while the unconditional one is $\Theta(n^{g(\beta)})$. Since $\tilde{g}(\beta)<g(\beta)$ when $\beta>\frac{1}{2k}\log\frac{a}{b}$, the conditional expectation is much smaller than the unconditional one in this case.
\begin{proposition}  \label{prop:df}
Assume that $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
If $0<\beta<\frac{1}{2k}\log\frac{a}{b}$, then
$$
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big] 
= (1+o(1)) E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big]
= (1+o(1)) n^{g(\beta)}  .
$$
If $\beta\ge\frac{1}{2k}\log\frac{a}{b}$, then
\begin{align*}
E \Big[ \sum_{i=1}^n  \exp\big(2\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big] 
= O(n^{\tilde{g}(\beta)})  .
\end{align*}
\end{proposition}
\begin{proof}
We first calculate the unconditional expectation:
By writing $A^r_i$ and $A^0_i$ as sums of independent Bernoulli random variables, we obtain that
\begin{align*}
E[e^{k\beta(A^r_i-A^0_i)}]
& =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^{k\beta} \Big)^{n/k}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-k\beta} \Big)^{n/k-1}  \\
& = 
\exp\Big(\frac{\log(n)}{k} ( a e^{-k\beta}+b e^{k\beta} -a-b )
+o(1) \Big) \\
& = (1+o(1)) n^{g(\beta)-1} .
\end{align*}
Therefore, for all $\beta$ we have
\begin{equation} \label{eq:lb}
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big]
= (1+o(1)) n^{g(\beta)}  .
\end{equation}
Now let us switch to conditional expectation. In light of \eqref{eq:lb}, for the case of $0<\beta<\frac{1}{2k}\log\frac{a}{b}$ we only need to prove that the conditional expectation is very close to the unconditional expectation.
To that end, we first reprove a weaker version of \eqref{eq:lb} using Proposition~\ref{prop:99}. This will help us estimate the difference between the conditional and unconditional expectations.

Define $D_r(G,t):=|\{i\in[n]:A^r_i-A^0_i= t\log(n)\}|$.
Similarly to \eqref{eq:fd}, we have
\begin{equation} \label{eq:s1}
\sum_{i=1}^n \exp\big(k\beta (A^r_i-A^0_i) \big) 
=  \sum_{t\log(n)=-n/k}^{n/k}
D_r(G,t) \exp\big(k\beta t \log(n) \big) .
\end{equation}
By definition, $D(G,t)=\sum_{i=1}^n \mathbbm{1}[A^r_i-A^0_i= t\log(n)]$, so
$E[D(G,t)]=n P(A^r_i-A^0_i= t\log(n))$. Therefore, by Proposition~\ref{prop:99} and the definition of function $f_{\beta}(t)$ in \eqref{eq:gt},
$$
E[D(G,t)
\exp\big(k\beta t \log(n) \big)]
\asymp \frac{1} {\sqrt{\log(n)}} \exp( f_{\beta}(t) \log(n) ) .
$$
As a consequence,
\begin{equation}  \label{eq:wf1}
\begin{aligned}
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big]     
& =  \sum_{t\log(n)=-n/k}^{n/k}
E \big[ D(G,t) \exp\big(k\beta t \log(n) \big) \big]  \\
& \asymp  \frac{1} {\sqrt{\log(n)}} \sum_{t\log(n)=-n/k}^{n/k}
 \exp( f_{\beta}(t) \log(n) ) .
\end{aligned}
\end{equation}
By the proof of Lemma~\ref{lm:tus}, $f_{\beta}(t)$ is a concave function and takes maximum at
$t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k}$. 
Similarly to the analysis of \eqref{eq:jh}, $\exp( f_{\beta}(t) \log(n))$ varies by a constant factor within a window of length $\Theta(1/\sqrt{\log(n)})$ around $t^\ast$, and
then drops off geometrically fast beyond that window. Since $t\log(n)$ takes $\Theta(\sqrt{\log(n)})$ integer values when $t$ takes values in such a window, we have
\begin{equation} \label{eq:wf2}
\sum_{t\log(n)=-n/k}^{n/k}
 \exp( f_{\beta}(t) \log(n) )
 \asymp \sqrt{\log(n)}
 \exp( f_{\beta}(t^\ast) \log(n) ) .
\end{equation}
By the proof of Lemma~\ref{lm:tus}, we have $f_{\beta}(t^\ast)=g(\beta)$. Taking this into \eqref{eq:wf1} and \eqref{eq:wf2}, we obtain that
$$
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big]
= \Theta (n^{g(\beta)}) .
$$
Now let us consider the conditional expectation. When conditioning on the event $G\in\cG_1$, we have $D_r(G,t)=0$ for all $t\ge 0$.
In this case, the range of sum in both \eqref{eq:s1} and \eqref{eq:wf1} reduces from $[-n/k,n/k]$ to
$[-n/k,0)$. By Lemma~\ref{lm:5t} below, we have $P(A^r_i-A^0_i= t\log(n)~|~G\in\cG_1)= (1+o(1))P(A^r_i-A^0_i= t\log(n))$ for $t<0$, and so
$E[D_r(G,t)|G\in\cG_1]=(1+o(1))E[D_r(G,t)]$ for $t<0$. Therefore,
$$
E\Big[ \sum_{i=1}^n \exp\big(2\beta (A^r_i-A^0_i) \big) \Big| G\in\cG_1 \Big]
= (1+o(1)) \sum_{t\log(n)=-n/k}^{-1}
E\big[ D_r(G,t) \exp\big(2\beta t \log(n) \big) \big] .
$$
From the analysis of \eqref{eq:wf1}, we know that 
\begin{align*}
& \sum_{t\log(n)=-n/k}^{n/k}
E\big[ D_r(G,t) \exp\big(k\beta t \log(n) \big) \big]  \\
= & (1+o(1)) \sum_{t\log(n)=t^\ast\log(n)-\Theta(\sqrt{\log(n)})}^{t^\ast\log(n)+\Theta(\sqrt{\log(n)})}
E\big[ D_r(G,t) \exp\big(k\beta t \log(n) \big) \big] .
\end{align*}
Therefore, if $t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k}<0$,
or equivalently $0<\beta<\frac{1}{2k}\log\frac{a}{b}$, then
$$
\sum_{t\log(n)=-n/k}^{n/k}
E\big[ D_r(G,t) \exp\big(k\beta t \log(n) \big) \big] = (1+o(1))
\sum_{t\log(n)=-n/k}^{-1}
E\big[ D_r(G,t) \exp\big(k\beta t \log(n) \big) \big] ,
$$
i.e.,
\begin{equation} \label{eq:bz}
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big] 
= (1+o(1)) E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big] .
\end{equation}
On the other hand,
if $t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k}\ge 0$, or equivalently $\beta\ge\frac{1}{2k}\log\frac{a}{b}$, then
\begin{equation}  \label{eq:wq}
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big]   
\asymp  \frac{1} {\sqrt{\log(n)}} \sum_{t\log(n)=-n/k}^{-1}
 \exp( f_{\beta}(t) \log(n) ) .
\end{equation}
Since $f_{\beta}(t)$ is concave, it is an increasing function when $t<t^\ast$.
Therefore, $f_{\beta}(t)<f_{\beta}(0)=g(\frac{1}{2k}\log\frac{a}{b})=\tilde{g}(\beta)$ for $\beta\ge\frac{1}{2k}\log\frac{a}{b}$.
Similarly to the analysis of \eqref{eq:jh} and \eqref{eq:wf1},
$\exp( f_{\beta}(t) \log(n))$ varies by a constant factor within a window of length $O(1/\sqrt{\log(n)})$ around $t=0$, and
then drops off geometrically fast beyond that window. As a consequence,
\begin{align*}
\sum_{t\log(n)=-n/k}^{-1}
 \exp( f_{\beta}(t) \log(n) )
=   O(\sqrt{\log(n)}) \exp(\tilde{g}(\beta) \log(n)) .
\end{align*}
Taking this into \eqref{eq:wq} completes the proof of the proposition.
\end{proof}

The following corollary follows immediately from Proposition~\ref{prop:df} and \eqref{eq:lb}:
\begin{corollary} \label{cr:yy}
For all $\beta>0$,
\begin{align*}
& E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big)  \Big] 
= \Theta(n^{g(\beta)}),  \\
& E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big] 
= O(n^{\tilde{g}(\beta)})  .
\end{align*}
\end{corollary}



\begin{remark}
From the proof of \eqref{eq:bz}, we can see that if $0<\beta<\frac{1}{2k}\log\frac{a}{b}$, then
\begin{equation} \label{eq:pl}
E \big[  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\big|~ G\in\cG_1 \big] 
= (1+o(1)) E \big[  \exp\big(k\beta (A^r_i-A^0_i) \big) \big] ,
\end{equation}
i.e., we can remove the summation in \eqref{eq:bz}. We will use this in the proof of Proposition~\ref{prop:con}.
\end{remark}



\begin{lemma}  \label{lm:5t}
Let $\cG_1:=\{G:A^r_i-A^0_i<0\text{~for all~}i\in[n]\text{~and~}1\leq r \leq k-1\}$. Then $P(A^r_i-A^0_i= t\log(n)~|~G\in\cG_1)= (1+o(1))P(A^r_i-A^0_i= t\log(n))$ for all $t<0$ such that $t\log(n)$ is an integer.
\end{lemma}

\begin{proof}
Note that 
$$
P(A^r_i-A^0_i= t\log(n)~|~G\in\cG_1)
=\frac{P \big(A^r_j-A^0_j<0\text{~for all~} j\in[n]\setminus\{i\}, A^r_i-A^0_i= t\log(n) \big)}{P(G\in\cG_1)} .
$$
By \eqref{eq:tD}, we have $P(G\in\cG_1)=1-o(1)$, so
\begin{align*}
& P(A^r_i-A^0_i= t\log(n)~|~G\in\cG_1) \\
= & (1+o(1)) P \big(A^r_j-A^0_j<0\text{~for all~} j\in[n]\setminus\{i\}, A^r_i-A^0_i= t\log(n) \big) \\
= & (1+o(1)) P \big(A^r_j-A^0_j<0\text{~for all~} j\in[n]\setminus\{i\} ~\big|~ A^r_i-A^0_i= t\log(n) \big) P(A^r_i-A^0_i= t\log(n)) .
\end{align*}
Therefore, to prove the lemma we only need to show that $P \big(A^r_j-A^0_j<0\text{~for all~} j\in[n]\setminus\{i\} ~\big|~ A^r_i-A^0_i= t\log(n) \big)=1-o(1)$. 

For $j\in[n]\setminus\{i\}$, define $\xi_{ij}=\xi_{ij}(G):=\mathbbm{1}[\{i,j\}\in E(G)]$ as the indicator function of the edge $\{i,j\}$ connected in graph $G$. We also define 
$$
A'^r_j=\left\{
\begin{array}{cl}
  A^r_j-\xi_{ij}   & \mbox{if~} X_i\neq X_j \\
  A^r_j   &   \mbox{if~} X_i= X_j
\end{array}
\right.
\quad \text{and} \quad
A'^0_j=\left\{
\begin{array}{cl}
  A^0_j   & \mbox{if~} X_i\neq X_j \\
  A^0_j-\xi_{ij}   &   \mbox{if~} X_i= X_j
\end{array}
\right. .
$$
Then $A'^r_j-A'^0_j$ differs from $A^r_j-A^0_j$ by at most $1$.
Therefore, $A'^r_j-A'^0_j<-1$ implies that $A^r_j-A^0_j<0$,
and so $P \big(A^r_j-A^0_j<0\text{~for all~} j\in[n]\setminus\{i\} ~\big|~ A^r_i-A^0_i= t\log(n) \big) \ge P \big(A'^r_j-A'^0_j<-1 \text{~for all~} j\in[n]\setminus\{i\} ~\big|~ A^r_i-A^0_i= t\log(n) \big)$.
Now we only need to prove that the right-hand side is $1-o(1)$.
Also note that the two sets of random variables $\{A'^r_j,A'^0_j:j\in[n]\setminus\{i\}\}$ and $\{A^r_i,A^0_i\}$ are independent,
so $P \big(A'^r_j-A'^0_j<-1 \text{~for all~} j\in[n]\setminus\{i\} ~\big|~ A^r_i-A^0_i= t\log(n) \big)=P \big(A'^r_j-A^0_j<-1 \text{~for all~} j\in[n]\setminus\{i\}  \big)$.
By definition, we have
$A'^r_j\sim\Binom(\frac{n}{k}-\Theta(1),b\log(n)/n)$
and $A'^0_j\sim\Binom(\frac{n}{k}-\Theta(1),a\log(n)/n)$ for all $j\in[n]\setminus\{i\}$.
Then following exactly the same proof\footnote{First use Chernoff bound as we did in Proposition~\ref{prop:cher} and then use the union bound.} as that of \eqref{eq:tD}, we have
$$
P \big(A'^r_j-A'^0_j <-1 \text{~for all~} j\in[n]\setminus\{i\}  \big)
\ge 1- n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)} = 1-o(1).
$$
This completes the proof of the lemma.
\end{proof}
Using the same techniques as in Lemma \ref{lm:5t}, we have the following corollary:
\begin{corollary}\label{lem:ucBA}
Let $\cG_1:=\{G:A^r_i-A^0_i<0\text{~for all~}i\in[n] \text{ and } 1\leq r \leq k-1\}$. Suppose $i\neq j$, then $P(A^r_i-A^0_i + A^r_j - A^0_j= t\log(n)~|~G\in\cG_1)= (1+o(1))P(A^r_i-A^0_i + A^r_j - A^0_j= t\log(n))$ for all $t<0$ such that $t\log(n)$ is an integer.
\end{corollary}
\begin{lemma}\label{lem:BijG}
\begin{equation} 
E \big[  \exp\big(k\beta (A^r_i-A^0_i + A^r_j - A^0_j) \big) ~\big|~ G\in\cG_1 \big] 
= (1+o(1)) E \big[  \exp\big(k\beta (A^r_i-A^0_i + A^r_j - A^0_j) \big) \big] ,
\end{equation}
\end{lemma}
\begin{proof}
First we have
\begin{align}
E \big[  \exp\big(k\beta (A^r_i - A^0_i + A^r_j - A^0_j) \big) ~\big|~ G\in\cG_1 \big] 
&= \sum_{t=-2\frac{n}{k}}^{-2} P(A^r_i - A^0_i + A^r_j - A^0_j = t\log n | G \in \cG_1) \exp(k\beta t \log n) \notag \\
\text{ Corollary \ref{lem:ucBA} implies }&= (1+o(1))\sum_{t=-2\frac{n}{k}}^{-2} P(A^r_i - A^0_i + A^r_j - A^0_j = t\log n) \exp(k\beta t \log n)
\end{align}
On the other hand, we suppose $X_i \neq X_j$ and we decompose $A^r_j = A'^r_j + \xi_{ij}, A^r_i = A'^r_i + \xi_{ij}$ where $\xi_{ij}$ is an indicator function of $\{i,j\} \in E(G)$. Then $A'^r_j, A'^r_i, A^r_j, A^r_i, \xi_{ij}$ are independent.
$\xi_{ij} \sim Bern(\frac{b\log n}{n})$.
Then we have
\begin{align*}
E \big[  \exp\big(k\beta (A^r_i - A^0_i + A^r_j - A^0_j ) \big) \big] & = E[\exp(2k\beta \xi_{ij})] E[\exp(k\beta (A'^r_i - A^r_i)]
E[\exp(k\beta (A'^r_j - A^r_j)] \\
& = (1+o(1))E[\exp(k\beta(A'^r_i - A^r_i))] E[\exp(k\beta(A'^r_j - A^r_j))]
\end{align*}
From Equation \eqref{eq:pl} we have
$$
E[\exp(k\beta(A^r_i - A^0_i))] = (1+o(1)) \sum_{t=-n/k}^{-1} P(A^r_i - A^0_i = t \log n)E[\exp(k\beta t \log n)]
$$
we have
\begin{align*}
E \big[  \exp\big(k\beta (A^r_i - A^0_i + A^r_j - A^0_j ) \big) \big] & = (1+o(1))
\sum_{t_1=-n/k}^{-1} P(A'^r_i - A^r_i = t_1 \log n)E[\exp(k\beta t_1 \log n)] \\
& \cdot
\sum_{t_2=-n/k}^{-1} P(A'^r_j - A^r_j = t_2 \log n)E[\exp(k\beta t_2 \log n)] \\
& = (1+o(1))  \sum_{t=-2\frac{n}{k}}^{-2} E[\exp(k\beta t \log n)]\\
& \cdot \sum_{\substack{t_1 + t_2 = t \\ t_1 < 0, t_2 < 0}}
P(A'^r_i - A^r_i = t_1 \log n) P(A'^r_j - A^r_j = t_2\log n)
\end{align*}
Since 
\begin{align*}
P(A^r_i - A^0_i + A^r_j - A^0_j = t\log n)
&= \sum_{\substack{t_1 + t_2 + t_3 = t\\ t_3 \in\{0, 1\}}} P(A'^r_i - A^r_i = t_1 \log n) P(A'^r_j - A^r_j = t_2 \log n) P(2\xi_{ij} = t_3 \log n) \\
&=(1+o(1)) \sum_{t_1 + t_2 = t} P(A'^r_i - A^r_i = t_1 \log n) P(A'^r_j - A^r_j = t_2 \log n)  \\
\end{align*}
By Equation \ref{eq:tD}, $P(G\in G_1) = 1-o(1)$. The above summation can be further restricted to $t_1 < 0, t_2 < 0$. Thus Lemma \ref{lem:BijG} follows. 
\end{proof}





\section{Auxiliary lemmas used in Section~\ref{sect:direct}}\label{ap:6}

\begin{lemma} \label{lm:bq}
	For $0<\theta<1$,
	\begin{align}
	& P_{\SIBM}(\sigma_i \neq X_i
	\big| \dist(\sigma,X) \le n^\theta) \le k n^{\theta-1}
	\quad \text{for all~} i\in[n] , \label{eq:l1}\\
	& P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI}
	~\big| \dist(\sigma,X) \le n^\theta) \le \Big(\frac{n^\theta}{n/k - |\tilde{\cI}|}
	\Big)^{|\tilde{\cI}|}
	\quad \text{for all~} \tilde{\cI}\subseteq [n] .   \label{eq:l2}
	\end{align}
\end{lemma}
\begin{proof}
	Define $\dist_j(\sigma,X):=|\{i\in[n]:X_i=\omega^j, \sigma_i \neq X_i\}|$ for $j \in \{0, \dots, k-1\}$. Clearly, $\dist(\sigma,X)=\sum_{j=0}^{k-1} \dist_j(\sigma,X)$.
	
	Inequality \eqref{eq:l1} follows immediately from the following equality:
	$$
	P_{\SIBM}(\sigma_i \neq X_i |
	\dist_j(\sigma,X)=u_j,
	j=0,\dots,k-1) 
	= k u_r / n \textrm{ where } X_i = \omega^r
	$$
	Without loss of generality,
	we only prove the case of $X_i=1 (r=0)$, and
	we need the following definition for the proof of this equality:
	For $\cI\subseteq[n]$, define $\cI_j:=\{i\in\cI:X_i=\omega^j\}$. Then
	\begin{align*}
	& P_{\SIBM}(\sigma_i \neq X_i |
	\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)  \\
	= & \frac{P_{\SIBM}(\sigma_i \neq X_i ,
		\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)}
	{P_{\SIBM}(
		\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)} \\
	= & \frac{\sum_{\cI:i\in\cI_0,|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }
	{\sum_{\cI: |\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) } \\
	\overset{(a)}{=} & \frac{\binom{n/k-1}{u_0 -1}}
	{\binom{n/k}{u_0} }
	= ku_0/n ,
	\end{align*}
	where equality (a) follows from Lemma~\ref{lm:cc} below.
	
	Similarly, inequality \eqref{eq:l2} follows from the following inequality:
	For $u_j\geq |\tilde{\cI}_j|, j=0, \dots, k-1$,
	\begin{align*}
	& P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI} ~ \big|
	\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)  \\
	= & \frac{P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI} ,
		\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)}{P_{\SIBM}(
		\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)} \\
	= & \frac{\sum_{\cI:\tilde{\cI}_j\subseteq\cI_j,
			|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }
	{\sum_{\cI:|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }  \\
	\overset{(a)}{=} & \prod_{j=0}^{k-1} \frac{\binom{n/k-|\tilde{\cI}_j|}{u_j -|\tilde{\cI}_j|} }
	{\binom{n/k}{u_j} }
	< \prod_{j=0}^{k-1}\Big(\frac{u_j}{n/k- |\tilde{\cI}_j|} \Big)^{|\tilde{\cI}_j|}
	\\
	< & \Big(\frac{\sum_{j=0}^{k-1} u_j}{n/k- |\tilde{\cI}|} \Big)^{|\tilde{\cI}|}  ,
	\end{align*}
	where equality (a) again follows from Lemma~\ref{lm:cc} below.
\end{proof}







\begin{lemma} \label{lm:cc}
	Let $\cI,\cI'\subseteq[n]$ be two subsets such that $|\cI_j|=|\cI_j'|$ for $j=0,\dots,k-1$. 
	Then $P_{\SIBM}(\sigma=X^{(\sim\cI)}) = P_{\SIBM}(\sigma=X^{(\sim\cI')})$.
\end{lemma}
\begin{proof}
Let $\cG_{[n]}$ be the set consisting of all the graphs with vertex set $[n]$.
A permutation $\pi\in S_n$ on the vertex set $[n]$ also induces a permutation on $\cG_{[n]}$: For $G\in\cG_{[n]}$, define the graph $\pi(G)\in\cG_{[n]}$ as the graph with the edge set $E(\pi(G))$ satisfying that $\{\pi(i),\pi(j)\}\in E(\pi(G))$ if and only if $\{i,j\}\in E(G)$.
It is easy to see that for any $\pi\in S_n$ and any $G\in\cG_{[n]}$,
$$
Z_G(\alpha,\beta)
=Z_{\pi(G)}(\alpha,\beta),
$$
where $Z_G(\alpha,\beta)$ is defined in \eqref{eq:zg}.
Furthermore, 
if $X_i=X_{\pi(i)}$ for all $i\in[n]$, then for any graph $G\in\cG_{[n]}$, we have
$$
P_{\SSBM}(G)=P_{\SSBM}(\pi(G))  ,
$$
where $P_{\SSBM}$ is the distribution given in Definition~\ref{def:SSBM}.

Under the assumptions $|\cI_j|=|\cI_j'|$ for $j=0, \dots, k-1$,
it is easy to see that there exists a permutation $\pi$ on the vertex set $[n]$ satisfying the following two conditions: 
(i) $X_i=X_{\pi(i)}$ for all $i\in[n]$; 
(ii) $\pi(\cI)=\cI'$, i.e., $\pi(i)\in\cI'$ for all $i\in\cI$.
For such a permutation $\pi$, one can verify that
$$
X_i^{(\sim\cI, v)} = X_{\pi(i), v}^{(\sim\pi(\cI), v)}
= X_{\pi(i)}^{(\sim \cI', v)}
$$
for all $i\in[n]$ and vector $v$.
Therefore,
\begin{align*}
& P_{\SIBM}(\sigma=X^{(\sim\cI)})
=\sum_{v}\sum_{G\in\cG_{[n]}}
P_{\SSBM}(G) P_{\sigma|G}(\sigma=X^{(\sim\cI, v)}) \\
= & \sum_{v}\sum_{G\in\cG_{[n]}} P_{\SSBM}(G) \frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\beta\sum_{\{i,j\}\in E(G)} I(X_i^{(\sim\cI,v)},X_j^{(\sim\cI,v)})
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)}  I(X_i^{(\sim\cI,v)},X_j^{(\sim\cI,v)}) \Big)  \\
= & \sum_{v}\sum_{G\in\cG_{[n]}} P_{\SSBM}(G) \frac{1}{Z_G(\alpha,\beta)}  \\
& \hspace*{1in}
\exp\Big(\sum_{\{i,j\}} I(X_i^{(\sim\cI,v)},X_j^{(\sim\cI,v)})
\Big( \big( \beta+\frac{\alpha\log(n)}{n} \big) \mathbbm{1}[\{i,j\}\in E(G)]
-\frac{\alpha\log(n)}{n} \Big)
 \Big)  \\
= &\sum_v \sum_{G\in\cG_{[n]}} P_{\SSBM}(G) \frac{1}{Z_G(\alpha,\beta)}  \\
& \hspace*{0.6in}
\exp\Big(\sum_{\{i,j\}} I(X_{\pi(i)}^{(\sim \cI',v)}, X_{\pi(j)}^{(\sim \cI',v)})
\Big( \big( \beta+\frac{\alpha\log(n)}{n} \big) \mathbbm{1}[\{\pi(i),\pi(j)\}\in E(\pi(G))]
-\frac{\alpha\log(n)}{n} \Big)
 \Big)   \\
 = &\sum_v  \sum_{G\in\cG_{[n]}} P_{\SSBM}(G) \frac{1}{Z_G(\alpha,\beta)}  \\
& \hspace*{0.8in}
\exp\Big(\sum_{\{i,j\}} I(X_i^{(\sim \cI',v)}, X_j^{(\sim \cI',v)})
\Big( \big( \beta+\frac{\alpha\log(n)}{n} \big) \mathbbm{1}[\{i, j\}\in E(\pi(G))]
-\frac{\alpha\log(n)}{n} \Big)
 \Big)   \\
  = &\sum_v \sum_{G\in\cG_{[n]}} P_{\SSBM}(\pi(G)) \frac{1}{Z_{\pi(G)}(\alpha,\beta)}  \\
& \hspace*{0.8in}
\exp\Big(\sum_{\{i,j\}} I(X_i^{(\sim \cI',v)}, X_j^{(\sim \cI',v)})
\Big( \big( \beta+\frac{\alpha\log(n)}{n} \big) \mathbbm{1}[\{i, j\}\in E(\pi(G))]
-\frac{\alpha\log(n)}{n} \Big)
 \Big)  \\
= & \sum_{G\in\cG_{[n]}}
P_{\SSBM}(\pi(G)) P_{\pi(G)}(\sigma=X^{(\sim\cI')}) \\
= & P_{\SIBM}(\sigma=X^{(\sim\cI')}) .
\end{align*}
This completes the proof of the lemma.
\end{proof}
\begin{lemma}\label{lem:post_independent}
	Let $p_{ij}=\Pr(\{\{i,j\} \in E(G) \})$ be the prior probability, which equals $\frac{a\log n}{n}$ or $\frac{b\log n}{n}$ depending on $X$.
	The event $\{\{i,j\} \in E(G) \}$ are independent given $\sigma$ and the posterior probability is
	\begin{equation}
	\Pr(\{\{i,j\} \in E(G) \} | \sigma) = \frac{c p_{ij} }{1-p_{ij} + cp_{ij}} \text{ where  } c= \exp\Big((\beta + \frac{\alpha \log n}{n} ) I(\bar{\sigma}_i, \bar{\sigma}_j) \Big)
	\end{equation}
\end{lemma}
\begin{proof}
	Let $Y_{ij}$ be a Bernoulli random variable with $\Pr(Y_{ij} = 1) = \Pr(\{\{i,j\} \in E(G)\}) = p_{ij}$. $Y_{ij}$ represents whether there is an edge between node $i$ and $j$.
	The prior distribution for $Y:=\{Y_{ij}\}$ can be written as:
	$$
	\Pr(Y) = \prod_{i,j} p_{ij}^{y_{ij}} (1-p_{ij})^{1-y_{ij}}
	$$ 
	Using Equation \eqref{eq:isingma}, the posterior probability for $Y| \sigma$ can be written as
	$$
	\Pr(Y|\sigma) = C\prod_{ij} \exp((\beta + \frac{\alpha \log n}{n})I(\bar{\sigma_i}, \bar{\sigma_j} )y_{ij}) \Pr(Y)
	$$
	where $C$ is a constant irrelevant with $Y$.
	We can see from the joint distribution of $Y|\sigma$ that $Y_{ij} | \sigma$ are independent and each marginal distribution has the following form:
	$$
	\Pr(Y_{ij} | \sigma) = C_{ij} \exp((\beta + \frac{\alpha \log n}{n})I(\bar{\sigma_i}, \bar{\sigma_j} )y_{ij}) p_{ij}^{y_{ij}} (1-p_{ij})^{1-y_{ij}}
	$$
	Using the normalization condition for $Y_{ij} | \sigma $, we can compute $C_{ij} = \frac{1}{1-p_{ij} + p_{ij}c}$. From $\Pr(\{\{i,j\} \in E(G) \} | \sigma) =\Pr(Y_{ij}=1|\sigma)$ the proof is complete.
\end{proof}


\begin{lemma} \label{lm:bmd}
	Let $Y\sim \Binom(n + o(n), a\log(n)/n)$. Then for $r>8, a \geq 2$ and large enough $n$, we have
	$$
	P(Y\ge r a \log(n)) < n^{-r} .
	$$
\end{lemma}

\section{Extra Lemmas used in the proof}
\begin{lemma}
	For given $X, Y \in W^n$, suppose $|\{i | X_i = w^k \}| = \frac{n}{k}$.
	Let $f^*=\arg\min_{f \in \Gamma } d(f(Y), X)$, then $d(X,f^*(Y)) \leq \frac{k-1}{k}n$.
\end{lemma}
\begin{proof}
	We need to show that there exists a function $g\in \Gamma$ such that $\sum_{i=1}^n \mathbbm{1}[X_i=g(Y)_i] \geq \frac{n}{k}$.
	Now we construct the required function $g$ as follows:
	First we define $N_{ij} = |\{s | X_s = w^i, Z_s = w^j\}|$, $(i_1^*,j_1^*) = \arg\max N^{(1)}_{ij}$ and $S^{(1)}_i = \{i_1^*\}, S_j^{(1)}=\{j_1^*\}$.
	Then we give the recursive definition:
	$(i_r^*,j_r^*) = \arg\max_{i \not\in S_i^{(r-1)}, j \not\in S_j^{(r-1)}} N_{ij}$ and $S^{(r)}_i = S^{(r-1)}_i \cup \{i_r^*\}, S_j^{(r)}= S_j^{(r-1)}\cup \{j_r^*\}$ for $r=2, \dots, k$.
	The function $g$ is defined by $g(w^{i_r^*}) = g(w^{j_r^*})$. It is easy to show that $g$ is well-defined and belongs to $\Gamma$.
	On the other hand, $\sum_{i=1}^n \mathbbm{1}[X_i=g(Y)_i] = \sum_{i=1}^k N_{i_r^*,j_r^*}$.
	$N_{i_k^*, j_k^*} = \frac{n}{k} - \sum_{r=1, r\neq i_k^*}^k N_{r,j_k^*} = \frac{n}{k} - \sum_{r=1}^{k-1} N_{i^*_r,j_k^*} \geq \frac{n}{k} - \sum_{r=1}^{k-1} N_{i^*_r,j^*_r}$. Therefore,  $\sum_{i=1}^n \mathbbm{1}[X_i=g(Y)_i] \geq \frac{n}{k}$.
\end{proof}
\begin{lemma}\label{lem:thm23eq}
\begin{align}
\lfloor \frac{m+k-1}{k} \rfloor \beta>\beta^\ast \iff  & m\ge k \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor + 1 \label{eq:m2} \\
\lfloor \frac{m+k-1}{k} \rfloor \beta <\beta^\ast  \iff & m <  m^* \textrm{ if } \beta^*/\beta \textrm{ is not an integer } \notag \\
& m <  m^* - k \textrm{ if } \beta^*/\beta \textrm{ is an integer } \label{eq:m22}
\end{align}
So Theorem~\ref{thm:wt1} and Theorem~\ref{thm:wt2} give the same threshold.
\end{lemma}
\begin{proof}
	First, we give a proof of Equation \eqref{eq:m2}: $\lfloor \frac{m+k-1}{k} \rfloor \beta>\beta^\ast$
	implies that $\frac{\beta^\ast}{\beta}<\lfloor \frac{m+k-1}{k} \rfloor$.
	The smallest integer that is larger than $\frac{\beta^\ast}{\beta}$ is
	$\lfloor \frac{\beta^\ast}{\beta}\rfloor +1$,
	so $\lfloor \frac{\beta^\ast}{\beta} \rfloor + 1 \le \lfloor \frac{m+k-1}{k} \rfloor\le \frac{m+k-1}{k}$,
	and thus $m\ge k \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1$.
	Now assume $m\ge 2 \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1$,
	then $\frac{m-1}{2} \ge \lfloor \frac{\beta^\ast}{\beta} \rfloor$.
	Since the right hand side is an integer,
	we have $\lfloor \frac{m+1}{2} \rfloor = \lfloor \frac{m-1}{2} \rfloor +1 
	\ge \lfloor \frac{\beta^\ast}{\beta} \rfloor +1 >\frac{\beta^\ast}{\beta}$.
	
	Secondly, we show Equation \eqref{eq:m22}. If $\beta^\ast \over \beta$ is not an integer, then
	\begin{align*}
	& \lfloor \frac{m+k-1}{k} \rfloor < {\beta^\ast \over \beta}  \\
	\iff & \frac{m+k-1}{k}  < \lfloor{\beta^\ast \over \beta}\rfloor + 1 \\
	\iff & m < k \lfloor \frac{\beta^\ast}{\beta} \rfloor  + 1 = m^*
	\end{align*}
	If  $\beta^\ast \over \beta$ is an integer, then
	\begin{align*}
	& \lfloor  \frac{m+k-1}{k}  \rfloor < {\beta^\ast \over \beta}  \\
	\iff & \frac{m+k-1}{k}  < \lfloor{\beta^\ast \over \beta}\rfloor \\
	\iff & m <  m^* - k
	\end{align*}
\end{proof}
\bibliographystyle{plain}
\bibliography{exportlist}



\end{document}

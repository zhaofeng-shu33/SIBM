\documentclass{article}
\input{macros.tex}
\title{Extension of SIBM to multiple communities}
\author{Feng Zhao}
\begin{document}
\maketitle

\section{Problem setup and main results} \label{s:Preliminaries}
We first recall the definition of Symmetric Stochastic Block Model (SSBM) with $k$ communities and the definition of Ising model with $k$ state (See Definition 4 in \cite{Abbe17}).
\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
Let $n$ be a positive even integer, $W= \{1, \omega, \dots, \omega^{k-1}\}$ is a cyclic group with order $k$ and let $p,q\in[0,1]$ be two real numbers. Let $X=(X_1,\dots,X_n)\in W^n$, and let $G=([n],E(G))$ be a undirected graph with vertex set $[n]$ and edge set $E(G)$. The pair $(X,G)$ is drawn under $\SSBM(n,k,p,q)$ if 

\noindent
(i) $X$ is drawn uniformly with the constraint that $|\{v \in [n] : X_v = \omega^i\}| = \frac{n}{k}$;

\noindent
(ii) the vertices $i$ and $j$ in $G$ are connected with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$, independently of other pairs of vertices.
\end{definition}
 
From each element $\sigma$ in a symmetric group $S_k$, we can induce a permutation function $f$ on $x=(x_1,\dots,x_n)\in W^n$ as $f(x_i) = \gamma(i)$ ($\gamma$ is a function on $[k]$).
$x$ and $f(x)$ correspond to the same balanced partition, and the conditional distribution $P(G|X=x)$ is the same as $P(G|X=f(x))$ in the above definition. Therefore, we can only hope to recover $X$ from $G$ up to a global permutation.

In this paper, we focus on the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. In this regime, it is well known that exact recovery of $X$ (up to a global sign) from $G$ is possible if and only if $\sqrt{a}-\sqrt{b} > \sqrt{k}$  \cite{abbe2015exact}.
 
Given a partition/labeling $X$ on $n$ vertices, the SBM specifies how to generate a random graph on these $n$ vertices according to the labeling. In some sense, Ising model works in the opposite direction, i.e., given a graph $G=([n],E(G))$, Ising model defines a probability distribution on all possible labelings $W^n$ of these $n$ vertices. 

We define the indicator function $I(x, y)$ as:
\begin{equation}
I(x, y) = \begin{cases}
 k-1 & x = y \\
 -1 & x \neq y
\end{cases}
\end{equation}
% A general Ising model without external fields is a probability distribution on the configurations $\{\pm 1\}^n$ such that
% $$
% P(\sigma)=\frac{1}{Z}
% \exp(\sum_{i\neq j} J_{ij} \sigma_i \sigma_j) 
% \quad\quad
% \text{for every~}
% \sigma=(\sigma_1,\dots,\sigma_n) \in \{\pm 1\}^n ,
% $$
% where $Z=\sum_{\sigma \in \{\pm 1\}^n} \exp(\sum_{i\neq j} J_{ij} \sigma_i \sigma_j)$ is the normalizing constant.
% We incorporate the graphical structure of $G$ into the Ising model by setting $J_{ij}=\beta$ for all  $\{i,j\}\in E(G)$ and $J_{ij}=-\alpha\log(n)/n$ for all $\{i,j\}\notin E(G)$, where $\alpha,\beta>0$ are positive constants. 
% This is summarized in the following definition.

 
 \begin{definition}[Ising model]
Define an Ising model on a graph $G=([n],E(G))$ with parameters $\alpha,\beta>0$ as the probability distribution on the configurations $\sigma\in W^n$ such that\footnote{When there is only one sample, we usually denote it as $\bar{\sigma}$. When there are $m$ (independent) samples, we usually denote them as $\sigma^{(1)},\dots,\sigma^{(m)}$.}
\begin{equation} \label{eq:isingma}
P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\beta\sum_{\{i,j\}\in E(G)} I(\bar{\sigma}_i ,\bar{\sigma}_j)
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j)\Big) ,
\end{equation}
where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and 
\begin{equation}  \label{eq:zg}
Z_G(\alpha,\beta)=\sum_{\bar{\sigma} \in W^n} \exp\Big(\beta\sum_{\{i,j\}\in E(G)}I(\bar{\sigma}_i, \bar{\sigma}_j)
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j) \Big) 
\end{equation}
is the normalizing constant.
\end{definition}






By definition we always have $P_{\sigma|G}(\sigma=\bar{\sigma})=P_{\sigma|G}(\sigma=f(\bar{\sigma}))$ in the Ising model. Next we present our new model, the Stochastic Ising Block Model (SIBM), which can be viewed as a natural composition of the SSBM and the Ising model. In SIBM, we first draw a pair $(X,G)$ under $\SSBM(n,k, p,q)$.  Then we draw $m$ independent samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ from the Ising model on the graph $G$, where $\sigma^{(u)}\in W^n$ for all $u\in[m]$.

\begin{definition}[Stochastic Ising Block Model]
Let $n$ and $m$ be positive integers such that $n$ is even. Let $p,q\in[0,1]$ be two real numbers and let $\alpha,\beta>0$. The triple $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ is drawn under $\SIBM(n,k, p,q,\alpha,\beta,m)$ if

\noindent
(i) the pair $(X,G)$ is drawn under $\SSBM(n,k, p,q)$;

\noindent
(ii) for every $i\in[m]$, each sample $\sigma^{(i)}=(\sigma_1^{(i)},\dots,\sigma_n^{(i)}) \in W^n$ is drawn independently according to the distribution \eqref{eq:isingma}.
\end{definition}

Notice that we only draw the graph $G$ once in SIBM, and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ are drawn independently from the Ising model on the {\em same} graph $G$.
Our objective is to recover the underlying partition (or the ground truth) $X$ from the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$, and we would like to use the smallest possible number of samples to guarantee the exact recovery of $X$ up to a global permutation.
Below we use the notation $P_{\SIBM}(A):=E_G[P_{\sigma|G}(A)]$ for an event $A$, where the expectation $E_G$ is taken with respect to the distribution given by SSBM. In other words, $P_{\sigma|G}$ is the conditional distribution of  $\sigma$ given a fixed $G$ while $P_{\SIBM}$ is the joint distribution of both $\sigma$ and $G$.
By definition, we have $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f$ is induced from $S_k$. We use the set $\Gamma = \{f_{\gamma}(X) | \gamma \in S_k\}$ to
represent all vectors with permutations on $X$.

\begin{definition}[Exact recovery in SIBM]
Let $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k, p,q,\alpha,\beta,m)$.
We say that exact recovery is solvable for $\SIBM(n,p,q,\alpha,\beta,m)$ if there is an algorithm that takes $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ as inputs and outputs $\hat{X}=\hat{X}(\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ such that
$$
P_{\SIBM}(\hat{X} \in \Gamma) \to 1
\text{~~~as~} n\to\infty ,
$$
and we call $P_{\SIBM}(\hat{X} \in \Gamma)$ the success probability of the recovery/decoding algorithm.
\end{definition}


As mentioned above, we consider the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. By definition, the ground truth $X$, the graph $G$ and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ form a Markov chain $X\to G\to \{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Therefor, if we cannot recover $X$ from $G$, then there is no hope to recover $X$ from $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Thus a necessary condition for the exact recovery in SIBM is $\sqrt{a}-\sqrt{b}> \sqrt{k}$, and we will limit ourselves to this case throughout the paper.

\vspace*{.1in}
\noindent{\bf Main Problem:} \emph{For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, what is the smallest sample size $m^\ast$ such that exact recovery is solvable for $\SIBM(n,a\log(n)/n, b\log(n)/n,\alpha,\beta,m^\ast)$?}

\vspace*{.1in}  It is this optimal sample size problem that we address---and resolve---in this paper. 
Our main results read as follows.

\begin{theorem} \label{thm:wt1}
For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, let
\begin{equation} \label{eq:defstar}
\beta^\ast := \frac{1}{k}
\log\frac{a+b-k-\sqrt{(a+b-k)^2-4ab}}{2 b} \text{~~and~~}
m^\ast := 2 \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1 .
\end{equation}
{\bf Case (i) when $\alpha>b\beta$}: If $m\ge m^\ast$, then exact recovery is solvable in $O(n)$ time for $\SIBM(n,a\log(n)/n, \linebreak[4] b\log(n)/n,\alpha,\beta,m)$.
If $\beta^\ast/\beta$ is not an integer and $m<m^\ast$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$. If $\beta^\ast/\beta$ is an integer and $m<m^\ast-2$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$.
{\bf Case (ii) when $\alpha<b\beta$}: Exact recovery of $\SIBM(n,a\log(n)/n, b\log(n)/n,\alpha,\beta,m)$ is not solvable for any $m=O(\log^{1/4}(n))$, and in particular, it is not solvable for any constant $m$ that does not grow with $n$.
\end{theorem}

Notation convention:
For $\cI \subseteq [n]$, we denote the event $\sigma = X^{(\sim \cI)}$ as $\sigma_i \neq X_i$ for all $i \in \cI$ and $\sigma_i = X_i$ for all $i \not\in \cI$.
When $\cI$ only contains one element, e.g., $\cI=\{i\}$, we write the event $\sigma = X^{(\sim i)}$ instead of $\sigma = X^{(\sim\{i\})}$.

For $\sigma,X\in W^n$, we define
$$
\dist(\sigma, X)
:=|\{i\in[n]:\sigma_i\neq X_i\}| 
\quad \text{and} \quad
\dist(\sigma,\Gamma)
:=\min\{\dist(\sigma, f(X)) | f(X) \in \Gamma\} .
$$
\begin{theorem}  \label{thm:wt3}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$. Let $m$ be a constant integer that does not grow with $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m).
$$
Define $g(\beta)  := \frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1$.
If $\beta>\beta^\ast$, then
$$
P_{\SIBM}(\sigma^{(i)} \in \Gamma \text{~for all~} i\in[m]) = 1-o(1).
$$
If $\beta\le \beta^\ast$, then
$$
P_{\SIBM}(\dist(\sigma^{(i)} \in \Gamma)= \Theta(n^{g(\beta)}) \text{~for all~} i\in[m]) = 1-o(1) .
$$
\end{theorem}

\section{Sketch of the proof}
\label{sect:sketch}

In this section, we illustrate the main ideas and explain the important steps in the proof of the main results. The complete proofs are given in Section~\ref{sect:aln}--\ref{sect:converse}.
As the first step, we prove that for $a>b>0$, if $\alpha>b\beta$, then all the samples are centered in $\Gamma$ with probability $1-o(1)$; if $\alpha<b\beta$, then all the samples are centered in $\Theta := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k\}$ where $\mathbf{1}_n$ is the all one vector with dimension $n$.

We use the concentration results for adjacency matrices of random graphs with independent edges to prove this. Let $A=A(G)$ be the adjacency matrix of the graph $G$. Then \eqref{eq:isingma} can be written as
\begin{align*}
 P_{\sigma|G}(\sigma=\bar{\sigma})  
=  \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j}\Big( \big(\beta+\frac{\alpha\log(n)}{n} \big) A
-\frac{\alpha\log(n)}{n} (J_n-I_n) \Big)_{ij} I(\bar{\sigma}_i,\bar{\sigma}_j) 
\Big)  ,
\end{align*}
where $J_n$ is the all one matrix and $I_n$ is the identity matrix, both of size $n\times n$.
Define a matrix
$
M:= \big(\beta+\frac{\alpha\log(n)}{n} \big) E[A|X]
-\frac{\alpha\log(n)}{n} (J_n-I_n).
$
Then we can further write \eqref{eq:isingma}
as
$$
P_{\sigma|G}(\sigma=\bar{\sigma})
= \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) + \frac{1}{2} \sum_{i,j}\big(\beta+\frac{\alpha\log(n)}{n} \big)  (A-E[A|X])_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) 
  \bar{\sigma}^T 
\Big)  .
$$
One can show that if $\alpha>b\beta$, then the maximizers of $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ consist of set $\Gamma$, and if $\alpha<b\beta$, then the maximizer set is $\Theta$.
Moreover, \cite{Hajek16} proved the concentration of $A$ around its expectation $E[A|X]$ in the sense that
$\|A-E[A|X]\| = O(\sqrt{\log(n)})$ with probability $1-o(1)$, we can further show that the error term above is bounded by
$$
\left|\frac{1}{2} \big(\beta+\frac{\alpha\log(n)}{n} \big)\sum_{i,j} (A-E[A|X])
 I( \bar{\sigma}_i, \bar{\sigma}_j) \right| = O \big( n \sqrt{\log(n)} \big)
  \text{~~for all~} \bar{\sigma}\in\{\pm 1\}^n  .
$$
This allows us to prove that in both cases (no matter $\alpha>b\beta$ or $\alpha<b\beta$), the Hamming distance between the samples and the maximizers of $\sum_{i,j}  M_{ij}I(\bar{\sigma}_i, \bar{\sigma}_j)$ is upper bounded by $kn/\log^{1/3}(n)=o(n)$.
More precisely, if $\alpha>b\beta$, then $\dist(\sigma^{(i)},\Gamma )< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$; if $\alpha<b\beta$, then $\dist(\sigma^{(i)}, \Theta)< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$; see Proposition~\ref{prop:1} for a rigorous proof.
In the latter case, each sample only take $\sum_{j=0}^{kn/\log^{1/3}(n)}\binom{n}{j}j^{k-1}$ values, so each sample contains at most $\log_k(\sum_{j=0}^{kn/\log^{1/3}(n)}\binom{n}{j}j^{k-1})=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$ bits of information about $X$. On the other hand, $X$ itself is uniformly distributed over a set of $\frac{n!}{(n/k)^k}$ vectors, so one needs at least $\log_k\frac{n!}{(n/k)^k}=\Theta(n)$ bits of information to recover $X$. Thus if $\alpha<b\beta$, then exact recovery of $X$ requires at least $\Omega(\frac{\log^{1/3}(n)}{\log\log(n)})\ge \Omega(\log^{1/4}(n))$ samples; see Proposition~\ref{prop:ab} for a rigorous proof.

For the rest of this section, we will focus on the case $\alpha>b\beta$ and establish the sharp threshold on the sample complexity. We first analyze the typical behavior of one sample and explain how to prove Theorem~\ref{thm:wt3}. Then we use the method developed for the one sample case to analyze the distribution of multiple samples, which allows us to prove Theorem~\ref{thm:wt2}. Note that Theorem~\ref{thm:wt1} follows directly from Theorem~\ref{thm:wt2}.

\subsection{Why is $\beta^\ast$ the threshold?} \label{sect:why}

Let us analyze the one sample case, i.e., we take $m=1$.
Theorem~\ref{thm:wt3} implies that $\beta^\ast$ is a sharp threshold for the event $\{\sigma \in \Gamma\}$, i.e., $P_{\SIBM}(\sigma\in \Gamma)=1-o(1)$ if $\beta$ is above this threshold and $P_{\SIBM}(\sigma\in \Gamma)=o(1)$ if $\beta$ is below this threshold.
We already know that
$
P_{\SIBM} \big(\dist(\sigma,\pm X)< kn/\log^{1/3}(n) \big) = 1- o(1) .
$
Therefore the following three statements are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $P_{\SIBM}(\sigma\in \Gamma)$ has a sharp transitions from $0$ to $1$ at $\beta^\ast$.
\item $P_{\SIBM} \big( 1\le \dist(\sigma,\Gamma)< kn/\log^{1/3}(n) \big)$ has a sharp transitions from $1$ to $0$ at $\beta^\ast$.
\item $\frac{P_{\SIBM} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\SIBM}(\sigma= X)}$ has a sharp transitions from $\infty$ (or $\omega(1)$) to $0$ at $\beta^\ast$.
\end{enumerate}
Statements (2) and (3) are equivalent because $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f$ is a permutation function.
We will show that the above three statements are further equivalent to
\begin{enumerate}[label=(\arabic*)]
\setcounter{enumi}{3}
  \item  $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}$ has a sharp transitions from $\infty$ (or $\omega(1)$) to $0$ at $\beta^\ast$.
\end{enumerate}
We first prove (4) and then show that it is equivalent to statement (3).
Instead of analyzing $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}$, we analyze $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}$ for a typical graph $G$.

Then,
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}
=\sum_{i=1}^n \frac{P_{\sigma|G} ( \sigma= X^{(\sim i)} )} {P_{\sigma|G}(\sigma= X)} .
$$
Given the ground truth $X$, a graph $G$ and a vertex $i\in[n]$, define
\begin{equation*}
A^r_i=A^r_i(G):=|\{j\in[n]\setminus\{i\}:\{i,j\}\in E(G), X_j=\omega^r \cdot X_i\} |, r=0, \dots, k-1
\end{equation*}
Then by \eqref{eq:isingma}, we have
\begin{align*}
\frac{P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)}
 = \sum_{r=1}^{k-1}\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i)
-\frac{2(k-1)\alpha\log(n)}{n} \Big) 
 = (1+o(1)) \sum_{r=1}^{k-1}\exp (k \beta(A^r_i-A^0_i))  ,
\end{align*}
where the second equality holds with high probability because $|A^r_i-A^0_i|=O(\log(n))$ with probability $1-o(1)$.
By definition,
$A^0_i\sim \Binom(\frac{n}{k}-1,\frac{a\log(n)}{n})$ and $A^r_i\sim \Binom(\frac{n}{k}, \frac{b\log(n)}{n})$ for $r=1,\dots, k-1$, and they are independent. Therefore,
\begin{align*}
E_G[\exp (k \beta (A^r_i-A^0_i))]
& =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^{k\beta} \Big)^{n/k}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-k\beta} \Big)^{n/k-1}  \\
& = 
\exp\Big(\frac{\log(n)}{k} ( a e^{-k\beta}+b e^{k\beta} -a-b )
+o(1) \Big) 
 = (1+o(1)) n^{g(\beta)-1} ,
\end{align*}
where $E_G$ means that the expectation is taken over the randomness of $G$, and the function
$g(\beta)  := \frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1$ is defined in Theorem~\ref{thm:wt3}.
As a consequence,
\begin{equation} \label{eq:mew}
E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} \Big]
= (1+o(1)) \sum_{i=1}^n\sum_{r=1}^{k-1} E_G[\exp (2 \beta (A^r_i-A^0_i))]
= (k-1+o(1)) n^{g(\beta)} .
\end{equation}
One can show that $g(\beta)$ is a convex function and takes minimum at $\beta=\frac{1}{2k}\log\frac{a}{b}$, so $g(\beta)$ is strictly decreasing in the interval $(0,\frac{1}{2k}\log\frac{a}{b})$. Furthermore, $\beta^\ast$ is a root of $g(\beta)=0$, and $0<\beta^\ast<\frac{1}{2k}\log\frac{a}{b}$, so $g(\beta)>0$ for $\beta<\beta^\ast$ and $g(\beta)<0$ for $\beta^\ast<\beta<\frac{1}{2k}\log\frac{a}{b}$.
Taking this into the above equation, we conclude that the expectation of $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}$ has a sharp transition from $\omega(1)$ to $o(1)$ at $\beta^\ast$.
This at least intuitively explains why $\beta^\ast$ is the threshold. However, in order to formally establish statement (4) above, we need to prove that this sharp transition happens for a typical graph $G$, not just for the expectation.
Moreover, $g(\beta)$ is an increasing function in the interval $\beta\in(\frac{1}{2k}\log\frac{a}{b}, +\infty)$, so the expectation first decreases in the interval $(0,\frac{1}{2k}\log\frac{a}{b}]$ and then starts increasing. We will prove that there is a ``cut-off" effect when $\beta>\frac{1}{2k}\log\frac{a}{b}$, i.e., although the expectation becomes much larger than $n^{g(\frac{1}{2k}\log\frac{a}{b})}$, for a typical graph $G$, we always have
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} 
= O( n^{g(\frac{1}{4}\log\frac{a}{b})} ) =o(1)
$$
whenever $\beta>\frac{1}{2k}\log\frac{a}{b}$.
Below we divide the proof into three cases: (i) $\beta\in(0,\beta^\ast]$, (ii) $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$, and (iii) $\beta\in(\frac{1}{2k}\log\frac{a}{b},+\infty)$.
Case (ii) is the simplest case, and its proof is essentially an application of Markov inequality, so we start with this case.

\subsection{Proof for $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$: An application of Markov inequality}
\label{sect:simreg}

We know that $g(\beta)<0$ for $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$. By \eqref{eq:mew} and Markov inequality, for almost all\footnote{By almost all $G$, we mean there is a set $\cG$ such that $P(G\in\cG)=1-o(1)$ and for every $G\in\cG$ certain property holds. The probability $P(G\in\cG)$ is calculated according to SSBM defined in Definition~\ref{def:SSBM}.} $G$, we have $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}=o(1)$. This proves that $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}=o(1)$ in this interval. With a bit more extra effort, let us also prove that $\frac{P_{\SIBM} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\SIBM}(\sigma= X)}=o(1)$.

For each $I$, we introduce a vector $v$ with $\mathrm{dim}(v) = |I|=r$. Each element of $v$ takes from $\{\omega, \omega^2, \omega^{k-1}\}$.
$X^{(\sim I, v)}$ is defined as flipping $i \in I$ as : $X_i \to v_{\mathrm{index}(i)}\cdot X_i$.
For example, if $I = {1,3}, n=3, v=(\omega^2, \omega)$ then $X^{(\sim I,v)} = (\omega^2 \cdot X_1, X_2, \omega \cdot X_3)$.
Then $P_{\sigma|G}(\sigma = X^{(\sim I)})=\sum_{v}P_{\sigma|G}(\sigma = X^{(\sim I,v)})$

By definition,
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = r )}{P_{\sigma|G}(\sigma= X)}
=\sum_{\cI\subseteq[n],|\cI|=r} \frac{P_{\sigma|G} ( \sigma= X^{(\sim \cI)} )} {P_{\sigma|G}(\sigma= X)} .
$$
Similarly to $A_i$ and $A^r_i$, for a set $\cI\subseteq[n]$, we define
define
$A_{\cI}=A_{\cI}(G):=|\{i,j\}\in E(G):  \{i, j\} \not\subseteq [n]\setminus\cI, X_j=X_i\}|$ and  
$B_{\cI,v}=B_{\cI,v}(G):=|\{\{i,j\}\in E(G): \{i, j\} \not\subseteq [n]\setminus\cI, X_j= v_{\mathrm{index}(i)} \cdot X_i\}|$.
Then by \eqref{eq:isingma} one can show that
$$
 \frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} 
\le \sum_{v}\exp\Big( k\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI,v}-A_{\cI})
\Big) = \sum_{v}\exp ( k (\beta + o(1)) (B_{\cI,v}-A_{\cI})
 ) .
$$
Since we are only interested in the case $|\cI|<kn/\log^{1/3}(n)=o(n)$,
by definition we have $A_{\cI}\sim\Binom((\frac{n}{k}-o(n))|\cI|,\frac{a\log(n)}{n})$ and $B_{\cI}\sim\Binom((\frac{n}{k}-o(n))|\cI|,\frac{b\log(n)}{n})$, and they are independent. Therefore,
\begin{align*}
E_G[\exp ( k (\beta +o(1)) (B_{\cI,v}-A_{\cI}) ) ] = &
\exp\Big(\frac{|\cI|\log(n)}{k} ( a e^{-k\beta}+b e^{k\beta} -a-b +o(1) )
 \Big) \\
 = & n^{|\cI|(g(\beta)-1+o(1))} .
\end{align*}
As a consequence,
\begin{equation} \label{eq:nn}
E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = r)}{P_{\sigma|G}(\sigma= X)} \Big]
\le \sum_{\cI\subseteq[n],|\cI|=r} 
n^{k (g(\beta)-1+o(1))}
= \binom{n}{r} (k-1)^r n^{r (g(\beta)-1+o(1))}
< (k-1)^rn^{r (g(\beta)+o(1))} .
\end{equation}
Then by Markov inequality, there is a set $\cG^{(r)}$ such that $P(G\in\cG^{(r)})=1-(k-1)^rn^{r g(\beta)/4}$ and for every $G\in\cG^{(r)}$, $\frac{P_{\sigma|G} ( \dist(\sigma, X) = k )}{P_{\sigma|G}(\sigma= X)} \le (k-1)^rn^{r g(\beta)/2}$.
Let $\cG=\cap_{r=1}^{kn/\log^{1/3}(n)} \cG^{(r)}$. By union bound, we have $P(G\in\cG)>1-\sum_{r=1}^\infty (k-1)^rn^{r g(\beta)/4} = 1-o(1)$. Moreover, for every $G\in\cG$,
$\frac{P_{\sigma|G} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\sigma|G}(\sigma= X)} < \sum_{r=1}^\infty (k-1)^rn^{r g(\beta)/2} = o(1)$.
This proves that $\frac{P_{\SIBM} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\SIBM}(\sigma= X)}=o(1)$, and so $P_{\SIBM}(\sigma \in \Gamma)=1-o(1)$ when $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$.


\subsection{Proof for $\beta\in(\frac{1}{2k}\log\frac{a}{b},+\infty)$: The ``cut-off" effect}

The analysis in this interval is more delicate. Since $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} 
= (1+o(1)) \sum_{i=1}^n \sum_{r=1}^{k-1}\exp (k\beta (A^r_i-A^0_i))$, we start with a more careful analysis of $\sum_{i=1}^n \exp (2 \beta (A^r_i-A^0_i))$.
Since $A^r_i-A^0_i$ takes integer value between $-n/k$ and $n/k$,
we can use indicator functions to write
\begin{align*}
 \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{n/k}
\mathbbm{1}[A^r_i-A^0_i=t \log(n)] \exp (k\beta t \log(n) ) ,
\end{align*}
where the quantity $t\log(n)$ ranges over all integer values from $-n/k$ to $n/k$ in the summation.
Define $D(G,t):=|\{i\in[n]:A^r_i-A^0_i= t\log(n)\}|=\sum_{i=1}^n \mathbbm{1}[A^r_i-A^0_i=t \log(n)]$.
Therefore,
\begin{equation}  \label{eq:gour}
 \sum_{i=1}^n \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{n/k}
D(G,t) \exp (k\beta t \log(n) ) .
\end{equation}
By Chernoff bound, we have $P(A^r_i-A^0_i\ge 0)\le \exp\big(\log(n)(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big)$.
Define $\cG_1:=\{G:A^r_i-A^0_i< 0~\forall i\in[n]\}$. Then by union bound, $P(G\notin\cG_1)\le\exp\big(\log(n)(1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big) = o(1)$, where the equality follows from the assumption that $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
For every $G\in\cG_1$, $D(G,t)=0$ for all $t\ge 0$, and so
\begin{equation} \label{eq:duj}
 \sum_{i=1}^n \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{-1}
D(G,t) \exp (k\beta t \log(n) ) .
\end{equation}
This indicates that there is a ``cut-off" effect at $t>0$, i.e., $D(G,t) \exp (2\beta t \log(n) )=0$ for all positive $t$ with probability $1-o(1)$, although its expectation can be very large, as we will show next.
Define a function
$$
f_{\beta}(t):=\frac{1}{k}\sqrt{k^2t^2+4ab} -t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big) -\frac{a+b}{k} +1 +k\beta t.
$$
Using Chernoff bound, one can show that 
$$
E[D(G,t)
\exp\big(2\beta t \log(n) \big)]
\le \exp( f_{\beta}(t) \log(n) ) .
$$
with probability one (Using a more careful analysis, one can show that this bound is tight up to a $\frac{1}{\sqrt{\log(n)}}$ factor; see Appendix~\ref{ap:um}.)
The function $f_{\beta}(t)$ is a concave function and takes maximum value at $t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k}$, and its maximum value is $f_{\beta}(t^\ast)=\frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1
=g(\beta)$.
Therefore, if we take expectation on both sides of \eqref{eq:gour}, then the sum on the right-hand side is concentrated on a small neighborhood of $t^\ast$. When $\beta>\frac{1}{2k}\log\frac{a}{b}$, we have $t^\ast>0$. Due to the ``cut-off" effect at $t>0$, we have $D(G,t)=0$ for all $t$ in the neighborhood of $t^\ast$ with probability $1-o(1)$, so the main contribution to the expectation comes from a rare event $G\notin\cG_1$. This explains why the behavior of a typical graph $G$ deviates from the behavior of the expectation.
Since $f_{\beta}(t)$ is a concave function, the sum $E[\sum_{t\log(n)=-n/k}^{-1}
D(G,t) \exp (k\beta t \log(n) )]$
is upper bounded by $O(\log(n))n^{f_{\beta}(0)}$  when $t^\ast>0$.
Notice that $f_{\beta}(0)=g(\frac{1}{2k}\log\frac{a}{b})=1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}<0$.
Now using \eqref{eq:duj} and Markov inequality, we conclude that $\sum_{i=1}^n \exp (2 \beta (A^r_i-A^0_i))=o(1)$ for almost all $G$,
and so $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} =o(1)$ for almost all $G$. Thus we have shown that $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}=o(1)$ when
$\beta>\frac{1}{2r}\log\frac{a}{b}$.
The analysis of $\frac{P_{\SIBM} ( \dist(\sigma, X) = r )}{P_{\SIBM}(\sigma= X)}$ for $1\le r<kn/\log^{1/3}(n)$ is
similar to the analysis in Section~\ref{sect:simreg}, and we do not repeat it here. By now we have given a sketched proof of
$P_{\SIBM}(\sigma \in \Gamma)=1-o(1)$ when $\beta>\beta^\ast$; see Section~\ref{sect:equal} for a rigorous proof. Next we move to the case $\beta\le\beta^\ast$.

\section{Samples are concentrated around $\Gamma$ or $\Theta$} \label{sect:aln}
In this section, we show that for $a>b$, if $\alpha>b\beta$, then all the samples produced by $\SIBM(n, k,\linebreak[4]
a\log(n)/n, b\log(n)/n,\alpha,\beta,m)$ are very close to one element from $\Gamma$. More precisely, they differ from the ground truth $\Theta$ in at most $kn/\log^{1/3}(n)$ coordinates.
On the other hand, if $\alpha<b\beta$, then the samples differ from  $\pm \Theta$ in at most $kn/\log^{1/3}(n)$ coordinates.
For the latter case, we prove that the number of samples needed for exact recovery of $X$ is at least $\Omega(\log^{1/4}(n))$.

Let $A=A(G)$ be the adjacency matrix of $G$. Then \eqref{eq:isingma} can be written as
\begin{align*}
& P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{\beta}{2} \sum_{i,j} A_{ij} I(\bar{\sigma}_i,\bar{\sigma}_j)
-\frac{\alpha\log(n)}{2n} \sum_{i,j} (J_n-I_n-A)_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)
\Big)  \\
= & \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j} \Big( \big(\beta+\frac{\alpha\log(n)}{n} \big) A
-\frac{\alpha\log(n)}{n} (J_n-I_n) \Big)_{ij} I(\bar{\sigma}_i,\bar{\sigma}_j)
\Big),
\end{align*}
where $J_n$ is the all one matrix and $I_n$ is the identity matrix, both of size $n\times n$.
Conditioned on the ground truth $X$,
$A-E[A|X]$ is a symmetric matrix whose upper triangular part consists of independent entries. According to Theorem~5 in \cite{Hajek16}, the spectral norm of $A-E[A|X]$ is upper bounded by $O(\sqrt{\log(n)})$.
\begin{theorem}[Theorem~5 in \cite{Hajek16}] \label{thm:a2}
For any $r>0$, there exists $c>0$ such that the spectral norm of $A-E[A|X]$ satisfies
$$
P\big(\|A-E[A|X]\| \le c\sqrt{\log(n)} \big)\ge 1-n^{-r} .
$$
\end{theorem}
Define a matrix
$$
M:= \big(\beta+\frac{\alpha\log(n)}{n} \big) E[A|X]
-\frac{\alpha\log(n)}{n} (J_n-I_n).
$$
Then we can further write \eqref{eq:isingma}
as
\begin{equation} \label{eq:M}
P_{\sigma|G}(\sigma=\bar{\sigma})
= \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j}M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) + \frac{1}{2}\big(\beta+\frac{\alpha\log(n)}{n} \big)  \sum_{i,j}  (A-E[A|X])_{ij}
 I(\bar{\sigma}_i, \bar{\sigma}_j)  
\Big)  .
\end{equation}


By definition, all the diagonal entries of $M$ are $0$.
For $i\neq j$, 
$$
M_{ij}=\left\{ 
\begin{array}{cc}
 \big(a\beta-\alpha+\frac{a\alpha\log(n)}{n} \big) \frac{\log(n)}{n} = a' + O(\frac{\log^2 n}{n^2})
   & \text{~if~} X_i=X_j \\
  \big(b\beta-\alpha+\frac{b\alpha\log(n)}{n} \big) \frac{\log(n)}{n} = b' + O(\frac{\log^2 n}{n^2})
   & \text{~if~} X_i\neq X_j
\end{array}
\right. .
$$
where $a' =(a\beta - \alpha) \frac{\log n }{n}, b'=(b\beta - \alpha) \frac{\log n}{n}$.
Given $\bar{\sigma}\in\{1, \omega, \dots, \omega^{k-1}\}^n$, define a $k\times k$ matrix $\Xi$ with 
$\Xi_{ij} = |\{k \in [n]: X_k = \omega^{i-1}, \sigma_k = \omega^{j-1}\}|$. It can be seen that each row of $\Xi$ sums to $\frac{n}{k}$. Define $Q_{ij} = \sum_{r,s=1, r<s}^k (\Xi_{ir} - \Xi_{is})(\Xi_{jr} - \Xi_{js})$
Therefore,
\begin{equation} \label{eq:sMs}
\begin{aligned}
 \sum_{i,j}M_{ij}I(\bar{\sigma}_i, \bar{\sigma}_j)
= & a'\sum_{i=1}^k \big( (k-1)(\sum_{r=1}^k \Xi_{ir}^2) - \sum_{r,s=1,r \neq s}^k \Xi_{ir}\Xi_{is} \big) \\
+ & b'\sum_{i,j=1, i\neq j}^k  \big( (k-1)  (\Xi_{ir} \Xi_{jr}) - \sum_{r,s=1,r \neq s}^k \Xi_{ir}\Xi_{js}  \big) + O(\log^2 n)\\
= & a' \sum_{i=1}^k Q_{ii} + b' \sum_{i,j=1,i\neq j}^k Q_{ij} + O(\log^2 n ) \\
 = & ( a' - b') \sum_{i=1}^k Q_{ii} + b' \sum_{i,j=1}^k Q_{ij} + O(\log^2 n ) 
\end{aligned}
\end{equation}
Further we have:
$$
\sum_{i,j=1}^k Q_{ij} = \sum_{r,s=1, r<s}^k (\sum_{i=1}^k \Xi_{ir} - \sum_{i=1}^k \Xi_{is})^2
$$
Therefore, both the coefficients of $(a'-b')$ and $b'$ are non-negative.
According to \eqref{eq:M}, the configuration $\bar{\sigma}$ that maximizes $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ is (roughly) the most likely output of the Ising model.
Since we assume $a>b$, $a'-b' = (a-b) \beta > 0$ and its coefficient term $\sum_{i=1}^k Q_{ii}$ takes maximum when there is only one non-zero term $\frac{n}{k}$ on each row of the matrix $\Xi$. Below we discuss the sign of $b'$.

If $b\beta<\alpha$, then $b' < 0$ and we should let $\sum_{i,j=1}^k Q_{ij} = 0$ to make the whole expression larger. This is equivalent to say the summation of each column of the matrix $\Xi$ are all the same. Combining the two conditions together we can see the maximum value is
the representation of $S_k$ in $k\times k$ matrix form multiplied by a coefficient $\frac{n}{k}$. And it is further equivalent to the statement $\sigma \in \Gamma$.

If $b\beta>\alpha$, then $b' > 0$ and we should take the maximum value of $\sum_{i,j=1}^k Q_{ij}$ to make the whole expression larger. We can show that in this case, the maximizer is taken when one column of $\Xi$ is $\frac{n}{k}  \mathbf{1}_k$ and other columns are all zero. And it is further equivalent to the statement $\sigma \in \Theta$.

To summarize, if $b\beta<\alpha$, then $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ takes maximum at $\sigma \in \Gamma$.
If $b\beta>\alpha$, then the maximum is at$\sigma \in \Theta$.
Taking into account the effect of the error term $ \sum_{i,j}  (A-E[A|X])_{ij}
 I(\bar{\sigma}_i, \bar{\sigma}_j)  $ in \eqref{eq:M}, we have the following proposition:

\begin{proposition} \label{prop:1}
Let $a>b>0$ and $\alpha,\beta>0$ be constants. Let $m$ be a positive integer that is upper bounded by some polynomial of $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n, \alpha,\beta, m) .
$$
If $b\beta <\alpha$, then for any (arbitrarily large) $r>0$, there exists $n_0(r)$ such that for all even integers $n>n_0(\delta, r)$,
$$
P_{\SIBM} \Big(\dist(\sigma^{(i)}, \Gamma)< kn/\log^{1/3}(n)
\text{~~for all~} i\in[m] \Big) \ge 1- n^{-r} .
$$
If $b\beta >\alpha$, then for any (arbitrarily large) $r>0$, there exists $n_0(r)$ such that for all even integers $n>n_0(\delta, r)$,
$$
P_{\SIBM} \Big(\dist(\sigma^{(i)},\pm \Theta)< kn/\log^{1/3}(n)
\text{~~for all~} i\in[m] \Big) \ge 1- n^{-r} .
$$
\end{proposition}
\begin{proof}
We only prove the case of $b\beta <\alpha$ as the proof of the other case is virtually identical.
Define
$$
\gamma^{r,s}_i = \begin{cases}
1 & \sigma_i = \omega^r \\
-1 & \sigma_i = \omega^s \\
0 & \textrm{ otherwise}
\end{cases}
$$
Then we can show that
$$
\sum_{i,j} I(\sigma_i, \sigma_j) (A-\mathbb{E}[A|X]) = \sum_{r,s} (\gamma^{r,s})^T (A-\mathbb{E}[A|X])\gamma^{r,s}
$$
Since $A-E[A|X]$ is a symmetric matrix,
$$
|\bar{\gamma}  (A-E[A|X]) \bar{\gamma}^T|\le
\|A-E[A|X]\| \bar{\gamma} \bar{\gamma}^T=n \|A-E[A|X]\|
$$
for every $\bar{\gamma}\in\{\pm 1\}^n$.
Therefore, by Theorem~\ref{thm:a2}, for any $r>0$, there is $c>0$ such that
\begin{equation} \label{eq:cnA}
\left|\frac{1}{2} \big(\beta+\frac{\alpha\log(n)}{n} \big)\sum_{i,j} I(\sigma_i, \sigma_j) (A-\mathbb{E}[A|X]) 
\right| \le c\binom{k}{2} n \sqrt{\log(n)}
  \text{~~for all~} \bar{\sigma}\in W^n
\end{equation}
with probability at least $1-n^{-2r}$.

Define a small neighborhood of $\Xi^*$ as
$$
N(\Xi^*) := \{\Xi |\, |\Xi_{i,j} - \Xi_{i,j}| \leq \frac{n}{\log^{1/3} n}, 1\leq i,j\leq 3\}
$$
We use the $L_{\infty}$ norm for the matrix.
 When there is only one non-zero term $\frac{n}{k}$ on each row of the matrix $\Xi$, we call this matrix a ''Corner Matrix''.
 We use $\Gamma'$ to denote all corner matrices, then $|\Gamma'|=k^k$ and $\Gamma \subset \Gamma'$. The neighborhood of all corner matrice is denoted by $N(\Gamma')$. Similary we can define $N(\Gamma)$. 

$N(\Gamma)$ consists of $\bar{\sigma}$'s that differ from $\Gamma$ in at most $kn/\log^{1/3}(n)$ coordinates.
Given a graph $G$ whose adjacency matrix satisfies \eqref{eq:cnA}, we will show that $P_{\sigma|G}(\sigma\notin N(\Gamma))< e^{-n}$. Since $P_{\sigma|G}(\sigma=X)<1$, it suffices to prove that
$\frac{P_{\sigma|G}(\sigma\notin N(\Gamma))}{P_{\sigma|G}(\sigma=X)}<e^{-n}$. 

Since  $N(\Gamma)^c=N(\Gamma')^c\cup N(\Gamma' \backslash \Gamma)$.
Next we prove that for every $G$ satisfying \eqref{eq:cnA} and every
$\bar{\sigma}\in N(\Gamma')^c\cup N(\Gamma' \backslash \Gamma)$, 
\begin{equation} \label{eq:xmb}
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}{P_{\sigma|G}(\sigma=X)}<k^{-n}e^{-n} .
\end{equation}
Together with the trivial upper bound $|N(\Gamma')^c\cup N(\Gamma' \backslash \Gamma)|<k^n$, this implies that $\frac{P_{\sigma|G}(\sigma\notin N(\Gamma))}{P_{\sigma|G}(\sigma=X)}<e^{-n}$.

We first prove \eqref{eq:xmb} for $\bar{\sigma}\in N(\Gamma')^c$.
Observe that $\sigma=X$ corresponds to $\Xi=\frac{n}{k} I_k$. By \eqref{eq:sMs}, we have
\begin{equation} \label{eq:X}
\begin{aligned}
P_{\sigma|G}(\sigma=X) & =\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{1}{2} X M X^T + O(n\sqrt{\log(n)}) \Big)  \\
& =\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{(a-b)\beta(k-1)}{2k}n\log(n) + O(n\sqrt{\log(n)}) \Big) .
\end{aligned}
\end{equation}
On the other hand, if $\bar{\sigma}\in N(\Gamma')^c$, using the definition of $\Xi$ we can find an 1-1 correspondence
between $\bar{\sigma}$ and $\bar{\Xi}$. For $\bar{\Xi}$ we can find one place where 
$\frac{n}{\log^{1/3} n} < |\bar{\Xi}_{ij}| < \frac{n}{k}- \frac{n}{\log^{1/3} n}$. Then
$$
Q_{ii} < (\frac{n}{k}-\frac{2n}{\log^{1/3} n})^2 + (k-2)\Big( (\frac{n}{k} - \frac{n}{\log^{1/3} n})^2 + (\frac{n}{\log^{1/3} n})^2 \Big)
$$

$$
\sum_{r=1}^k Q_{rr} < \frac{k-1}{2k}n^2 - \frac{2n^2}{\log^{1/3} n}
+O(n^2/\log^{2/3}(n)).
$$
Since $b' = b\beta-\alpha<0$ and $\sum_{i,j=1}^k Q_{ij} \geq 0$, by \eqref{eq:sMs} we have
$$
\frac{1}{2}\bar{\sigma} M \bar{\sigma}^T  
\le \frac{a\beta-b\beta}{4}n\log(n)
-(a\beta-b\beta)n\log^{2/3}(n)
+O(n\log^{1/3}(n)) ,
$$
and so
\begin{align*}
P_{\sigma|G}(\sigma=\bar{\sigma}) \le \frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{a\beta-b\beta}{4}n\log(n) 
-(a\beta-b\beta)n\log^{2/3}(n)
+ O(n\sqrt{\log(n)}) \Big) .
\end{align*}
Combining this with \eqref{eq:X}, we have
$$
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}{P_{\sigma|G}(\sigma=X)}
\le \exp\Big(-(a\beta-b\beta)n\log^{2/3}(n)
+ O(n\sqrt{\log(n)}) \Big)
<k^{-n} e^{-n}
$$
for all $\bar{\sigma}\in N(\Gamma')^c$ and all $G$ satisfying \eqref{eq:cnA}. 

For $\bar{\sigma}\in N(\Gamma' \backslash \Gamma)$, we can find $r,s$ such that
$\sum_{i=1}^k \Xi_{ir} \geq \frac{2n}{k} - \frac{2n}{\log^{1/3} n }$ and 
$\sum_{i=1}^k \Xi_{is} \leq \frac{2n}{\log^{1/3} n }$, Therefore
$$
\sum_{i,j=1}^k Q_{ij} \geq (\sum_{i=1}^k \Xi_{ir} - \sum_{i=1}^k \Xi_{is})^2 \geq \frac{n^2}{k^2}
$$
for large $n$. Since $b' = b\beta-\alpha<0$, this implies that
$$
\frac{1}{2}\sum_{i,j=1}^k Q_{ij}
\frac{(b\beta-\alpha)\log(n)}{n}
< - \frac{\alpha-b\beta}{2k^2} n\log(n) ,
$$
so
$$
\frac{1}{2}\bar{\sigma} M \bar{\sigma}^T  
\le \frac{(a-b)\beta(k-1)}{2k}n\log(n)
- \frac{\alpha-b\beta}{2k^2} n\log(n) .
$$
Therefore,
\begin{align*}
P_{\sigma|G}(\sigma=\bar{\sigma}) \le \frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{(a-b)\beta(k-1)}{2k}n\log(n) 
-\frac{\alpha-b\beta}{2k^2} n\log(n)
+ O(n\sqrt{\log(n)}) \Big) .
\end{align*}
Combining this with \eqref{eq:X}, we have
$$
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}{P_{\sigma|G}(\sigma=X)}
\le \exp\Big(-\frac{\alpha-b\beta}{2k^2} n\log(n)
+ O(n\sqrt{\log(n)}) \Big)
<k^{-n} e^{-n}
$$
for all $\bar{\sigma}\in N(\Gamma' \backslash \Gamma)$ and all $G$ satisfying \eqref{eq:cnA}.

Now we have shown that for a single sample $\sigma$ produced by the SIBM, $P_{\sigma|G}(\sigma\in\Gamma)\ge 1- e^{-n}$ provided that $G$ satisfies \eqref{eq:cnA}. By the union bound, for $m$ independent samples $\sigma^{(1)},\dots,\sigma^{(m)}$ produced by the SIBM, $P_{\sigma|G}(\sigma^{(1)},\dots,\sigma^{(m)}\in\Gamma)\ge 1- m e^{-n}$ provided that $G$ satisfies \eqref{eq:cnA}.
We also know that $G$ satisfies \eqref{eq:cnA} with probability at least $1-n^{-2r}$, and by assumption $m$ is upper bounded by some polynomial of $n$. Therefore, the overall probability of $\sigma^{(1)},\dots,\sigma^{(m)}\in\Gamma$ is at least $1-n^{-r}$ when $n$ is large enough. This completes the proof of the proposition.
\end{proof}
\begin{proposition}  \label{prop:ab}
Let $a>b>0$ and $\alpha,\beta>0$ be constants. Let $m$ be a positive integer that is upper bounded by some polynomial of $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
$$
If $\alpha<b\beta$, then it is not possible to recover $X$ from the samples when $m=O(\log^{1/4}(n))$.
\end{proposition}

\begin{proof}
First observe that there are $\frac{n!}{(n/k)^k}$ balanced partitions, so one needs at least $\log_k \frac{n!}{(n/k)^k}=\Theta(n)$ bits to recover $X$.
By Proposition~\ref{prop:1}, with probability $1-o(n^{-4})$, $\dist(\sigma^{(i)}, \Theta)< kn/\log^{1/3}(n)$
for all $i\in[m]$. Therefore, each $\sigma^{(i)}$ takes at most
$$
T:=\sum_{j=0}^{kn/\log^{1/3}(n)} \binom{n}{j}(k-1)^j
$$
values, so each $\sigma^{(i)}$ contains at most $\log_k T$ bits of information. Next we prove that $\log_k T=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$, so we need at least $\Omega(\frac{\log^{1/3}(n)}{\log\log(n)})$ samples to recover $X$, which proves the proposition.

In order to upper bound $T$, we define a binomial random variable $Y\sim\Binom(n,\frac{k-1}{k})$. Then
$$
T=k^n P(Y\le kn/\log^{1/3}(n))
= k^n P(Y\ge n- kn/\log^{1/3}(n)).
$$
The moment generating function of $Y$ is $(\frac{1}{k}+\frac{k-1}{k}e^s)^n$. By Chernoff bound, for any $s>0$,
$$
P(Y\ge n- kn/\log^{1/3}(n)) \le
(\frac{1}{k}+\frac{k-1}{k}e^s)^n e^{-sn}
e^{ksn/\log^{1/3}(n)}
= k^{-n} (k-1+e^{-s})^n e^{ksn/\log^{1/3}(n)} .
$$
As a consequence, for any $s>0$,
$$
\log_k T\le n\Big(\log_k(k-1+e^{-s})
+\frac{ks}{\log^{1/3}(n)}\log_k e \Big) .
$$
Taking $s=\log\log(n)$ into this bound, we obtain that $\log_2 T=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$.
\end{proof}

\section{$\sigma \in \Gamma$ with probability $1-o(1)$ when $\beta>\beta^\ast$} \label{sect:equal}

Recall the definition of $\beta^\ast$ in \eqref{eq:defstar}.
\begin{proposition} \label{prop:tt}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\beta>\beta^\ast$ and $\alpha>b\beta$. 
Let 
$
(X,G,\sigma) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, 1) .
$
Then
$$
P_{\SIBM}(\sigma \in \Gamma)=1-o(1) .
$$
\end{proposition}

We have proved in Proposition~\ref{prop:1} that if $\alpha>b\beta$, then $\dist(\sigma \in \Gamma) \le kn/\log^{1/3}(n)$
 with probability $1-o(1)$.
Then Proposition~\ref{prop:1} tells us that
$$
\sum_{\cI\subseteq[n],~
kn/\log^{1/3}(n)<|\cI|<n-kn/\log^{1/3}(n)} P_{\SIBM}(\sigma=X^{(\sim\cI)})  = o(1) .
$$
By definition, $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$. Therefore, 
$$
\sum_{\cI\subseteq[n],1\le |\cI|\le kn/\log^{1/3}(n)} P_{\SIBM}(\sigma=f(X^{(\sim\cI)})) =\sum_{\cI\subseteq[n],1\le |\cI|\le kn/\log^{1/3}(n)} P_{\SIBM}(\sigma=X^{(\sim\cI)}).
$$
As a consequence, to prove Proposition~\ref{prop:tt}, we only need to show that
$$
\sum_{\cI\subseteq[n],1\le |\cI|\le kn/\log^{1/3}(n)} P_{\SIBM}(\sigma=X^{(\sim\cI)}) 
= o(1) .
$$
This is further equivalent to proving that there exists a set $\cG$ such that

\noindent (i)
$P(G\in\cG)=1-o(1)$, where the probability is calculated according to the $\SSBM(n, k,a\log(n)/n, \linebreak[4] b\log(n)/n)$.

\noindent (ii)
For every $G\in\cG$,
$$
\sum_{\cI\subseteq[n],1\le |\cI|\le kn/\log^{1/3}(n)}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} =o(1) .
$$
In order to prove the existence of such a set $\cG$, we define two functions
\begin{equation}  \label{eq:gbt}
\begin{aligned}
g(\beta) & := \frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1 , \\
\tilde{g}(\beta) & :=\left\{
\begin{array}{cc}
  g(\beta)   & \text{~if~} \beta< \frac{1}{2k}\log\frac{a}{b} \\
  g(\frac{1}{2k}\log\frac{a}{b})=1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}  & \text{~if~} \beta\ge \frac{1}{2k}\log\frac{a}{b}
\end{array}
\right. ,
\end{aligned}
\end{equation}
and we will prove in Lemma~\ref{lm:ele} below (see the end of Section~\ref{sect:k=1}) that $\tilde{g}(\beta)<0$ under the conditions of Proposition~\ref{prop:tt} (i.e., $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\beta>\beta^\ast$).
The existence of $\cG$ is guaranteed by the following proposition:
\begin{proposition} \label{prop:big}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\beta>\beta^\ast$ and $\alpha>b\beta$. 
Let 
$
(X,G,\sigma) \sim \SIBM(n,k,a\log(n)/n, b\log(n)/n,\alpha,\beta, 1) .
$
There is an integer $n_0$ such that for every even integer $n>n_0$ and  every integer $1\le r\le kn/\log^{1/3}(n)$, there is a set $\cG^{(r)}$ for which

\noindent (i)
$P(G\in\cG^{(r)}) \ge 1- 2(k-1) n^{r\tilde{g}(\beta)/8}$ ,

\noindent (ii) For every $G\in\cG^{(r)}$,
$$
\sum_{\cI\subseteq[n],|\cI|=r}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} <
n^{r \tilde{g}(\beta) /2} .
$$
\end{proposition}
With the $\cG^{(r)}$'s given by Proposition~\ref{prop:big}, we
define 
$$
\cG:=\bigcap_{r=1}^{kn/\log^{1/3}(n)} \cG^{(r)} .
$$
By the union bound,
$$
P(G\in\cG)\ge 1-2 \sum_{r=1}^{kn/\log^{1/3}(n)} n^{k\tilde{g}(\beta)/8}
> 1- \frac{2 n^{\tilde{g}(\beta)/8}}{1-n^{\tilde{g}(\beta)/8}}
=1-o(1),
$$
where the last equality follows from $\tilde{g}(\beta)<0$. Moreover, for every $G\in\cG$,
\begin{align*}
& \sum_{\cI\subseteq[n],1\le |\cI|\le kn/\log^{1/3}(n)}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} =
\sum_{r=1}^{kn/\log^{1/3}(n)}
\hspace*{0.05in}
\sum_{\cI\subseteq[n],|\cI|=r}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)}  \\
& < \sum_{r=1}^{kn/\log^{1/3}(n)}
n^{r \tilde{g}(\beta) /2}
< \frac{n^{\tilde{g}(\beta)/2}}{1-n^{\tilde{g}(\beta)/2}} =o(1) .
\end{align*}
Thus we have shown that Proposition~\ref{prop:tt} is implied by Proposition~\ref{prop:big}. In the rest of this section, we will prove the latter proposition. In Section~\ref{sect:k=1}, we will prove Proposition~\ref{prop:big} for the special case of $r=1$ to illustrate the basic idea of the proof. Then we prove Proposition~\ref{prop:big} for general $k$ in Section~\ref{sect:gen}.


\subsection{Proof of Proposition~\ref{prop:big} for $r=1$} \label{sect:k=1}

Given the ground truth $X$, a graph $G$ and a vertex $i\in[n]$, define
\begin{equation} \label{eq:defAB}
A^r_i=A^r_i(G):=\{j\in[n]\setminus\{i\}:\{i,j\}\in E(G), X_j= \omega^r \cdot X_i\} 
\end{equation}
Next we give an upper bound on $P(A^1_i-A^0_i\ge t\log(n))$ for $t\in [\frac{1}{k}(b-a), 0]$. We take the left boundary to be $\frac{1}{k}(b-a)$ because $\frac{E[A^1_i-A^0_i]}{\log(n)}\to\frac{1}{k}(b-a)$ as $n\to\infty$.
\begin{proposition}  \label{prop:cher}
For $t\in [\frac{1}{k}(b-a), 0]$,
\begin{equation} \label{eq:upba}
\begin{aligned}
& P(A^1_i-A^0_i\ge t\log(n))  \\
\le &  \exp\Big(\frac{\log(n)}{k}
\Big(\sqrt{k^2t^2+4ab} -kt\log\frac{\sqrt{k^2t^2+4ab}+kt}{2b} -a-b + O\big(\frac{\log(n)}{n}\big) \Big)\Big) .
\end{aligned}
\end{equation}
\end{proposition}

\begin{proof}
When $t=\frac{1}{k}(b-a)$, the right-hand side of \eqref{eq:upba} is equal to $1$, so we only need to prove \eqref{eq:upba} for $t\in (\frac{1}{k}(b-a), 0]$.
By definition,
$A^0_i\sim \Binom(\frac{n}{k}-1,\frac{a\log(n)}{n})$ and $A^1_i\sim \Binom(\frac{n}{k}, \frac{b\log(n)}{n})$, and they are independent.
The moment generating function of $A^1_i-A^0_i$ is
\begin{align*}
E[e^{s(A^1_i-A^0_i)}]
& =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^s \Big)^{n/k}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-s} \Big)^{n/k-1}  \\
& = 
\exp\Big(\frac{\log(n)}{k} \Big( b e^s-b +O\big(\frac{\log(n)}{n}\big) \Big)\Big)
\exp\Big(\frac{\log(n)}{k} \Big( a e^{-s}-a + O\big(\frac{\log(n)}{n}\big) \Big) \Big)
 \\
& = 
\exp\Big(\frac{\log(n)}{k} \Big( a e^{-s}+b e^s-a-b + O\big(\frac{\log(n)}{n}\big) \Big)\Big) ,
\end{align*}
where we use the Taylor expansion $\log(1+x)=x+O(x^2)$ to obtain the second equality.
By Chernoff bound, for any $s>0$, we have
\begin{equation} \label{eq:mmd}
\begin{aligned}
& P(A^1_i-A^0_i\ge t\log(n))\le
\frac{E[e^{s(A^1_i-A^0_i)}]}{e^{st\log(n)}}  \\
 \le & \exp\Big(\frac{\log(n)}{k} \Big( a e^{-s}+b e^s -kst -a-b + O\big(\frac{\log(n)}{n}\big) \Big)\Big)  .
 \end{aligned}
\end{equation}
Let $h(s):=a e^{-s}+be^s-kst$. We want to find $\min_{s>0} h(s)$ to plug into the above upper bound. Since 
$h'(s)=-ae^{-s}+be^s-kt$ and 
$f''(s)=ae^{-s}+be^s>0$, $f(s)$ is a convex function and takes global minimum at $s^\ast$ such that $h'(s^\ast)=0$. Next we show that $s^\ast>0$ for all $t>\frac{1}{k}(b-a)$, so $\min_{s>0}h(s)=h(s^\ast)$. Indeed, this follows directly from the facts that $h'(0)=b-a-kt<0=h'(s^\ast)$ and that $h'(s)$ is an increasing function. Taking $s^\ast=\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b)$ into \eqref{eq:mmd}, we obtain \eqref{eq:upba} for all $t\in(\frac{1}{2}(b-a), 0]$ and large enough $n$.
\end{proof}

Note that $A^r_i$ are functions of the underlying graph $G$.
Given a graph $G$, define 
$$
\tilde{D}_r(G):=|\{i\in[n]:A^r_i- A^0_i\ge 0\}|
\text{~~and~~}
\tilde{D}_{ri}(G):=\mathbbm{1}[A^r_i- A^0_i\ge 0] ,
$$
where $\mathbbm{1}[\cdot]$ is the indicator function. Then
$\tilde{D}_r(G)=\sum_{i=1}^n \tilde{D}_{ri}(G)$ and
$$
E[\tilde{D}_r(G)]=\sum_{i=1}^n E[\tilde{D}_{ri}(G)]
=\sum_{i=1}^n P( A^r_i- A^0_i\ge 0).
$$
Taking $t=0$ into \eqref{eq:upba}, we have
$P(A^r_i- A^0_i\ge 0)\le \exp\big(\log(n)(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big)$. Therefore,
$$
E[\tilde{D}_r(G)]\le n \exp\Big(\log(n)\big(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1) \big) \Big)
= n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)}.
$$
By Markov inequality,
\begin{equation} \label{eq:tD}
P\big(\tilde{D}_r(G)=0 \big) = 1-
P\big(\tilde{D}_r(G)\ge 1\big) \ge 1- E[\tilde{D}_r(G)]
\ge 1- n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)} .
\end{equation}
Since $\sqrt{a}-\sqrt{b} > \sqrt{k}$, we have
$P\big(\tilde{D}(G)=0 \big)= 1-o(1)$.


Next we calculate the ratio
$$
\frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)} .
$$
By \eqref{eq:isingma}, we have
\begin{align*}
\frac{P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)}
& = \sum_{r=1}^{k-1}\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i)
-\frac{2(k-1)\alpha\log(n)}{n} \Big) \\
& \le \sum_{r=1}^{k-1}\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i) \Big)  .
\end{align*}
Since $A^r_i-A^0_i$ takes integer value between $-n/k$ and $n/k$,
we can use indicator functions to write
\begin{align*}
& \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i) \Big) \\
= & \sum_{t\log(n)=-n/k}^{n/k}
\mathbbm{1}[A^r_i-A^0_i=t \log(n)] \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t \log(n) \Big) ,
\end{align*}
where the quantity $t\log(n)$ ranges over all integer values from $-n/k$ to $n/k$ in the summation on the second line.
Define $D_r(G,t):=|\{i\in[n]:A^r_i-A^0_i= t\log(n)\}|$ and notice that $D_r(G,t)=\sum_{i=1}^n \mathbbm{1}[A^r_i-A^0_i=t \log(n)]$.
Therefore,
\begin{equation}  \label{eq:fd}
\begin{aligned}
 & \frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)}
\le \sum_{i=1}^n \sum_{r=1}^{k-1} \exp\Big(2\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i) \Big) \\
= & \sum_{i=1}^n\sum_{r=1}^{k-1}
\hspace*{0.05in}
\sum_{t\log(n)=-n/k}^{n/k}
\mathbbm{1}[A^r_i-A^0_i=t \log(n)] \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t \log(n) \Big) \\
= & \sum_{r=1}^{k-1}\sum_{t\log(n)=-n/k}^{n/k}
D_r(G,t) \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t \log(n) \Big)
\end{aligned}
\end{equation}
Define a set 
$$
\cG_1:=\bigcap_{r=1}^{k-1} \{G:\tilde{D}_r(G)=0\}.
$$
By \eqref{eq:tD}, $P(G\in\cG_1)= 1-o(1)$. By definition of $\tilde{D}_r(G)$, $G\in\cG_1$ implies that $D_r(G,t)=0$ for all $t\ge 0$ and $r=1,\dots,k-1$. Therefore, for $G\in\cG_1$, we have
\begin{equation} \label{eq:lq}
\begin{aligned}
& \frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)}
\le \sum_{r=1}^{k-1}\sum_{t\log(n)=-n/k}^{-1}
D_r(G,t) \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t \log(n) \Big)   \\
\overset{(a)}{\le} & \sum_{r=1}^{k-1}\sum_{t\log(n)=-n/k}^{-1}
D_r(G,t) \exp\big(k\beta t \log(n) \big) \\
= & \sum_{r=1}^{k-1}\sum_{t\log(n)=-n/k}^{\lfloor\frac{b-a}{k}\log(n) \rfloor} D_r(G,t)
\exp\big(k\beta t \log(n) \big)  +  \sum_{r=1}^{k-1}\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1} D_r(G,t)
\exp\big(k\beta t \log(n) \big) \\
\le &  \sum_{r=1}^{k-1}\sum_{t\log(n)=-n/k}^{\lfloor\frac{b-a}{k}\log(n) \rfloor} D_r(G,t)
\exp\big(\beta(b-a)\log(n) \big)  +  \sum_{r=1}^{k-1}\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1} D_r(G,t)
\exp\big(k\beta t \log(n) \big)  \\
\overset{(b)}{\le} & n(k-1)
\exp\big(\beta(b-a)\log(n) \big)  + \sum_{r=1}^{k-1}\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1} D_r(G,t)
\exp\big(k\beta t \log(n) \big) ,
\end{aligned}
\end{equation}
where inequality $(a)$ holds because $t\log(n)$ only takes negative values in the summation, and inequality $(b)$ follows from the trivial upper bound
$\sum_{t\log(n)=-n/k}^{\lfloor\frac{b-a}{k}\log(n) \rfloor} D_r(G,t)\le n$.
Define a function
\begin{equation} \label{eq:gt}
f_{\beta}(t):=\frac{1}{k}\sqrt{k^2t^2+4ab} -t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big) -\frac{a+b}{k} +1 +k\beta t.
\end{equation}
Then for $t\in [\frac{1}{k}(b-a), 0]$, we have 
\begin{align*}
& E[D_r(G, t)
\exp\big(k\beta t \log(n) \big)] \\
= & \sum_{i=1}^n E[\mathbbm{1}[A^r_i-A^0_i =t \log(n)]] \exp\big(k\beta t \log(n) \big) \\
= & \sum_{i=1}^n P\big(A^r_i-A^0_i=t \log(n) \big) \exp\big(k\beta t \log(n) \big) \\
\le & \sum_{i=1}^n P\big(A^r_i-A^0_i \ge t \log(n) \big) \exp\big(k\beta t \log(n) \big) \\
\le &  \sum_{i=1}^n \exp\Big(\log(n)
\Big(\frac{1}{k}\sqrt{k^2t^2+4ab} -t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big) -\frac{a+b}{k} +k\beta t +o(1)\Big)\Big) \\
= &  n^{f_{\beta}(t) +o(1)} ,
\end{align*}
where the second inequality follows from \eqref{eq:upba}.
For $\epsilon>0$, define a set
$$
\cG^r(\epsilon):=\left\{
\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1} D_r(G,t)
\exp\big(k\beta t \log(n) \big)
\le \sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1}
 n^{f_{\beta}(t) + \epsilon}
\right\} .
$$
Then by Markov inequality,
\begin{equation} \label{eq:mk}
P(G\in \cG^r(\epsilon))\ge 1-n^{-(\epsilon-o(1))} >
1-n^{-\epsilon/2}
\end{equation}
for large $n$ and positive $\epsilon$.
Using \eqref{eq:lq}, we obtain that for $G\in\cG_1\cap \bigcap_{r=1}^{k-1}\cG^r(\epsilon)$,
\begin{equation}  \label{eq:zz}
\begin{aligned}
\frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)} 
& \le n(k-1)
\exp\big(\beta(b-a)\log(n) \big)  + (k-1)\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1}  n^{f_{\beta}(t) + \epsilon} \\
& = (k-1)n^{f_{\beta}((b-a)/k)} + (k-1)\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1}  n^{f_{\beta}(t) + \epsilon} ,
\end{aligned}
\end{equation}
where the equality follows from the fact that $f_{\beta}(\frac{1}{k}(b-a))=\beta(b-a)+1$.
Recall the definitions of the functions $g(\beta)$ and $\tilde{g}(\beta)$ in \eqref{eq:gbt}.
By Lemma~\ref{lm:tus} below, we have
$f_{\beta}(t)\le \tilde{g}(\beta)<0$ for all $t\le 0$.
Combining this with \eqref{eq:zz}, we obtain that for
$G\in\cG_1\cap\cG(\epsilon)$,
\begin{equation} \label{eq:lh}
\begin{aligned}
& \frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)} 
\le (k-1)n^{\tilde{g}(\beta)}
+ (k-1)\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1}  n^{\tilde{g}(\beta) + \epsilon}  \\
& \le (k-1)n^{\tilde{g}(\beta) + \epsilon}
\big(\frac{a-b}{k}\log(n)+1 \big)
< n^{\tilde{g}(\beta) + 2\epsilon} 
\end{aligned}
\end{equation}
for large $n$ and positive $\epsilon$.
Let $\epsilon=-\tilde{g}(\beta)/4>0$ and define
$$
\cG^{(1)}:=\cG_1\cap \bigcap_{r=1}^{k-1}\cG(-\tilde{g}(\beta)/4) .
$$
By \eqref{eq:lh}, for $G\in\cG^{(1)}$ we have
$$
\frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)} <
n^{\tilde{g}(\beta) /2} .
$$
By \eqref{eq:tD} and \eqref{eq:mk}, 
$$
P(G\in\cG^{(1)})\ge 1-(k-1)n^{\tilde{g}(\beta)/8}- (k-1)n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{2}+o(1)}> 1- 2(k-1)n^{\tilde{g}(\beta)/8},
$$
where the last inequality follows from the fact that $1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}\le
\tilde{g}(\beta)<\tilde{g}(\beta)/8< 0$.
This completes the proof of Proposition~\ref{prop:big} for the special case of $r=1$. 
Next we prove the two auxiliary lemmas used above.
\begin{lemma}[Elementary properties of $\beta^\ast$ defined in \eqref{eq:defstar}] \label{lm:ele}
Let $g(\beta)$ and $\tilde{g}(\beta)$ be the functions defined in \eqref{eq:gbt}. Assume that $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
Then,

\begin{enumerate}[label=(\roman*)]
\item The equation $g(\beta) = 0$ has two roots, and the smaller one of them is $\beta^\ast$.

\item Denote the other root as $\beta'$. Then
$\beta^\ast< \frac{1}{2k}\log\frac{a}{b} <\beta'$.

\item $g(\beta)<0$ for all $\beta^\ast< \beta \le \frac{1}{2k}\log\frac{a}{b}$.

\item $\tilde{g}(\beta)<0$ for all $\beta>\beta^\ast$.

\item $\tilde{g}(\beta)$ is a decreasing function in $[0,+\infty)$. 

\item $\tilde{g}(\beta)<1$ for all $\beta>0$.
\end{enumerate}
\end{lemma}
\begin{proof}
{\bf Proof of (i)}:
We write $x=e^{k\beta}$. Then $g(\beta)=0$ can be written as $bx^2-(a+b-k)x+a=0$. This quadratic equation has two roots if and only if $(a+b-k)^2-4ab>0$, which is guaranteed by the assumption $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
The two roots of $bx^2-(a+b-k)x+a=0$ are
$x^\ast=\frac{a+b-k - \sqrt{(a+b-k)^2-4ab}}{2b}$ and $x'=\frac{a+b-k + \sqrt{(a+b-k)^2-4ab}}{2b}$.
Therefore, $\beta^\ast=\frac{1}{k}\log(x^\ast)$ and $\beta'=\frac{1}{k}\log(x')$.
{\bf Proof of (ii)}:
Since $x^\ast x'=\frac{a}{b}=(\sqrt{\frac{a}{b}})^2$, we have $x^\ast<\sqrt{\frac{a}{b}}<x'$, so $\beta^\ast< \frac{1}{2k}\log\frac{a}{b} <\beta'$.
{\bf Proof of (iii)}:
Since $b>0$, $bx^2-(a+b-k)x+a<0$ if and only if $x^\ast<x<x'$. Therefore, $g(\beta)<0$ if and only if $\beta^\ast< \beta <\beta'$. This implies (iii). 
{\bf Proof of (iv)}: (iv) follows directly from (iii).
{\bf Proof of (v)}: $g'(\beta)=b e^{k\beta} - a e^{-k\beta}$, so $g'(\beta)<0$ for $0\le \beta<\frac{1}{2k}\log\frac{a}{b}$, and $g(\beta)$ takes minimum value at $\beta=\frac{1}{2k}\log\frac{a}{b}$. This implies (v).
{\bf Proof of (vi)}: (vi) follows directly from (v) and the fact that $\tilde{g}(0)=1$.
\end{proof}









\begin{lemma} \label{lm:tus}
Let $f_{\beta}(t)$ be the function defined in \eqref{eq:gt}. If $a>b>0$, then $f_{\beta}(t)\le \tilde{g}(\beta)$ for all $t\le 0$.
If $\sqrt{a}-\sqrt{b}>\sqrt{k}$
and $\beta>\beta^\ast$, then we further have $f_{\beta}(t)\le \tilde{g}(\beta)<0$ for all $t\le 0$.
\end{lemma}
\begin{proof}
The first and second derivatives are
$f_{\beta}'(t)=
-\log(\sqrt{k^2 t^2+4ab}+kt)+\log(2b) +k\beta$ and $f_{\beta}''(t)=-\frac{k}{\sqrt{k^2 t^2+4ab}}<0$.
Therefore $f_{\beta}(t)$ is a concave function and takes global maximum at $t^\ast$ such that $f_{\beta}'(t^\ast)=0$.
Simple calculation shows that 
$$
t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k} \quad \text{~and~} \quad
f_{\beta}(t^\ast)=\frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1
=g(\beta) .
$$
We divide the proof into two cases. {\bf Case 1}: If $\beta\ge \frac{1}{2k}\log\frac{a}{b}$, then $t^\ast\ge 0$. Since $f_{\beta}(t)$ is an increasing function for $t\le t^\ast$, we have $f_{\beta}(t)\le f_{\beta}(0)=1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}= \tilde{g}(\beta)$ for all $t\le 0$.
{\bf Case 2}: If $\beta< \frac{1}{2k}\log\frac{a}{b}$, then we simply use the global maximum $f_{\beta}(t^\ast)$ to upper bound $f_{\beta}(t)$, i.e., $f_{\beta}(t)\le f_{\beta}(t^\ast)=g(\beta)$ for all $t$.
Combining these two cases, we have $f_{\beta}(t)\le \tilde{g}(\beta)$ for all $t\le 0$ as long as $a>b>0$.
If $\sqrt{a}-\sqrt{b}>\sqrt{k}$ and $\beta>\beta^\ast$, then by property (iv) of Lemma~\ref{lm:ele} we further have $f_{\beta}(t)\le \tilde{g}(\beta)<0$ for all $t\le 0$.
\end{proof}
\subsection{Proof of Proposition~\ref{prop:big} for general $r$} \label{sect:gen}
We want to bound the ratio
$$
\sum_{\cI\subseteq[n]:|\cI|=r}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)}
$$
for all $r\le kn/\log^{1/3}(n)$.
To that end,
for $\cI\subseteq[n]$ and an index vector $v$ of $\cI$,
define the different parts of $\cI$ as
$$
\cI_j:=\{i\in\cI:X_i= \omega^j\}, j=0, 1, \dots, k-1 \text{ and } \cI_{j_1,j_2}:=\{i\in\cI:X_i= \omega^{j_1}, v_{\mathrm{index}(i)} = \omega^{j_2}\}
$$
and define 
$$
\nabla \cI:=\{\{i,j\}: \{i,j\} \not\subseteq [n]\setminus\cI\} .
$$
We further define
$$
\nabla\cI_+:=\{\{i,j\}\in\nabla\cI:X_i=X_j\}
\quad \text{and} \quad
\nabla\cI_v:=\{\{i,j\}\in\nabla\cI:X_j= v_{\mathrm{index}(i)} \cdot X_i\}.
$$
Then 
\begin{equation} \label{eq:partial}
\begin{aligned}
& |\nabla\cI_+|=\sum_{j=0}^{k-1}\Big(|\cI_j|(\frac{n}{k}-|\cI_j|) + \frac{|\cI_j| (|\cI_j|-1)}{2}\Big)
=\frac{n}{k}|\cI|- \frac{1}{2}\sum_{j=0}^{k-1}|\cI_j|(|\cI_j|+1) , \\
& |\nabla\cI_v|=\sum_{j_1}^{k-1}\sum_{j_2=1}^{k-1}|\cI_{j_1, j_2}|\frac{n}{k} =\frac{n}{k}|\cI|
.
\end{aligned}
\end{equation}
Given a graph $G$,
define
\begin{align*}
& A_{\cI}=A_{\cI}(G):=|\{\{i,j\}\in\nabla\cI\cap E(G):X_i=X_j\}| ,  \\
& B_{\cI,v}=B_{\cI,v}(G):=|\{\{i,j\}\in\nabla\cI\cap E(G):X_j= v_{\mathrm{index}(i)} \cdot X_i\}| .
\end{align*}




\begin{proposition}
For $t\in [\frac{1}{k}(b-a), 0]$
and $|\cI|\le kn/\log^{1/3}(n),v$
\begin{equation} \label{eq:upmpt}
\begin{aligned}
& P(B_{\cI,v}-A_{\cI}\ge t|\cI|\log(n))  \\
\le & \exp\Big(|\cI|\log(n)
\Big(\frac{1}{k}\sqrt{k^2 t^2 + 4ab} -t\big(\log(\sqrt{k^2 t^2 + 4ab}+t)-\log(2b) \big) -\frac{a+b}{k} 
+ O(\log^{-1/3}(n)) \Big)\Big) .
\end{aligned}
\end{equation}
\end{proposition}
\begin{proof}
By definition, $A_{\cI}\sim\Binom(|\nabla\cI_+|,\frac{a\log(n)}{n})$ and $B_{\cI,v}\sim\Binom(|\nabla\cI_v|,\frac{b\log(n)}{n})$, and they are independent. For $s>0$, the moment generating function of $B_{\cI,v}-A_{\cI}$ for $|\cI|\le kn/\log^{1/3}(n)$ can be bounded from above as follows:
\begin{align*}
& E[e^{s(B_{\cI,v}-A_{\cI})}] \\
& =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^s \Big)^{n|\cI|/k
}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-s} \Big)^{n |\cI|/k-\frac{1}{2}\sum_{j=0}^{k-1}(|\cI_j|+1)|\cI_j| }  \\
& \le 
\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^s \Big)^{n|\cI|/k}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-s} \Big)^{n |\cI|/k-|\cI|^2}
 \\
& \le
\exp\Big(\frac{|\cI|\log(n)}{k}(a e^{-s}+b e^s-a-b +\frac{ka|\cI|}{n})
+|\cI|O(\frac{\log^2(n)}{n})\Big) \\
& =
\exp\Big(\frac{|\cI|\log(n)}{k}(a e^{-s}+b e^s-a-b +
O(\log^{-1/3}(n))) \Big),
\end{align*}
where the first inequality follows from $1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^s>1$ and $1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-s}<1$; in the second inequality we use the Taylor expansion $\log(1+x)=x+O(x^2)$; the last equality follows from the assumption that $|\cI|\le kn/\log^{1/3}(n)$.
By Chernoff bound, for $s>0$, we have
\begin{align*} 
& P(B_{\cI,v}-A_{\cI}\ge t|{\cI}|\log(n))\le
\frac{E[e^{s(B_{\cI,v}-A_{\cI})}]}{e^{st|{\cI}|\log(n)}}  \\
\le & \exp\Big(\frac{{\cI}\log(n)}{k} \big(a e^{-s}+b e^s -kst -a-b
 + O(\log^{-1/3}(n)) \big)\Big)  .
\end{align*}
The rest of the proof is to find $s^\ast$ to minimize $a e^{-s}+b e^s -kst$ and take $s^\ast$ into the above bound. This is exactly the same as the proof of \eqref{eq:upba}, and we do not repeat it here.
\end{proof}


Given a graph $G$, define 
$$
\tilde{D}^{(r,v)}(G):=|\{\cI\subseteq[n],|\cI|=r:B_{\cI,v}- A_{\cI}\ge 0\}|
\text{~~and~~}
\tilde{D}^v_{\cI}(G):=\mathbbm{1}[B_{\cI,v}-A_{\cI}\ge 0] .
$$
 Then
$\tilde{D}^{(r,v)}(G)=\sum_{\cI\subseteq[n],|\cI|=r} \tilde{D}^v_{\cI}(G)$ and
$$
E[\tilde{D}^{(r, v)}(G)]=\sum_{\cI\subseteq[n],|\cI|=r} E[\tilde{D}^v_{\cI}(G)]
=\sum_{\cI\subseteq[n],|\cI|=r} P(B_{\cI,v}- A_{\cI}\ge 0).
$$
Taking $t=0$ into \eqref{eq:upmpt}, we have
$P(B_{\cI,v}- A_{\cI}\ge 0)\le \exp\big(|\cI|\log(n)(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) ) \big)$. Therefore,
\begin{align*}
& E[\tilde{D}^{(r,v)}(G)]\le \binom{n}{r} \exp\Big(r \log(n)\big(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) \big) \Big) \\
\le & n^r \exp\Big(r \log(n)\big(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) \big) \Big)
= n^{r( 1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) )}.
\end{align*}
By Markov inequality,
\begin{equation} \label{eq:Dk}
P\big(\tilde{D}^{(r,v)}(G)=0 \big) = 1-
P\big(\tilde{D}^{(r,v)}(G)\ge 1\big) \ge 1- E[\tilde{D}^{(r,v)}(G)]
\ge 1-  n^{r (1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) )} .
\end{equation}
Since $\sqrt{a}-\sqrt{b} > \sqrt{k}$, we have
$P\big(\tilde{D}^{(r,v)}(G)=0 \big)= 1-o(1)$.


By \eqref{eq:isingma}, we have
\begin{equation} \label{eq:ts}
\begin{aligned}
& \frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} \\
= & \sum_{v}\exp\Big(\beta\sum_{\{i,j\}\in E(G)} (I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)})
-I(X_i,X_j))
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} (I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)})
-I(X_i,X_j)) \Big) \\
=& \exp\Big((\beta+ \frac{\alpha \log(n)}{n})\sum_{\{i,j\}\in E(G) \cap \nabla \cI} (I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)})
-I(X_i,X_j))
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\} \in \nabla \cI}  (I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)})
-I(X_i,X_j))\Big) \\
\overset{(a)}{=}  & \exp\Big( k(\beta+ \frac{\alpha \log(n)}{n})( B_{\cI,v} - A_{\cI})
-\frac{k\alpha\log(n)}{n} \big( |\nabla \cI_v | - | \nabla \cI_+ | \big)\Big) \\
\overset{(b)}{=} & \exp\Big( k\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI,v}-A_{\cI})
- \frac{k\alpha\log(n)}{2n}\sum_{j=0}^{j-1} | \cI_j | ( |\cI_j| + 1)
\Big) \\
\le & \exp\Big( k\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI,v}-A_{\cI})
\Big) ,
\end{aligned}
\end{equation}
where $(a)$ follows from the fact that:
\begin{align*}
\sum_{\{i,j\}\in E(G) \cap \nabla \cI} I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)}) &= (k-1) B_{\cI, v} - \sum_{v'\neq v} B_{\cI, v'} - A_{\cI} \\
\sum_{\{i,j\}\in E(G) \cap \nabla \cI} I(X_i, X_j) &= (k-1)A_{\cI} - \sum_{v'} B_{\cI, v'} \\
\sum_{\{i,j\}\in \nabla \cI} I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)}) &= (k-1) |\nabla I_v| - \sum_{v'\neq v} | \nabla I_{v'}| - | \nabla I_{+}| \\
\sum_{\{i,j\}\in \nabla \cI} I(X_i, X_j) &= (k-1) | \nabla I_{+} | - \sum_{v'} | \nabla I_{v'} |
\end{align*}
and $(b)$ follows from \eqref{eq:partial}.
Since $B_{\cI,v}-A_{\cI}$ takes integer value between $-|\cI|n/k$ and $|\cI|n/k$,
we can use indicator functions to write
\begin{align*}
& \exp\Big(2\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI}-A_{\cI}) \Big) \\
= & \sum_{t|\cI|\log(n)=-|\cI|n/k}^{|\cI|n/k}
\mathbbm{1}[B_{\cI}-A_{\cI}=t|\cI| \log(n)] \exp\Big(2\big(\beta+\frac{\alpha\log(n)}{n} \big) t|\cI| \log(n) \Big) ,
\end{align*}
where the quantity $t|\cI|\log(n)$ ranges over all integer values from $-|\cI|n/2$ to $|\cI|n/2$ in the summation on the second line.
Define $D^{(k)}(G,t):=|\{\cI\subseteq[n],|\cI|=k:B_{\cI}-A_{\cI}= tk\log(n)\}|$ and notice that $D^{(k)}(G,t)=\sum_{\cI\subseteq[n],|\cI|=k} \mathbbm{1}[B_{\cI}-A_{\cI}= tk\log(n)]$.
Therefore,
\begin{align*}
 & \sum_{\cI\subseteq[n],|\cI|=r}
 \frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)}
\le \sum_{\cI\subseteq[n],|\cI|=k} \exp\Big(2\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI}-A_{\cI}) \Big) \\
= & \sum_{\cI\subseteq[n],|\cI|=k}
\hspace*{0.05in}
\sum_{tk\log(n)=-kn/2}^{kn/2}
\mathbbm{1}[B_{\cI}-A_{\cI}=tk \log(n)] \exp\Big(2\big(\beta+\frac{\alpha\log(n)}{n} \big) tk \log(n) \Big) \\
= & \sum_{tk\log(n)=-kn/2}^{kn/2}
D^{(k)}(G,t) \exp\Big(2\big(\beta+\frac{\alpha\log(n)}{n} \big) t k \log(n) \Big)
\end{align*}
Define a set 
\begin{equation} \label{eq:g1k}
\cG_1^{(k)}:=\{G:\tilde{D}^{(k)}(G)=0\}.
\end{equation}
By \eqref{eq:Dk}, $P(G\in\cG_1^{(k)})= 1-o(1)$. By definition of $\tilde{D}^{(k)}(G)$, $G\in\cG_1^{(k)}$ implies that $D^{(k)}(G,t)=0$ for all $t\ge 0$. Therefore, for $G\in\cG_1^{(k)}$, we have
\begin{equation} \label{eq:bk}
\begin{aligned}
& \sum_{\cI\subseteq[n],|\cI|=k}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} \\
\le & \sum_{tk\log(n)=-kn/2}^{-1}
D^{(k)}(G,t) \exp\Big(2\big(\beta+\frac{\alpha\log(n)}{n} \big) t k \log(n) \Big)   \\
\overset{(a)}{\le} & \sum_{tk\log(n)=-kn/2}^{-1}
D^{(k)}(G,t) \exp\big(2\beta t k \log(n) \big) \\
= & \sum_{tk\log(n)=-kn/2}^{\lfloor\frac{b-a}{2}k\log(n) \rfloor} D^{(k)}(G,t) \exp\big(2\beta t k \log(n) \big) \\
& \hspace*{1.5in} + \sum_{tk\log(n)=\lceil\frac{b-a}{2}k\log(n) \rceil}^{-1} D^{(k)}(G,t) \exp\big(2\beta t k \log(n) \big) \\
\le & \sum_{tk\log(n)=-kn/2}^{\lfloor\frac{b-a}{2}k\log(n) \rfloor} D^{(k)}(G,t)
\exp\big(\beta(b-a)k\log(n) \big) \\
& \hspace*{1.5in} + \sum_{tk\log(n)=\lceil\frac{b-a}{2}k\log(n) \rceil}^{-1} D^{(k)}(G,t) \exp\big(2\beta t k \log(n) \big) \\
\overset{(b)}{\le} & \binom{n}{k}
\exp\big(\beta(b-a) k \log(n) \big)  + \sum_{tk\log(n)=\lceil\frac{b-a}{2}k\log(n) \rceil}^{-1} D^{(k)}(G,t) \exp\big(2\beta t k \log(n) \big) ,
\end{aligned}
\end{equation}
where inequality $(a)$ holds because $tk\log(n)$ only takes negative values in the summation, and inequality $(b)$ follows from the trivial upper bound
$\sum_{tk\log(n)=-kn/2}^{\lfloor\frac{b-a}{2}k\log(n) \rfloor} D^{(k)}(G,t)\le \binom{n}{k}$.




Recall the function $f_{\beta}(t)$ defined in \eqref{eq:gt}.
Then for $t\in [\frac{1}{2}(b-a), 0]$, we have 
\begin{align*}
& E[D^{(k)}(G,t) \exp\big(2\beta t k \log(n) \big)] \\
= & \sum_{\cI\subseteq[n],|\cI|=k} E[\mathbbm{1}[B_{\cI}-A_{\cI}=t k \log(n)]] \exp\big(2\beta t k \log(n) \big) \\
= & \sum_{\cI\subseteq[n],|\cI|=k} P\big(B_{\cI}-A_{\cI}=t k \log(n) \big) \exp\big(2\beta t k \log(n) \big) \\
\le & \sum_{\cI\subseteq[n],|\cI|=k} P\big(B_{\cI}-A_{\cI} \ge t k \log(n) \big) \exp\big(2\beta t k \log(n) \big) \\
\le &  \sum_{\cI\subseteq[n],|\cI|=k} \exp\Big( k \log(n)
\Big(\sqrt{t^2+ab} -t\big(\log(\sqrt{t^2+ab}+t)-\log(b) \big) -\frac{a+b}{2} +2\beta t +o(1) \Big)\Big) \\
= & \binom{n}{k} n^{k(f_{\beta}(t)-1+o(1))} ,
\end{align*}
where the second inequality follows from \eqref{eq:upmpt}.
For $\epsilon>0$, define a set
\begin{equation} \label{eq:gep}
\begin{aligned}
\cG^{(k)}(\epsilon):=\left\{
\sum_{tk\log(n)=\lceil\frac{b-a}{2}k\log(n) \rceil}^{-1} D^{(k)}(G,t) \exp  \big(2\beta t k \log(n) \big) \hspace*{1.5in} \right. \\
\left.
 \le  \sum_{tk\log(n)=\lceil\frac{b-a}{2}k\log(n) \rceil}^{-1}
\binom{n}{k} n^{k(f_{\beta}(t)-1+\epsilon)}
\right\} .
\end{aligned}
\end{equation}
Then by Markov inequality,
\begin{equation} \label{eq:Gk}
P(G\in \cG^{(k)}(\epsilon))\ge 1-n^{-k(\epsilon-o(1))} >1-n^{-k\epsilon/2}
\end{equation}
for large $n$ and positive $\epsilon$.
Using \eqref{eq:bk}, we obtain that for $G\in\cG_1^{(k)}\cap\cG^{(k)}(\epsilon)$,
\begin{equation}  \label{eq:3l}
\begin{aligned}
& \sum_{\cI\subseteq[n],|\cI|=k}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)}  \\
\le & \binom{n}{k}
\exp\big(\beta(b-a) k \log(n) \big)  + \sum_{tk\log(n)=\lceil\frac{b-a}{2}k\log(n) \rceil}^{-1}
\binom{n}{k} n^{k(f_{\beta}(t)-1+\epsilon)} \\
= & \binom{n}{k} n^{k(f_{\beta}((b-a)/2)-1)} + \sum_{tk\log(n)=\lceil\frac{b-a}{2}k\log(n) \rceil}^{-1}
\binom{n}{k} n^{k(f_{\beta}(t)-1+\epsilon)} ,
\end{aligned}
\end{equation}
where the equality follows from the fact that $f_{\beta}(\frac{1}{2}(b-a))=\beta(b-a)+1$.
Recall the function $\tilde{g}(\beta)$ defined in \eqref{eq:gbt}, and recall from  Lemma~\ref{lm:tus} that
$f_{\beta}(t)\le \tilde{g}(\beta)<0$ for all $t\le 0$.
Using this in \eqref{eq:3l} together with the fact $\binom{n}{k}<n^k$, we obtain that for
$G\in\cG_1^{(k)}\cap\cG^{(k)}(\epsilon)$,
\begin{equation} \label{eq:wuh}
\begin{aligned}
& \sum_{\cI\subseteq[n],|\cI|=k}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} 
\le n^{k\tilde{g}(\beta)}
+ \sum_{tk\log(n)=\lceil\frac{b-a}{2}k\log(n) \rceil}^{-1}  n^{k(\tilde{g}(\beta) + \epsilon)}  \\
& \le n^{k(\tilde{g}(\beta) + \epsilon)}
\Big(\frac{a-b}{2}\log(n^k)+1 \Big)
< n^{k(\tilde{g}(\beta) + 2\epsilon)} 
\end{aligned}
\end{equation}
for large $n$ and positive $\epsilon$.kld
Let $\epsilon=-\tilde{g}(\beta)/4>0$ and define
$$
\cG^{(k)}:=\cG_1^{(k)}\cap\cG^{(k)}(-\tilde{g}(\beta)/4) .
$$
By \eqref{eq:wuh}, for $G\in\cG^{(k)}$ we have
$$
\sum_{\cI\subseteq[n],|\cI|=k}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} <
n^{k \tilde{g}(\beta) /2} .
$$
By \eqref{eq:Dk}, \eqref{eq:g1k} and \eqref{eq:Gk}, 
$$
P(G\in\cG^{(k)})\ge 1-n^{k\tilde{g}(\beta)/8}- n^{k (1-\frac{(\sqrt{a}-\sqrt{b})^2}{2} + o(1) )}> 1- 2 n^{k\tilde{g}(\beta)/8},
$$
where the last inequality follows from the fact that $1-\frac{(\sqrt{a}-\sqrt{b})^2}{2}\le
\tilde{g}(\beta)<\tilde{g}(\beta)/8< 0$.


\section{Exact recovery is not solvable when $\lfloor \frac{m+1}{2} \rfloor \beta < \beta^\ast$}\label{sect:converse}


\appendix
\section{Auxiliary lemmas used in Section}

\begin{lemma} \label{lm:bq}
For $0<\theta<1$,
\begin{align}
& P_{\SIBM}(\sigma_i \neq X_i
\big| \dist(\sigma,X) \le n^\theta) \le k n^{\theta-1}
\quad \text{for all~} i\in[n] , \label{eq:l1}\\
& P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI}
~\big| \dist(\sigma,X) \le n^\theta) \le \Big(\frac{n^\theta}{n/k - |\tilde{\cI}|}
\Big)^{|\tilde{\cI}|}
\quad \text{for all~} \tilde{\cI}\subseteq [n] .   \label{eq:l2}
\end{align}
\end{lemma}
\begin{proof}
Define $\dist_j(\sigma,X):=|\{i\in[n]:X_i=\omega^j, \sigma_i \neq X_i\}|$ for $j \in \{0, \dots, k-1\}$. Clearly, $\dist(\sigma,X)=\sum_{j=0}^{k-1} \dist_j(\sigma,X)$.

Inequality \eqref{eq:l1} follows immediately from the following equality:
$$
P_{\SIBM}(\sigma_i \neq X_i |
\dist_j(\sigma,X)=u_j,
j=0,\dots,k-1) 
= k u_r / n \textrm{ where } X_i = \omega^r
$$
Without loss of generality,
we only prove the case of $X_i=1 (r=0)$, and
we need the following definition for the proof of this equality:
For $\cI\subseteq[n]$, define $\cI_j:=\{i\in\cI:X_i=\omega^j\}$ Then
\begin{align*}
& P_{\SIBM}(\sigma_i \neq X_i |
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)  \\
= & \frac{P_{\SIBM}(\sigma_i \neq X_i ,
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)}
{P_{\SIBM}(
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)} \\
= & \frac{\sum_{\cI:i\in\cI_0,|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }
{\sum_{\cI: |\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) } \\
\overset{(a)}{=} & \frac{\binom{n/k-1}{u_0 -1}}
{\binom{n/k}{u_0} }
= ku_0/n ,
\end{align*}
where equality (a) follows from Lemma~\ref{lm:cc} below.

Similarly, inequality \eqref{eq:l2} follows from the following inequality:
For $u_j\geq |\tilde{\cI}_j|, j=0, \dots, k-1$,
\begin{align*}
& P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI} ~ \big|
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)  \\
= & \frac{P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI} ,
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)}{P_{\SIBM}(
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)} \\
= & \frac{\sum_{\cI:\tilde{\cI}_j\subseteq\cI_j,
|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }
{\sum_{\cI:|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }  \\
\overset{(a)}{=} & \prod_{j=0}^{k-1} \frac{\binom{n/k-|\tilde{\cI}_j|}{u_j -|\tilde{\cI}_j|} }
{\binom{n/k}{u_j} }
< \prod_{j=0}^{k-1}\Big(\frac{u_j}{n/k- |\tilde{\cI}_j|} \Big)^{|\tilde{\cI}_j|}
\\
< & \Big(\frac{\sum_{j=0}^{k-1} u_j}{n/k- |\tilde{\cI}|} \Big)^{|\tilde{\cI}|}  ,
\end{align*}
where equality (a) again follows from Lemma~\ref{lm:cc} below.
\end{proof}







\begin{lemma} \label{lm:cc}
Let $\cI,\cI'\subseteq[n]\setminus\{i\}$ be two subsets such that $|\cI_j|=|\cI_j'|$ for $j=0,\dots,k-1$. 
Then $P_{\SIBM}(\sigma=X^{(\sim\cI)}) = P_{\SIBM}(\sigma=X^{(\sim\cI')})$.
\end{lemma}

\bibliographystyle{plain}
\bibliography{exportlist}
\end{document}

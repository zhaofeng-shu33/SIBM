\documentclass{article}
\input{macros.tex}
\title{Extension of SIBM to multiple communities}
\author{Feng Zhao}
\begin{document}
\maketitle

\section{Problem setup and main results} \label{s:Preliminaries}
We first recall the definition of Symmetric Stochastic Block Model (SSBM) with $k$ communities and the definition of Ising model with $k$ state (See Definition 4 in \cite{Abbe17}).
\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
Let $n$ be a positive even integer, $W= \{1, \omega, \dots, \omega^{k-1}\}$ is a cyclic group with order $k$ and let $p,q\in[0,1]$ be two real numbers. Let $X=(X_1,\dots,X_n)\in W^n$, and let $G=([n],E(G))$ be a undirected graph with vertex set $[n]$ and edge set $E(G)$. The pair $(X,G)$ is drawn under $\SSBM(n,k,p,q)$ if 

\noindent
(i) $X$ is drawn uniformly with the constraint that $|\{v \in [n] : X_v = \omega^i\}| = \frac{n}{k}$;

\noindent
(ii) the vertices $i$ and $j$ in $G$ are connected with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$, independently of other pairs of vertices.
\end{definition}
 
From each element $\sigma$ in a symmetric group $S_k$, we can induce a permutation function $f$ on $x=(x_1,\dots,x_n)\in W^n$ as $f(x_i) = \gamma(i)$ ($\gamma$ is a function on $[k]$).
$x$ and $f(x)$ correspond to the same balanced partition, and the conditional distribution $P(G|X=x)$ is the same as $P(G|X=f(x))$ in the above definition. Therefore, we can only hope to recover $X$ from $G$ up to a global permutation.

In this paper, we focus on the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. In this regime, it is well known that exact recovery of $X$ (up to a global sign) from $G$ is possible if and only if $\sqrt{a}-\sqrt{b} > \sqrt{k}$  \cite{abbe2015exact}.
 
Given a partition/labeling $X$ on $n$ vertices, the SBM specifies how to generate a random graph on these $n$ vertices according to the labeling. In some sense, Ising model works in the opposite direction, i.e., given a graph $G=([n],E(G))$, Ising model defines a probability distribution on all possible labelings $W^n$ of these $n$ vertices. 

We define the indicator function $I(x, y)$ as:
\begin{equation}
I(x, y) = \begin{cases}
 k-1 & x = y \\
 -1 & x \neq y
\end{cases}
\end{equation}
% A general Ising model without external fields is a probability distribution on the configurations $\{\pm 1\}^n$ such that
% $$
% P(\sigma)=\frac{1}{Z}
% \exp(\sum_{i\neq j} J_{ij} \sigma_i \sigma_j) 
% \quad\quad
% \text{for every~}
% \sigma=(\sigma_1,\dots,\sigma_n) \in \{\pm 1\}^n ,
% $$
% where $Z=\sum_{\sigma \in \{\pm 1\}^n} \exp(\sum_{i\neq j} J_{ij} \sigma_i \sigma_j)$ is the normalizing constant.
% We incorporate the graphical structure of $G$ into the Ising model by setting $J_{ij}=\beta$ for all  $\{i,j\}\in E(G)$ and $J_{ij}=-\alpha\log(n)/n$ for all $\{i,j\}\notin E(G)$, where $\alpha,\beta>0$ are positive constants. 
% This is summarized in the following definition.

 
 \begin{definition}[Ising model]
Define an Ising model on a graph $G=([n],E(G))$ with parameters $\alpha,\beta>0$ as the probability distribution on the configurations $\sigma\in\{\pm 1\}^n$ such that\footnote{When there is only one sample, we usually denote it as $\bar{\sigma}$. When there are $m$ (independent) samples, we usually denote them as $\sigma^{(1)},\dots,\sigma^{(m)}$.}
\begin{equation} \label{eq:isingma}
P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\beta\sum_{\{i,j\}\in E(G)} I(\bar{\sigma}_i ,\bar{\sigma}_j)
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j)\Big) ,
\end{equation}
where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and 
\begin{equation}  \label{eq:zg}
Z_G(\alpha,\beta)=\sum_{\bar{\sigma} \in W^n} \exp\Big(\beta\sum_{\{i,j\}\in E(G)}I(\bar{\sigma}_i, \bar{\sigma}_j)
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j) \Big) 
\end{equation}
is the normalizing constant.
\end{definition}






By definition we always have $P_{\sigma|G}(\sigma=\bar{\sigma})=P_{\sigma|G}(\sigma=f(\bar{\sigma}))$ in the Ising model. Next we present our new model, the Stochastic Ising Block Model (SIBM), which can be viewed as a natural composition of the SSBM and the Ising model. In SIBM, we first draw a pair $(X,G)$ under $\SSBM(n,k, p,q)$.  Then we draw $m$ independent samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ from the Ising model on the graph $G$, where $\sigma^{(u)}\in W^n$ for all $u\in[m]$.

\begin{definition}[Stochastic Ising Block Model]
Let $n$ and $m$ be positive integers such that $n$ is even. Let $p,q\in[0,1]$ be two real numbers and let $\alpha,\beta>0$. The triple $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ is drawn under $\SIBM(n,k, p,q,\alpha,\beta,m)$ if

\noindent
(i) the pair $(X,G)$ is drawn under $\SSBM(n,p,q)$;

\noindent
(ii) for every $i\in[m]$, each sample $\sigma^{(i)}=(\sigma_1^{(i)},\dots,\sigma_n^{(i)}) \in W^n$ is drawn independently according to the distribution \eqref{eq:isingma}.
\end{definition}

Notice that we only draw the graph $G$ once in SIBM, and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ are drawn independently from the Ising model on the {\em same} graph $G$.
Our objective is to recover the underlying partition (or the ground truth) $X$ from the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$, and we would like to use the smallest possible number of samples to guarantee the exact recovery of $X$ up to a global permutation.
Below we use the notation $P_{\SIBM}(A):=E_G[P_{\sigma|G}(A)]$ for an event $A$, where the expectation $E_G$ is taken with respect to the distribution given by SSBM. In other words, $P_{\sigma|G}$ is the conditional distribution of  $\sigma$ given a fixed $G$ while $P_{\SIBM}$ is the joint distribution of both $\sigma$ and $G$.
By definition, we have $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f$ is induced from $S_k$. We use the set $\Gamma = \{f_{\gamma}(X) | \gamma \in S_k\}$ to
represent all vectors with permutations on $X$.

\begin{definition}[Exact recovery in SIBM]
Let $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k, p,q,\alpha,\beta,m)$.
We say that exact recovery is solvable for $\SIBM(n,p,q,\alpha,\beta,m)$ if there is an algorithm that takes $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ as inputs and outputs $\hat{X}=\hat{X}(\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ such that
$$
P_{\SIBM}(\hat{X} \in \Gamma) \to 1
\text{~~~as~} n\to\infty ,
$$
and we call $P_{\SIBM}(\hat{X} \in \Gamma)$ the success probability of the recovery/decoding algorithm.
\end{definition}


As mentioned above, we consider the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. By definition, the ground truth $X$, the graph $G$ and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ form a Markov chain $X\to G\to \{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Therefor, if we cannot recover $X$ from $G$, then there is no hope to recover $X$ from $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Thus a necessary condition for the exact recovery in SIBM is $\sqrt{a}-\sqrt{b}> \sqrt{k}$, and we will limit ourselves to this case throughout the paper.

\vspace*{.1in}
\noindent{\bf Main Problem:} \emph{For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, what is the smallest sample size $m^\ast$ such that exact recovery is solvable for $\SIBM(n,a\log(n)/n, b\log(n)/n,\alpha,\beta,m^\ast)$?}

Notation convention: $\dist(\sigma, X) := |\{ i \in [n]: \sigma_i \neq X_i\}|$.
For $\cI \subseteq [n]$, we denote the event $\sigma = X^{(\sim \cI)}$ as $\sigma_i \neq X_i$ for all $i \in \cI$ and $\sigma_i = X_i$ for all $i \not\in \cI$.

\section{Sketch of the proof}
\label{sect:sketch}

In this section, we illustrate the main ideas and explain the important steps in the proof of the main results. The complete proofs are given in Section~\ref{sect:aln}--\ref{sect:converse}.
As the first step, we prove that for $a>b>0$, if $\alpha>b\beta$, then all the samples are centered in $\Gamma$ with probability $1-o(1)$; if $\alpha<b\beta$, then all the samples are centered in $\Theta := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k\}$ where $\mathbf{1}_n$ is the all one vector with dimension $n$.

We use the concentration results for adjacency matrices of random graphs with independent edges to prove this. Let $A=A(G)$ be the adjacency matrix of the graph $G$. Then \eqref{eq:isingma} can be written as
\begin{align*}
 P_{\sigma|G}(\sigma=\bar{\sigma})  
=  \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j}\Big( \big(\beta+\frac{\alpha\log(n)}{n} \big) A
-\frac{\alpha\log(n)}{n} (J_n-I_n) \Big)_{ij} I(\bar{\sigma}_i,\bar{\sigma}_j) 
\Big)  ,
\end{align*}
where $J_n$ is the all one matrix and $I_n$ is the identity matrix, both of size $n\times n$.
Define a matrix
$
M:= \big(\beta+\frac{\alpha\log(n)}{n} \big) E[A|X]
-\frac{\alpha\log(n)}{n} (J_n-I_n).
$
Then we can further write \eqref{eq:isingma}
as
$$
P_{\sigma|G}(\sigma=\bar{\sigma})
= \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) + \frac{1}{2} \sum_{i,j}\big(\beta+\frac{\alpha\log(n)}{n} \big)  (A-E[A|X])_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) 
  \bar{\sigma}^T 
\Big)  .
$$
One can show that if $\alpha>b\beta$, then the maximizers of $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ consist of set $\Gamma$, and if $\alpha<b\beta$, then the maximizer set is $\Theta$.
Moreover, \cite{Hajek16} proved the concentration of $A$ around its expectation $E[A|X]$ in the sense that
$\|A-E[A|X]\| = O(\sqrt{\log(n)})$ with probability $1-o(1)$, we can further show that the error term above is bounded by
$$
\left|\frac{1}{2} \big(\beta+\frac{\alpha\log(n)}{n} \big)\sum_{i,j} (A-E[A|X])
 I( \bar{\sigma}_i, \bar{\sigma}_j) \right| = O \big( n \sqrt{\log(n)} \big)
  \text{~~for all~} \bar{\sigma}\in\{\pm 1\}^n  .
$$
This allows us to prove that in both cases (no matter $\alpha>b\beta$ or $\alpha<b\beta$), the Hamming distance between the samples and the maximizers of $\sum_{i,j}  M_{ij}I(\bar{\sigma}_i, \bar{\sigma}_j)$ is upper bounded by $kn/\log^{1/3}(n)=o(n)$.
More precisely, if $\alpha>b\beta$, then $\dist(\sigma^{(i)},\Gamma )< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$; if $\alpha<b\beta$, then $\dist(\sigma^{(i)}, \Theta)< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$; see Proposition~\ref{prop:1} for a rigorous proof.
In the latter case, each sample only take $\sum_{j=0}^{2n/\log^{1/3}(n)}\binom{n}{j}$ values, so each sample contains at most $\log_2(\sum_{j=0}^{2n/\log^{1/3}(n)}\binom{n}{j})=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$ bits of information about $X$. On the other hand, $X$ itself is uniformly distributed over a set of $\binom{n}{n/2}$ vectors, so one needs at least $\log_2\binom{n}{n/2}=\Theta(n)$ bits of information to recover $X$. Thus if $\alpha<b\beta$, then exact recovery of $X$ requires at least $\Omega(\frac{\log^{1/3}(n)}{\log\log(n)})\ge \Omega(\log^{1/4}(n))$ samples; see Proposition~\ref{prop:ab} for a rigorous proof.

For the rest of this section, we will focus on the case $\alpha>b\beta$ and establish the sharp threshold on the sample complexity. We first analyze the typical behavior of one sample and explain how to prove Theorem~\ref{thm:wt3}. Then we use the method developed for the one sample case to analyze the distribution of multiple samples, which allows us to prove Theorem~\ref{thm:wt2}. Note that Theorem~\ref{thm:wt1} follows directly from Theorem~\ref{thm:wt2}.

\section{Samples are concentrated around $\Gamma$ or $\Theta$} \label{sect:aln}
In this section, we show that for $a>b$, if $\alpha>b\beta$, then all the samples produced by $\SIBM(n, \linebreak[4]
a\log(n)/n, b\log(n)/n,\alpha,\beta,m)$ are very close to one element from $\Gamma$. More precisely, they differ from the ground truth $\Theta$ in at most $kn/\log^{1/3}(n)$ coordinates.
On the other hand, if $\alpha<b\beta$, then the samples differ from  $\pm \Theta$ in at most $kn/\log^{1/3}(n)$ coordinates.
For the latter case, we prove that the number of samples needed for exact recovery of $X$ is at least $\Omega(\log^{1/4}(n))$.

Let $A=A(G)$ be the adjacency matrix of $G$. Then \eqref{eq:isingma} can be written as
\begin{align*}
& P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{\beta}{2} \sum_{i,j} A_{ij} I(\bar{\sigma}_i,\bar{\sigma}_j)
-\frac{\alpha\log(n)}{2n} \sum_{i,j} (J_n-I_n-A)_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)
\Big)  \\
= & \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j} \Big( \big(\beta+\frac{\alpha\log(n)}{n} \big) A
-\frac{\alpha\log(n)}{n} (J_n-I_n) \Big)_{ij} I(\bar{\sigma}_i,\bar{\sigma}_j)
\Big),
\end{align*}
where $J_n$ is the all one matrix and $I_n$ is the identity matrix, both of size $n\times n$.
Conditioned on the ground truth $X$,
$A-E[A|X]$ is a symmetric matrix whose upper triangular part consists of independent entries. According to Theorem~5 in \cite{Hajek16}, the spectral norm of $A-E[A|X]$ is upper bounded by $O(\sqrt{\log(n)})$.
\begin{theorem}[Theorem~5 in \cite{Hajek16}] \label{thm:a2}
For any $r>0$, there exists $c>0$ such that the spectral norm of $A-E[A|X]$ satisfies
$$
P\big(\|A-E[A|X]\| \le c\sqrt{\log(n)} \big)\ge 1-n^{-r} .
$$
\end{theorem}
Define a matrix
$$
M:= \big(\beta+\frac{\alpha\log(n)}{n} \big) E[A|X]
-\frac{\alpha\log(n)}{n} (J_n-I_n).
$$
Then we can further write \eqref{eq:isingma}
as
\begin{equation} \label{eq:M}
P_{\sigma|G}(\sigma=\bar{\sigma})
= \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j}M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) + \frac{1}{2}\big(\beta+\frac{\alpha\log(n)}{n} \big)  \sum_{i,j}  (A-E[A|X])_{ij}
 I(\bar{\sigma}_i, \bar{\sigma}_j)  
\Big)  .
\end{equation}


By definition, all the diagonal entries of $M$ are $0$.
For $i\neq j$, 
$$
M_{ij}=\left\{ 
\begin{array}{cc}
 \big(a\beta-\alpha+\frac{a\alpha\log(n)}{n} \big) \frac{\log(n)}{n} = a' + O(\frac{\log^2 n}{n^2})
   & \text{~if~} X_i=X_j \\
  \big(b\beta-\alpha+\frac{b\alpha\log(n)}{n} \big) \frac{\log(n)}{n} = b' + O(\frac{\log^2 n}{n^2})
   & \text{~if~} X_i\neq X_j
\end{array}
\right. .
$$
where $a' =(a\beta - \alpha) \frac{\log n }{n}, b'=(b\beta - \alpha) \frac{\log n}{n}$.
Given $\bar{\sigma}\in\{1, \omega, \dots, \omega^{k-1}\}^n$, define a $k\times k$ matrix $\Xi$ with 
$\Xi_{ij} = |\{k \in [n]: X_k = \omega^{i-1}, \sigma_k = \omega^{j-1}\}|$. It can be seen that each row of $\Xi$ sums to $\frac{n}{k}$. Define $Q_{ij} = \sum_{r,s=1, r<s}^k (\Xi_{ir} - \Xi_{is})(\Xi_{jr} - \Xi_{js})$
Therefore,
\begin{equation} \label{eq:sMs}
\begin{aligned}
 \sum_{i,j}M_{ij}I(\bar{\sigma}_i, \bar{\sigma}_j)
= & a'\sum_{i=1}^k \big( (k-1)(\sum_{r=1}^k \Xi_{ir}^2) - \sum_{r,s=1,r \neq s}^k \Xi_{ir}\Xi_{is} \big) \\
+ & b'\sum_{i,j=1, i\neq j}^k  \big( (k-1)  (\Xi_{ir} \Xi_{jr}) - \sum_{r,s=1,r \neq s}^k \Xi_{ir}\Xi_{js}  \big) + O(\log^2 n)\\
= & a' \sum_{i=1}^k Q_{ii} + b' \sum_{i,j=1,i\neq j}^k Q_{ij} + O(\log^2 n ) \\
 = & ( a' - b') \sum_{i=1}^k Q_{ii} + b' \sum_{i,j=1}^k Q_{ij} + O(\log^2 n ) 
\end{aligned}
\end{equation}
Further we have:
$$
\sum_{i,j=1}^k Q_{ij} = \sum_{r,s=1, r<s}^k (\sum_{i=1}^k \Xi_{ir} - \sum_{i=1}^k \Xi_{is})^2
$$
Therefore, both the coefficients of $(a'-b')$ and $b'$ are non-negative.
According to \eqref{eq:M}, the configuration $\bar{\sigma}$ that maximizes $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ is (roughly) the most likely output of the Ising model.
Since we assume $a>b$, $a'-b' = (a-b) \beta > 0$ and its coefficient term $\sum_{i=1}^k Q_{ii}$ takes maximum when there is only one non-zero term $\frac{n}{k}$ on each row of the matrix $\Xi$. Below we discuss the sign of $b'$.

If $b\beta<\alpha$, then $b' < 0$ and we should let $\sum_{i,j=1}^k Q_{ij} = 0$ to make the whole expression larger. This is equivalent to say the summation of each column of the matrix $\Xi$ are all the same. Combining the two conditions together we can see the maximum value is
the representation of $S_k$ in $k\times k$ matrix form multiplied by a coefficient $\frac{n}{k}$. And it is further equivalent to the statement $\sigma \in \Gamma$.

If $b\beta>\alpha$, then $b' > 0$ and we should take the maximum value of $\sum_{i,j=1}^k Q_{ij}$ to make the whole expression larger. We can show that in this case, the maximizer is taken when one column of $\Xi$ is $\frac{n}{k}  \mathbf{1}_k$ and other columns are all zero. And it is further equivalent to the statement $\sigma \in \Theta$.

To summarize, if $b\beta<\alpha$, then $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ takes maximum at $\sigma \in \Gamma$.
If $b\beta>\alpha$, then the maximum is at$\sigma \in \Theta$.
Taking into account the effect of the error term $ \sum_{i,j}  (A-E[A|X])_{ij}
 I(\bar{\sigma}_i, \bar{\sigma}_j)  $ in \eqref{eq:M}, we have the following proposition:

\begin{proposition} \label{prop:1}
Let $a>b>0$ and $\alpha,\beta>0$ be constants. Let $m$ be a positive integer that is upper bounded by some polynomial of $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n, \alpha,\beta, m) .
$$
If $b\beta <\alpha$, then for any (arbitrarily large) $r>0$, there exists $n_0(r)$ such that for all even integers $n>n_0(\delta, r)$,
$$
P_{\SIBM} \Big(\dist(\sigma^{(i)}, \Gamma)< kn/\log^{1/3}(n)
\text{~~for all~} i\in[m] \Big) \ge 1- n^{-r} .
$$
If $b\beta >\alpha$, then for any (arbitrarily large) $r>0$, there exists $n_0(r)$ such that for all even integers $n>n_0(\delta, r)$,
$$
P_{\SIBM} \Big(\dist(\sigma^{(i)},\pm \Theta)< kn/\log^{1/3}(n)
\text{~~for all~} i\in[m] \Big) \ge 1- n^{-r} .
$$
\end{proposition}
\begin{proof}
We only prove the case of $b\beta <\alpha$ as the proof of the other case is virtually identical.
Define
$$
\gamma^{r,s}_i = \begin{cases}
1 & \sigma_i = \omega^r \\
-1 & \sigma_i = \omega^s \\
0 & \textrm{ otherwise}
\end{cases}
$$
Then we can show that
$$
\sum_{i,j} I(\sigma_i, \sigma_j) (A-\mathbb{E}[A|X]) = \sum_{r,s} (\gamma^{r,s})^T (A-\mathbb{E}[A|X])\gamma^{r,s}
$$
Since $A-E[A|X]$ is a symmetric matrix,
$$
|\bar{\gamma}  (A-E[A|X]) \bar{\gamma}^T|\le
\|A-E[A|X]\| \bar{\gamma} \bar{\gamma}^T=n \|A-E[A|X]\|
$$
for every $\bar{\gamma}\in\{\pm 1\}^n$.
Therefore, by Theorem~\ref{thm:a2}, for any $r>0$, there is $c>0$ such that
\begin{equation} \label{eq:cnA}
\left|\frac{1}{2} \big(\beta+\frac{\alpha\log(n)}{n} \big)\sum_{i,j} I(\sigma_i, \sigma_j) (A-\mathbb{E}[A|X]) 
\right| \le c\binom{k}{2} n \sqrt{\log(n)}
  \text{~~for all~} \bar{\sigma}\in W^n
\end{equation}
with probability at least $1-n^{-2r}$.

Define a small neighborhood of $\Xi^*$ as
$$
N(\Xi^*) := \{\Xi |\, |\Xi_{i,j} - \Xi_{i,j}| \leq \frac{n}{\log^{1/3} n}, 1\leq i,j\leq 3\}
$$
We use the $L_{\infty}$ norm for the matrix.
 When there is only one non-zero term $\frac{n}{k}$ on each row of the matrix $\Xi$, we call this matrix a ''Corner Matrix''.
 We use $\Gamma'$ to denote all corner matrices, then $|\Gamma'|=k^k$ and $\Gamma \subset \Gamma'$. The neighborhood of all corner matrice is denoted by $N(\Gamma')$. Similary we can define $N(\Gamma)$. 

$N(\Gamma)$ consists of $\bar{\sigma}$'s that differ from $\Gamma$ in at most $kn/\log^{1/3}(n)$ coordinates.
Given a graph $G$ whose adjacency matrix satisfies \eqref{eq:cnA}, we will show that $P_{\sigma|G}(\sigma\notin N(\Gamma))< e^{-n}$. Since $P_{\sigma|G}(\sigma=X)<1$, it suffices to prove that
$\frac{P_{\sigma|G}(\sigma\notin N(\Gamma))}{P_{\sigma|G}(\sigma=X)}<e^{-n}$. 

Since  $N(\Gamma)^c=N(\Gamma')^c\cup N(\Gamma' \backslash \Gamma)$.
Next we prove that for every $G$ satisfying \eqref{eq:cnA} and every
$\bar{\sigma}\in N(\Gamma')^c\cup N(\Gamma' \backslash \Gamma)$, 
\begin{equation} \label{eq:xmb}
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}{P_{\sigma|G}(\sigma=X)}<k^{-n}e^{-n} .
\end{equation}
Together with the trivial upper bound $|N(\Gamma')^c\cup N(\Gamma' \backslash \Gamma)|<k^n$, this implies that $\frac{P_{\sigma|G}(\sigma\notin N(\Gamma))}{P_{\sigma|G}(\sigma=X)}<e^{-n}$.

We first prove \eqref{eq:xmb} for $\bar{\sigma}\in N(\Gamma')^c$.
Observe that $\sigma=X$ corresponds to $\Xi=\frac{n}{k} I_k$. By \eqref{eq:sMs}, we have
\begin{equation} \label{eq:X}
\begin{aligned}
P_{\sigma|G}(\sigma=X) & =\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{1}{2} X M X^T + O(n\sqrt{\log(n)}) \Big)  \\
& =\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{(a-b)\beta(k-1)}{2k}n\log(n) + O(n\sqrt{\log(n)}) \Big) .
\end{aligned}
\end{equation}
On the other hand, if $\bar{\sigma}\in N(\Gamma')^c$, using the definition of $\Xi$ we can find an 1-1 correspondence
between $\bar{\sigma}$ and $\bar{\Xi}$. For $\bar{\Xi}$ we can find one place where 
$\frac{n}{\log^{1/3} n} < |\bar{\Xi}_{ij}| < \frac{n}{k}- \frac{n}{\log^{1/3} n}$. Then
$$
Q_{ii} < (\frac{n}{k}-\frac{2n}{\log^{1/3} n})^2 + (k-2)\Big( (\frac{n}{k} - \frac{n}{\log^{1/3} n})^2 + (\frac{n}{\log^{1/3} n})^2 \Big)
$$

$$
\sum_{r=1}^k Q_{rr} < \frac{k-1}{2k}n^2 - \frac{2n^2}{\log^{1/3} n}
+O(n^2/\log^{2/3}(n)).
$$
Since $b' = b\beta-\alpha<0$ and $\sum_{i,j=1}^k Q_{ij} \geq 0$, by \eqref{eq:sMs} we have
$$
\frac{1}{2}\bar{\sigma} M \bar{\sigma}^T  
\le \frac{a\beta-b\beta}{4}n\log(n)
-(a\beta-b\beta)n\log^{2/3}(n)
+O(n\log^{1/3}(n)) ,
$$
and so
\begin{align*}
P_{\sigma|G}(\sigma=\bar{\sigma}) \le \frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{a\beta-b\beta}{4}n\log(n) 
-(a\beta-b\beta)n\log^{2/3}(n)
+ O(n\sqrt{\log(n)}) \Big) .
\end{align*}
Combining this with \eqref{eq:X}, we have
$$
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}{P_{\sigma|G}(\sigma=X)}
\le \exp\Big(-(a\beta-b\beta)n\log^{2/3}(n)
+ O(n\sqrt{\log(n)}) \Big)
<k^{-n} e^{-n}
$$
for all $\bar{\sigma}\in N(\Gamma')^c$ and all $G$ satisfying \eqref{eq:cnA}. 

For $\bar{\sigma}\in N(\Gamma' \backslash \Gamma)$, we can find $r,s$ such that
$\sum_{i=1}^k \Xi_{ir} \geq \frac{2n}{k} - \frac{2n}{\log^{1/3} n }$ and 
$\sum_{i=1}^k \Xi_{is} \leq \frac{2n}{\log^{1/3} n }$, Therefore
$$
\sum_{i,j=1}^k Q_{ij} \geq (\sum_{i=1}^k \Xi_{ir} - \sum_{i=1}^k \Xi_{is})^2 \geq \frac{n^2}{k^2}
$$
for large $n$. Since $b' = b\beta-\alpha<0$, this implies that
$$
\frac{1}{2}\sum_{i,j=1}^k Q_{ij}
\frac{(b\beta-\alpha)\log(n)}{n}
< - \frac{\alpha-b\beta}{2k^2} n\log(n) ,
$$
so
$$
\frac{1}{2}\bar{\sigma} M \bar{\sigma}^T  
\le \frac{(a-b)\beta(k-1)}{2k}n\log(n)
- \frac{\alpha-b\beta}{2k^2} n\log(n) .
$$
Therefore,
\begin{align*}
P_{\sigma|G}(\sigma=\bar{\sigma}) \le \frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{(a-b)\beta(k-1)}{2k}n\log(n) 
-\frac{\alpha-b\beta}{2k^2} n\log(n)
+ O(n\sqrt{\log(n)}) \Big) .
\end{align*}
Combining this with \eqref{eq:X}, we have
$$
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}{P_{\sigma|G}(\sigma=X)}
\le \exp\Big(-\frac{\alpha-b\beta}{2k^2} n\log(n)
+ O(n\sqrt{\log(n)}) \Big)
<k^{-n} e^{-n}
$$
for all $\bar{\sigma}\in N(\Gamma' \backslash \Gamma)$ and all $G$ satisfying \eqref{eq:cnA}.

Now we have shown that for a single sample $\sigma$ produced by the SIBM, $P_{\sigma|G}(\sigma\in\Gamma)\ge 1- e^{-n}$ provided that $G$ satisfies \eqref{eq:cnA}. By the union bound, for $m$ independent samples $\sigma^{(1)},\dots,\sigma^{(m)}$ produced by the SIBM, $P_{\sigma|G}(\sigma^{(1)},\dots,\sigma^{(m)}\in\Gamma)\ge 1- m e^{-n}$ provided that $G$ satisfies \eqref{eq:cnA}.
We also know that $G$ satisfies \eqref{eq:cnA} with probability at least $1-n^{-2r}$, and by assumption $m$ is upper bounded by some polynomial of $n$. Therefore, the overall probability of $\sigma^{(1)},\dots,\sigma^{(m)}\in\Gamma$ is at least $1-n^{-r}$ when $n$ is large enough. This completes the proof of the proposition.
\end{proof}
\begin{proposition}  \label{prop:ab}
Let $a>b>0$ and $\alpha,\beta>0$ be constants. Let $m$ be a positive integer that is upper bounded by some polynomial of $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
$$
If $\alpha<b\beta$, then it is not possible to recover $X$ from the samples when $m=O(\log^{1/4}(n))$.
\end{proposition}

\begin{proof}
First observe that there are $\frac{n!}{(n/k)^k}$ balanced partitions, so one needs at least $\log_k \frac{n!}{(n/k)^k}=\Theta(n)$ bits to recover $X$.
By Proposition~\ref{prop:1}, with probability $1-o(n^{-4})$, $\dist(\sigma^{(i)}, \Theta)< kn/\log^{1/3}(n)$
for all $i\in[m]$. Therefore, each $\sigma^{(i)}$ takes at most
$$
T:=\sum_{j=0}^{kn/\log^{1/3}(n)} \binom{n}{j}(k-1)^j
$$
values, so each $\sigma^{(i)}$ contains at most $\log_k T$ bits of information. Next we prove that $\log_k T=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$, so we need at least $\Omega(\frac{\log^{1/3}(n)}{\log\log(n)})$ samples to recover $X$, which proves the proposition.

In order to upper bound $T$, we define a binomial random variable $Y\sim\Binom(n,\frac{k-1}{k})$. Then
$$
T=k^n P(Y\le kn/\log^{1/3}(n))
= k^n P(Y\ge n- kn/\log^{1/3}(n)).
$$
The moment generating function of $Y$ is $(\frac{1}{k}+\frac{k-1}{k}e^s)^n$. By Chernoff bound, for any $s>0$,
$$
P(Y\ge n- kn/\log^{1/3}(n)) \le
(\frac{1}{k}+\frac{k-1}{k}e^s)^n e^{-sn}
e^{ksn/\log^{1/3}(n)}
= k^{-n} (k-1+e^{-s})^n e^{ksn/\log^{1/3}(n)} .
$$
As a consequence, for any $s>0$,
$$
\log_k T\le n\Big(\log_k(k-1+e^{-s})
+\frac{ks}{\log^{1/3}(n)}\log_k e \Big) .
$$
Taking $s=\log\log(n)$ into this bound, we obtain that $\log_2 T=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$.
\end{proof}

\section{Exact recovery is not solvable when $\lfloor \frac{m+1}{2} \rfloor \beta < \beta^\ast$}
\label{sect:converse}

\appendix
\section{Auxiliary lemmas used in Section}

\begin{lemma} \label{lm:bq}
For $0<\theta<1$,
\begin{align}
& P_{\SIBM}(\sigma_i \neq X_i
\big| \dist(\sigma,X) \le n^\theta) \le k n^{\theta-1}
\quad \text{for all~} i\in[n] , \label{eq:l1}\\
& P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI}
~\big| \dist(\sigma,X) \le n^\theta) \le \Big(\frac{n^\theta}{n/k - |\tilde{\cI}|}
\Big)^{|\tilde{\cI}|}
\quad \text{for all~} \tilde{\cI}\subseteq [n] .   \label{eq:l2}
\end{align}
\end{lemma}
\begin{proof}
Define $\dist_j(\sigma,X):=|\{i\in[n]:X_i=\omega^j, \sigma_i \neq X_i\}|$ for $j \in \{0, \dots, k-1\}$. Clearly, $\dist(\sigma,X)=\sum_{j=0}^{k-1} \dist_j(\sigma,X)$.

Inequality \eqref{eq:l1} follows immediately from the following equality:
$$
P_{\SIBM}(\sigma_i \neq X_i |
\dist_j(\sigma,X)=u_j,
j=0,\dots,k-1) 
= k u_r / n \textrm{ where } X_i = \omega^r
$$
Without loss of generality,
we only prove the case of $X_i=1 (r=0)$, and
we need the following definition for the proof of this equality:
For $\cI\subseteq[n]$, define $\cI_j:=\{i\in\cI:X_i=\omega^j\}$ Then
\begin{align*}
& P_{\SIBM}(\sigma_i \neq X_i |
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)  \\
= & \frac{P_{\SIBM}(\sigma_i \neq X_i ,
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)}
{P_{\SIBM}(
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)} \\
= & \frac{\sum_{\cI:i\in\cI_0,|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }
{\sum_{\cI: |\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) } \\
\overset{(a)}{=} & \frac{\binom{n/k-1}{u_0 -1}}
{\binom{n/k}{u_0} }
= ku_0/n ,
\end{align*}
where equality (a) follows from Lemma~\ref{lm:cc} below.

Similarly, inequality \eqref{eq:l2} follows from the following inequality:
For $u_j\geq |\tilde{\cI}_j|, j=0, \dots, k-1$,
\begin{align*}
& P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI} ~ \big|
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)  \\
= & \frac{P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI} ,
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)}{P_{\SIBM}(
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)} \\
= & \frac{\sum_{\cI:\tilde{\cI}_j\subseteq\cI_j,
|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }
{\sum_{\cI:|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }  \\
\overset{(a)}{=} & \prod_{j=0}^{k-1} \frac{\binom{n/k-|\tilde{\cI}_j|}{u_j -|\tilde{\cI}_j|} }
{\binom{n/k}{u_j} }
< \prod_{j=0}^{k-1}\Big(\frac{u_j}{n/k- |\tilde{\cI}_j|} \Big)^{|\tilde{\cI}_j|}
\\
< & \Big(\frac{\sum_{j=0}^{k-1} u_j}{n/k- |\tilde{\cI}|} \Big)^{|\tilde{\cI}|}  ,
\end{align*}
where equality (a) again follows from Lemma~\ref{lm:cc} below.
\end{proof}







\begin{lemma} \label{lm:cc}
Let $\cI,\cI'\subseteq[n]\setminus\{i\}$ be two subsets such that $|\cI_j|=|\cI_j'|$ for $j=0,\dots,k-1$. 
Then $P_{\SIBM}(\sigma=X^{(\sim\cI)}) = P_{\SIBM}(\sigma=X^{(\sim\cI')})$.
\end{lemma}

\bibliographystyle{plain}
\bibliography{exportlist}
\end{document}

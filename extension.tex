\documentclass{article}
\input{macros.tex}
\title{Extension of SIBM to multiple communities}
\author{Feng Zhao}
\begin{document}
\maketitle

\section{Problem setup and main results} \label{s:Preliminaries}
We first recall the definition of Symmetric Stochastic Block Model (SSBM) with $k$ communities and the definition of Ising model with $k$ state (See Definition 4 in \cite{Abbe17}).
\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
Let $n$ be a positive even integer, $W= \{1, \omega, \dots, \omega^{k-1}\}$ is a cyclic group with order $k$ and let $p,q\in[0,1]$ be two real numbers. Let $X=(X_1,\dots,X_n)\in W^n$, and let $G=([n],E(G))$ be a undirected graph with vertex set $[n]$ and edge set $E(G)$. The pair $(X,G)$ is drawn under $\SSBM(n,k,p,q)$ if 

\noindent
(i) $X$ is drawn uniformly with the constraint that $|\{v \in [n] : X_v = \omega^i\}| = \frac{n}{k}$;

\noindent
(ii) the vertices $i$ and $j$ in $G$ are connected with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$, independently of other pairs of vertices.
\end{definition}
 
From each element $\sigma$ in a symmetric group $S_k$, we can induce a permutation function $f$ on $x=(x_1,\dots,x_n)\in W^n$ as $f(x_i) = \gamma(i)$ ($\gamma$ is a function on $[k]$).
$x$ and $f(x)$ correspond to the same balanced partition, and the conditional distribution $P(G|X=x)$ is the same as $P(G|X=f(x))$ in the above definition. Therefore, we can only hope to recover $X$ from $G$ up to a global permutation.

In this paper, we focus on the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. In this regime, it is well known that exact recovery of $X$ (up to a global sign) from $G$ is possible if and only if $\sqrt{a}-\sqrt{b} > \sqrt{k}$  \cite{abbe2015exact}.
 
Given a partition/labeling $X$ on $n$ vertices, the SBM specifies how to generate a random graph on these $n$ vertices according to the labeling. In some sense, Ising model works in the opposite direction, i.e., given a graph $G=([n],E(G))$, Ising model defines a probability distribution on all possible labelings $W^n$ of these $n$ vertices. 

We define the indicator function $I(x, y)$ as:
\begin{equation}
I(x, y) = \begin{cases}
 k-1 & x = y \\
 -1 & x \neq y
\end{cases}
\end{equation}
% A general Ising model without external fields is a probability distribution on the configurations $\{\pm 1\}^n$ such that
% $$
% P(\sigma)=\frac{1}{Z}
% \exp(\sum_{i\neq j} J_{ij} \sigma_i \sigma_j) 
% \quad\quad
% \text{for every~}
% \sigma=(\sigma_1,\dots,\sigma_n) \in \{\pm 1\}^n ,
% $$
% where $Z=\sum_{\sigma \in \{\pm 1\}^n} \exp(\sum_{i\neq j} J_{ij} \sigma_i \sigma_j)$ is the normalizing constant.
% We incorporate the graphical structure of $G$ into the Ising model by setting $J_{ij}=\beta$ for all  $\{i,j\}\in E(G)$ and $J_{ij}=-\alpha\log(n)/n$ for all $\{i,j\}\notin E(G)$, where $\alpha,\beta>0$ are positive constants. 
% This is summarized in the following definition.

 
 \begin{definition}[Ising model]
Define an Ising model on a graph $G=([n],E(G))$ with parameters $\alpha,\beta>0$ as the probability distribution on the configurations $\sigma\in W^n$ such that\footnote{When there is only one sample, we usually denote it as $\bar{\sigma}$. When there are $m$ (independent) samples, we usually denote them as $\sigma^{(1)},\dots,\sigma^{(m)}$.}
\begin{equation} \label{eq:isingma}
P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\beta\sum_{\{i,j\}\in E(G)} I(\bar{\sigma}_i ,\bar{\sigma}_j)
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j)\Big) ,
\end{equation}
where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and 
\begin{equation}  \label{eq:zg}
Z_G(\alpha,\beta)=\sum_{\bar{\sigma} \in W^n} \exp\Big(\beta\sum_{\{i,j\}\in E(G)}I(\bar{\sigma}_i, \bar{\sigma}_j)
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j) \Big) 
\end{equation}
is the normalizing constant.
\end{definition}






By definition we always have $P_{\sigma|G}(\sigma=\bar{\sigma})=P_{\sigma|G}(\sigma=f(\bar{\sigma}))$ in the Ising model. Next we present our new model, the Stochastic Ising Block Model (SIBM), which can be viewed as a natural composition of the SSBM and the Ising model. In SIBM, we first draw a pair $(X,G)$ under $\SSBM(n,k, p,q)$.  Then we draw $m$ independent samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ from the Ising model on the graph $G$, where $\sigma^{(u)}\in W^n$ for all $u\in[m]$.

\begin{definition}[Stochastic Ising Block Model]
Let $n$ and $m$ be positive integers such that $n$ is even. Let $p,q\in[0,1]$ be two real numbers and let $\alpha,\beta>0$. The triple $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ is drawn under $\SIBM(n,k, p,q,\alpha,\beta,m)$ if

\noindent
(i) the pair $(X,G)$ is drawn under $\SSBM(n,k, p,q)$;

\noindent
(ii) for every $i\in[m]$, each sample $\sigma^{(i)}=(\sigma_1^{(i)},\dots,\sigma_n^{(i)}) \in W^n$ is drawn independently according to the distribution \eqref{eq:isingma}.
\end{definition}

Notice that we only draw the graph $G$ once in SIBM, and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ are drawn independently from the Ising model on the {\em same} graph $G$.
Our objective is to recover the underlying partition (or the ground truth) $X$ from the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$, and we would like to use the smallest possible number of samples to guarantee the exact recovery of $X$ up to a global permutation.
Below we use the notation $P_{\SIBM}(A):=E_G[P_{\sigma|G}(A)]$ for an event $A$, where the expectation $E_G$ is taken with respect to the distribution given by SSBM. In other words, $P_{\sigma|G}$ is the conditional distribution of  $\sigma$ given a fixed $G$ while $P_{\SIBM}$ is the joint distribution of both $\sigma$ and $G$.
By definition, we have $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f$ is induced from $S_k$. We use the set $\Gamma = \{f_{\gamma}(X) | \gamma \in S_k\}$ to
represent all vectors with permutations on $X$.

\begin{definition}[Exact recovery in SIBM]
Let $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k, p,q,\alpha,\beta,m)$.
We say that exact recovery is solvable for $\SIBM(n,p,q,\alpha,\beta,m)$ if there is an algorithm that takes $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ as inputs and outputs $\hat{X}=\hat{X}(\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ such that
$$
P_{\SIBM}(\hat{X} \in \Gamma) \to 1
\text{~~~as~} n\to\infty ,
$$
and we call $P_{\SIBM}(\hat{X} \in \Gamma)$ the success probability of the recovery/decoding algorithm.
\end{definition}


As mentioned above, we consider the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. By definition, the ground truth $X$, the graph $G$ and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ form a Markov chain $X\to G\to \{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Therefor, if we cannot recover $X$ from $G$, then there is no hope to recover $X$ from $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Thus a necessary condition for the exact recovery in SIBM is $\sqrt{a}-\sqrt{b}> \sqrt{k}$, and we will limit ourselves to this case throughout the paper.

\vspace*{.1in}
\noindent{\bf Main Problem:} \emph{For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, what is the smallest sample size $m^\ast$ such that exact recovery is solvable for $\SIBM(n,a\log(n)/n, b\log(n)/n,\alpha,\beta,m^\ast)$?}

\vspace*{.1in}  It is this optimal sample size problem that we address---and resolve---in this paper. 
Our main results read as follows.

\begin{theorem} \label{thm:wt1}
For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, let
\begin{equation} \label{eq:defstar}
\beta^\ast := \frac{1}{k}
\log\frac{a+b-k-\sqrt{(a+b-k)^2-4ab}}{2 b} \text{~~and~~}
m^\ast := 2 \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1 .
\end{equation}
{\bf Case (i) when $\alpha>b\beta$}: If $m\ge m^\ast$, then exact recovery is solvable in $O(n)$ time for $\SIBM(n,a\log(n)/n, \linebreak[4] b\log(n)/n,\alpha,\beta,m)$.
If $\beta^\ast/\beta$ is not an integer and $m<m^\ast$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$. If $\beta^\ast/\beta$ is an integer and $m<m^\ast-2$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$.
{\bf Case (ii) when $\alpha<b\beta$}: Exact recovery of $\SIBM(n,a\log(n)/n, b\log(n)/n,\alpha,\beta,m)$ is not solvable for any $m=O(\log^{1/4}(n))$, and in particular, it is not solvable for any constant $m$ that does not grow with $n$.
\end{theorem}

Notation convention:
For $\cI \subseteq [n]$, we denote the event $\sigma = X^{(\sim \cI)}$ as $\sigma_i \neq X_i$ for all $i \in \cI$ and $\sigma_i = X_i$ for all $i \not\in \cI$.
When $\cI$ only contains one element, e.g., $\cI=\{i\}$, we write the event $\sigma = X^{(\sim i)}$ instead of $\sigma = X^{(\sim\{i\})}$.

For $\sigma,X\in W^n$, we define
$$
\dist(\sigma, X)
:=|\{i\in[n]:\sigma_i\neq X_i\}| 
\quad \text{and} \quad
\dist(\sigma,\Gamma)
:=\min\{\dist(\sigma, f(X)) | f(X) \in \Gamma\} .
$$
\begin{theorem}  \label{thm:wt3}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$. Let $m$ be a constant integer that does not grow with $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m).
$$
Define $g(\beta)  := \frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1$.
If $\beta>\beta^\ast$, then
$$
P_{\SIBM}(\sigma^{(i)} \in \Gamma \text{~for all~} i\in[m]) = 1-o(1).
$$
If $\beta\le \beta^\ast$, then
$$
P_{\SIBM}(\dist(\sigma^{(i)} \in \Gamma)= \Theta(n^{g(\beta)}) \text{~for all~} i\in[m]) = 1-o(1) .
$$
\end{theorem}

\section{Sketch of the proof}
\label{sect:sketch}

In this section, we illustrate the main ideas and explain the important steps in the proof of the main results. The complete proofs are given in Section~\ref{sect:aln}--\ref{sect:converse}.
As the first step, we prove that for $a>b>0$, if $\alpha>b\beta$, then all the samples are centered in $\Gamma$ with probability $1-o(1)$; if $\alpha<b\beta$, then all the samples are centered in $\Theta := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k\}$ where $\mathbf{1}_n$ is the all one vector with dimension $n$.

We use the concentration results for adjacency matrices of random graphs with independent edges to prove this. Let $A=A(G)$ be the adjacency matrix of the graph $G$. Then \eqref{eq:isingma} can be written as
\begin{align*}
 P_{\sigma|G}(\sigma=\bar{\sigma})  
=  \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j}\Big( \big(\beta+\frac{\alpha\log(n)}{n} \big) A
-\frac{\alpha\log(n)}{n} (J_n-I_n) \Big)_{ij} I(\bar{\sigma}_i,\bar{\sigma}_j) 
\Big)  ,
\end{align*}
where $J_n$ is the all one matrix and $I_n$ is the identity matrix, both of size $n\times n$.
Define a matrix
$
M:= \big(\beta+\frac{\alpha\log(n)}{n} \big) E[A|X]
-\frac{\alpha\log(n)}{n} (J_n-I_n).
$
Then we can further write \eqref{eq:isingma}
as
$$
P_{\sigma|G}(\sigma=\bar{\sigma})
= \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) + \frac{1}{2} \sum_{i,j}\big(\beta+\frac{\alpha\log(n)}{n} \big)  (A-E[A|X])_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) 
  \bar{\sigma}^T 
\Big)  .
$$
One can show that if $\alpha>b\beta$, then the maximizers of $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ consist of set $\Gamma$, and if $\alpha<b\beta$, then the maximizer set is $\Theta$.
Moreover, \cite{Hajek16} proved the concentration of $A$ around its expectation $E[A|X]$ in the sense that
$\|A-E[A|X]\| = O(\sqrt{\log(n)})$ with probability $1-o(1)$, we can further show that the error term above is bounded by
$$
\left|\frac{1}{2} \big(\beta+\frac{\alpha\log(n)}{n} \big)\sum_{i,j} (A-E[A|X])
 I( \bar{\sigma}_i, \bar{\sigma}_j) \right| = O \big( n \sqrt{\log(n)} \big)
  \text{~~for all~} \bar{\sigma}\in\{\pm 1\}^n  .
$$
This allows us to prove that in both cases (no matter $\alpha>b\beta$ or $\alpha<b\beta$), the Hamming distance between the samples and the maximizers of $\sum_{i,j}  M_{ij}I(\bar{\sigma}_i, \bar{\sigma}_j)$ is upper bounded by $kn/\log^{1/3}(n)=o(n)$.
More precisely, if $\alpha>b\beta$, then $\dist(\sigma^{(i)},\Gamma )< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$; if $\alpha<b\beta$, then $\dist(\sigma^{(i)}, \Theta)< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$; see Proposition~\ref{prop:1} for a rigorous proof.
In the latter case, each sample only take $\sum_{j=0}^{kn/\log^{1/3}(n)}\binom{n}{j}j^{k-1}$ values, so each sample contains at most $\log_k(\sum_{j=0}^{kn/\log^{1/3}(n)}\binom{n}{j}j^{k-1})=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$ bits of information about $X$. On the other hand, $X$ itself is uniformly distributed over a set of $\frac{n!}{(n/k)^k}$ vectors, so one needs at least $\log_k\frac{n!}{(n/k)^k}=\Theta(n)$ bits of information to recover $X$. Thus if $\alpha<b\beta$, then exact recovery of $X$ requires at least $\Omega(\frac{\log^{1/3}(n)}{\log\log(n)})\ge \Omega(\log^{1/4}(n))$ samples; see Proposition~\ref{prop:ab} for a rigorous proof.

For the rest of this section, we will focus on the case $\alpha>b\beta$ and establish the sharp threshold on the sample complexity. We first analyze the typical behavior of one sample and explain how to prove Theorem~\ref{thm:wt3}. Then we use the method developed for the one sample case to analyze the distribution of multiple samples, which allows us to prove Theorem~\ref{thm:wt2}. Note that Theorem~\ref{thm:wt1} follows directly from Theorem~\ref{thm:wt2}.

\subsection{Why is $\beta^\ast$ the threshold?} \label{sect:why}

Let us analyze the one sample case, i.e., we take $m=1$.
Theorem~\ref{thm:wt3} implies that $\beta^\ast$ is a sharp threshold for the event $\{\sigma \in \Gamma\}$, i.e., $P_{\SIBM}(\sigma\in \Gamma)=1-o(1)$ if $\beta$ is above this threshold and $P_{\SIBM}(\sigma\in \Gamma)=o(1)$ if $\beta$ is below this threshold.
We already know that
$
P_{\SIBM} \big(\dist(\sigma,\pm X)< kn/\log^{1/3}(n) \big) = 1- o(1) .
$
Therefore the following three statements are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $P_{\SIBM}(\sigma\in \Gamma)$ has a sharp transitions from $0$ to $1$ at $\beta^\ast$.
\item $P_{\SIBM} \big( 1\le \dist(\sigma,\Gamma)< kn/\log^{1/3}(n) \big)$ has a sharp transitions from $1$ to $0$ at $\beta^\ast$.
\item $\frac{P_{\SIBM} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\SIBM}(\sigma= X)}$ has a sharp transitions from $\infty$ (or $\omega(1)$) to $0$ at $\beta^\ast$.
\end{enumerate}
Statements (2) and (3) are equivalent because $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f$ is a permutation function.
We will show that the above three statements are further equivalent to
\begin{enumerate}[label=(\arabic*)]
\setcounter{enumi}{3}
  \item  $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}$ has a sharp transitions from $\infty$ (or $\omega(1)$) to $0$ at $\beta^\ast$.
\end{enumerate}
We first prove (4) and then show that it is equivalent to statement (3).
Instead of analyzing $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}$, we analyze $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}$ for a typical graph $G$.

Then,
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}
=\sum_{i=1}^n \frac{P_{\sigma|G} ( \sigma= X^{(\sim i)} )} {P_{\sigma|G}(\sigma= X)} .
$$
Given the ground truth $X$, a graph $G$ and a vertex $i\in[n]$, define
\begin{equation*}
A^r_i=A^r_i(G):=|\{j\in[n]\setminus\{i\}:\{i,j\}\in E(G), X_j=\omega^r \cdot X_i\} |, r=0, \dots, k-1
\end{equation*}
Then by \eqref{eq:isingma}, we have
\begin{align*}
\frac{P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)}
 = \sum_{r=1}^{k-1}\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i)
-\frac{2(k-1)\alpha\log(n)}{n} \Big) 
 = (1+o(1)) \sum_{r=1}^{k-1}\exp (k \beta(A^r_i-A^0_i))  ,
\end{align*}
where the second equality holds with high probability because $|A^r_i-A^0_i|=O(\log(n))$ with probability $1-o(1)$.
By definition,
$A^0_i\sim \Binom(\frac{n}{k}-1,\frac{a\log(n)}{n})$ and $A^r_i\sim \Binom(\frac{n}{k}, \frac{b\log(n)}{n})$ for $r=1,\dots, k-1$, and they are independent. Therefore,
\begin{align*}
E_G[\exp (k \beta (A^r_i-A^0_i))]
& =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^{k\beta} \Big)^{n/k}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-k\beta} \Big)^{n/k-1}  \\
& = 
\exp\Big(\frac{\log(n)}{k} ( a e^{-k\beta}+b e^{k\beta} -a-b )
+o(1) \Big) 
 = (1+o(1)) n^{g(\beta)-1} ,
\end{align*}
where $E_G$ means that the expectation is taken over the randomness of $G$, and the function
$g(\beta)  := \frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1$ is defined in Theorem~\ref{thm:wt3}.
As a consequence,
\begin{equation} \label{eq:mew}
E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} \Big]
= (1+o(1)) \sum_{i=1}^n\sum_{r=1}^{k-1} E_G[\exp (2 \beta (A^r_i-A^0_i))]
= (k-1+o(1)) n^{g(\beta)} .
\end{equation}
One can show that $g(\beta)$ is a convex function and takes minimum at $\beta=\frac{1}{2k}\log\frac{a}{b}$, so $g(\beta)$ is strictly decreasing in the interval $(0,\frac{1}{2k}\log\frac{a}{b})$. Furthermore, $\beta^\ast$ is a root of $g(\beta)=0$, and $0<\beta^\ast<\frac{1}{2k}\log\frac{a}{b}$, so $g(\beta)>0$ for $\beta<\beta^\ast$ and $g(\beta)<0$ for $\beta^\ast<\beta<\frac{1}{2k}\log\frac{a}{b}$.
Taking this into the above equation, we conclude that the expectation of $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}$ has a sharp transition from $\omega(1)$ to $o(1)$ at $\beta^\ast$.
This at least intuitively explains why $\beta^\ast$ is the threshold. However, in order to formally establish statement (4) above, we need to prove that this sharp transition happens for a typical graph $G$, not just for the expectation.
Moreover, $g(\beta)$ is an increasing function in the interval $\beta\in(\frac{1}{2k}\log\frac{a}{b}, +\infty)$, so the expectation first decreases in the interval $(0,\frac{1}{2k}\log\frac{a}{b}]$ and then starts increasing. We will prove that there is a ``cut-off" effect when $\beta>\frac{1}{2k}\log\frac{a}{b}$, i.e., although the expectation becomes much larger than $n^{g(\frac{1}{2k}\log\frac{a}{b})}$, for a typical graph $G$, we always have
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} 
= O( n^{g(\frac{1}{4}\log\frac{a}{b})} ) =o(1)
$$
whenever $\beta>\frac{1}{2k}\log\frac{a}{b}$.
Below we divide the proof into three cases: (i) $\beta\in(0,\beta^\ast]$, (ii) $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$, and (iii) $\beta\in(\frac{1}{2k}\log\frac{a}{b},+\infty)$.
Case (ii) is the simplest case, and its proof is essentially an application of Markov inequality, so we start with this case.

\subsection{Proof for $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$: An application of Markov inequality}
\label{sect:simreg}

We know that $g(\beta)<0$ for $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$. By \eqref{eq:mew} and Markov inequality, for almost all\footnote{By almost all $G$, we mean there is a set $\cG$ such that $P(G\in\cG)=1-o(1)$ and for every $G\in\cG$ certain property holds. The probability $P(G\in\cG)$ is calculated according to SSBM defined in Definition~\ref{def:SSBM}.} $G$, we have $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}=o(1)$. This proves that $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}=o(1)$ in this interval. With a bit more extra effort, let us also prove that $\frac{P_{\SIBM} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\SIBM}(\sigma= X)}=o(1)$.

For each $I$, we introduce a vector $v$ with $\mathrm{dim}(v) = |I|=r$. Each element of $v$ takes from $\{\omega, \omega^2, \omega^{k-1}\}$.
$X^{(\sim I, v)}$ is defined as flipping $i \in I$ as : $X_i \to v_{\mathrm{index}(i)}\cdot X_i$.
For example, if $I = {1,3}, n=3, v=(\omega^2, \omega)$ then $X^{(\sim I,v)} = (\omega^2 \cdot X_1, X_2, \omega \cdot X_3)$.
Then $P_{\sigma|G}(\sigma = X^{(\sim I)})=\sum_{v}P_{\sigma|G}(\sigma = X^{(\sim I,v)})$

By definition,
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = r )}{P_{\sigma|G}(\sigma= X)}
=\sum_{\cI\subseteq[n],|\cI|=r} \frac{P_{\sigma|G} ( \sigma= X^{(\sim \cI)} )} {P_{\sigma|G}(\sigma= X)} .
$$
Similarly to $A_i$ and $A^r_i$, for a set $\cI\subseteq[n]$, we define
define
$A_{\cI}=A_{\cI}(G):=|\{i,j\}\in E(G):  \{i, j\} \not\subseteq [n]\setminus\cI, X_j=X_i\}|$ and  
$B_{\cI,v}=B_{\cI,v}(G):=|\{\{i,j\}\in E(G): \{i, j\} \not\subseteq [n]\setminus\cI, X_j= v_{\mathrm{index}(i)} \cdot X_i\}|$.
Then by \eqref{eq:isingma} one can show that
$$
 \frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} 
\le \sum_{v}\exp\Big( k\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI,v}-A_{\cI})
\Big) = \sum_{v}\exp ( k (\beta + o(1)) (B_{\cI,v}-A_{\cI})
 ) .
$$
Since we are only interested in the case $|\cI|<kn/\log^{1/3}(n)=o(n)$,
by definition we have $A_{\cI}\sim\Binom((\frac{n}{k}-o(n))|\cI|,\frac{a\log(n)}{n})$ and $B_{\cI}\sim\Binom((\frac{n}{k}-o(n))|\cI|,\frac{b\log(n)}{n})$, and they are independent. Therefore,
\begin{align*}
E_G[\exp ( k (\beta +o(1)) (B_{\cI,v}-A_{\cI}) ) ] = &
\exp\Big(\frac{|\cI|\log(n)}{k} ( a e^{-k\beta}+b e^{k\beta} -a-b +o(1) )
 \Big) \\
 = & n^{|\cI|(g(\beta)-1+o(1))} .
\end{align*}
As a consequence,
\begin{equation} \label{eq:nn}
E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = r)}{P_{\sigma|G}(\sigma= X)} \Big]
\le \sum_{\cI\subseteq[n],|\cI|=r} 
n^{k (g(\beta)-1+o(1))}
= \binom{n}{r} (k-1)^r n^{r (g(\beta)-1+o(1))}
< (k-1)^rn^{r (g(\beta)+o(1))} .
\end{equation}
Then by Markov inequality, there is a set $\cG^{(r)}$ such that $P(G\in\cG^{(r)})=1-(k-1)^rn^{r g(\beta)/4}$ and for every $G\in\cG^{(r)}$, $\frac{P_{\sigma|G} ( \dist(\sigma, X) = k )}{P_{\sigma|G}(\sigma= X)} \le (k-1)^rn^{r g(\beta)/2}$.
Let $\cG=\cap_{r=1}^{kn/\log^{1/3}(n)} \cG^{(r)}$. By union bound, we have $P(G\in\cG)>1-\sum_{r=1}^\infty (k-1)^rn^{r g(\beta)/4} = 1-o(1)$. Moreover, for every $G\in\cG$,
$\frac{P_{\sigma|G} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\sigma|G}(\sigma= X)} < \sum_{r=1}^\infty (k-1)^rn^{r g(\beta)/2} = o(1)$.
This proves that $\frac{P_{\SIBM} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\SIBM}(\sigma= X)}=o(1)$, and so $P_{\SIBM}(\sigma \in \Gamma)=1-o(1)$ when $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$.


\subsection{Proof for $\beta\in(\frac{1}{2k}\log\frac{a}{b},+\infty)$: The ``cut-off" effect}

The analysis in this interval is more delicate. Since $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} 
= (1+o(1)) \sum_{i=1}^n \sum_{r=1}^{k-1}\exp (k\beta (A^r_i-A^0_i))$, we start with a more careful analysis of $\sum_{i=1}^n \exp (2 \beta (A^r_i-A^0_i))$.
Since $A^r_i-A^0_i$ takes integer value between $-n/k$ and $n/k$,
we can use indicator functions to write
\begin{align*}
 \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{n/k}
\mathbbm{1}[A^r_i-A^0_i=t \log(n)] \exp (k\beta t \log(n) ) ,
\end{align*}
where the quantity $t\log(n)$ ranges over all integer values from $-n/k$ to $n/k$ in the summation.
Define $D(G,t):=|\{i\in[n]:A^r_i-A^0_i= t\log(n)\}|=\sum_{i=1}^n \mathbbm{1}[A^r_i-A^0_i=t \log(n)]$.
Therefore,
\begin{equation}  \label{eq:gour}
 \sum_{i=1}^n \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{n/k}
D(G,t) \exp (k\beta t \log(n) ) .
\end{equation}
By Chernoff bound, we have $P(A^r_i-A^0_i\ge 0)\le \exp\big(\log(n)(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big)$.
Define $\cG_1:=\{G:A^r_i-A^0_i< 0~\forall i\in[n]\}$. Then by union bound, $P(G\notin\cG_1)\le\exp\big(\log(n)(1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big) = o(1)$, where the equality follows from the assumption that $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
For every $G\in\cG_1$, $D(G,t)=0$ for all $t\ge 0$, and so
\begin{equation} \label{eq:duj}
 \sum_{i=1}^n \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{-1}
D(G,t) \exp (k\beta t \log(n) ) .
\end{equation}
This indicates that there is a ``cut-off" effect at $t>0$, i.e., $D(G,t) \exp (2\beta t \log(n) )=0$ for all positive $t$ with probability $1-o(1)$, although its expectation can be very large, as we will show next.
Define a function
$$
f_{\beta}(t):=\frac{1}{k}\sqrt{k^2t^2+4ab} -t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big) -\frac{a+b}{k} +1 +k\beta t.
$$
Using Chernoff bound, one can show that 
$$
E[D(G,t)
\exp\big(2\beta t \log(n) \big)]
\le \exp( f_{\beta}(t) \log(n) ) .
$$
with probability one (Using a more careful analysis, one can show that this bound is tight up to a $\frac{1}{\sqrt{\log(n)}}$ factor; see Appendix~\ref{ap:um}.)
The function $f_{\beta}(t)$ is a concave function and takes maximum value at $t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k}$, and its maximum value is $f_{\beta}(t^\ast)=\frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1
=g(\beta)$.
Therefore, if we take expectation on both sides of \eqref{eq:gour}, then the sum on the right-hand side is concentrated on a small neighborhood of $t^\ast$. When $\beta>\frac{1}{2k}\log\frac{a}{b}$, we have $t^\ast>0$. Due to the ``cut-off" effect at $t>0$, we have $D(G,t)=0$ for all $t$ in the neighborhood of $t^\ast$ with probability $1-o(1)$, so the main contribution to the expectation comes from a rare event $G\notin\cG_1$. This explains why the behavior of a typical graph $G$ deviates from the behavior of the expectation.
Since $f_{\beta}(t)$ is a concave function, the sum $E[\sum_{t\log(n)=-n/k}^{-1}
D(G,t) \exp (k\beta t \log(n) )]$
is upper bounded by $O(\log(n))n^{f_{\beta}(0)}$  when $t^\ast>0$.
Notice that $f_{\beta}(0)=g(\frac{1}{2k}\log\frac{a}{b})=1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}<0$.
Now using \eqref{eq:duj} and Markov inequality, we conclude that $\sum_{i=1}^n \exp (2 \beta (A^r_i-A^0_i))=o(1)$ for almost all $G$,
and so $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} =o(1)$ for almost all $G$. Thus we have shown that $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}=o(1)$ when
$\beta>\frac{1}{2r}\log\frac{a}{b}$.
The analysis of $\frac{P_{\SIBM} ( \dist(\sigma, X) = r )}{P_{\SIBM}(\sigma= X)}$ for $1\le r<kn/\log^{1/3}(n)$ is
similar to the analysis in Section~\ref{sect:simreg}, and we do not repeat it here. By now we have given a sketched proof of
$P_{\SIBM}(\sigma \in \Gamma)=1-o(1)$ when $\beta>\beta^\ast$; see Section~\ref{sect:equal} for a rigorous proof. Next we move to the case $\beta\le\beta^\ast$.

\subsection{Proof for $\beta\le\beta^\ast$: Structural results and tight concentration}

In the last inequality of \eqref{eq:nn}, we use a coarse bound $\binom{n}{r}<n^r$. Now let us use a tighter bound $\binom{n}{r}<n^r/(r!)$. 
For $r>n^{g(\beta)+\delta}$, we have
$
r!>(r/e)^r
=\exp(r\log(r)-r)
>\exp(r(g(\beta)+\delta)\log(n)-r)
=n^{r(g(\beta)+\delta-o(1))} .
$
Taking these into the last inequality of \eqref{eq:nn}, we obtain that for all $r>n^{g(\beta)+\delta}$,
$$
E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = r )}{P_{\sigma|G}(\sigma= X)} \Big]
\le  (k-1)^r\binom{n}{k} n^{k (g(\beta)-1+o(1))}
< n^{r (g(\beta)+o(1))} /(r!) 
< (k-1)^rn^{-r(\delta-o(1))} .
$$
This immediately implies that  $P_{\SIBM} (\dist(\sigma, \Gamma)<n^{g(\beta)+\delta} ) = 1- o(1)$ for any $\delta>0$. Since $g(\beta)<1$ for all $0<\beta\le\beta^\ast$, we have $P_{\SIBM} (\dist(\sigma, \Gamma)<n^{\theta} ) = 1- o(1)$ for all $\theta\in (g(\beta), 1)$. This improves upon the upper bound 
$\dist(\sigma, \Gamma)< kn/\log^{1/3}(n)$ we obtained using spectral method at the beginning of this section.

More importantly, this allows us to prove a powerful structural result. (All the discussions below are conditioning on the event $\dist(\sigma,X)\le n/2$, i.e., $\sigma$ is closer to $X$ than to $-X$.) We say that $j$ is a ``bad" neighbor of vertex $i$ if the edge $\{i,j\}$ is connected in graph $G$ and $\sigma_j\neq X_j$. Then there is an integer $z>0$ such that with probability $1-o(1)$, every vertex has at most $z$ ``bad" neighbors.
Conditioning on the event every vertex has at most $z$ ``bad" neighbors, it is easy to show that $P_{\sigma|G}(\sigma_i=- X_i)$ differs from $\exp (2 \beta (B_i-A_i))$ by at most a constant factor $\exp(4\beta z)$.
Therefore, $E_{\sigma|G}[\dist(\sigma,X)]=\sum_{i=1}^n P_{\sigma|G}(\sigma_i=- X_i)$ differs from $\sum_{i=1}^n\exp (2 \beta (B_i-A_i))$ by at most a constant factor for almost all $G$.
We can further prove that the pairwise correlation of the events $\{\sigma_i=- X_i\}$ and $\{\sigma_j=- X_j\}$ is very small, so $\dist(\sigma,X)$ concentrates around its expectation. Thus we conclude that $\dist(\sigma,X)$ differs from $\sum_{i=1}^n\exp (2 \beta (B_i-A_i))$ by at most a constant factor for almost all $G$.
In Section~\ref{sect:why} (see \eqref{eq:mew}), we have shown that $E_G[\sum_{i=1}^n\exp (2 \beta (B_i-A_i))]=(1+o(1))n^{g(\beta)}$. Quite surprisingly, when $\beta\le\beta^\ast$, we can prove a very tight concentration around the expectation: For almost all graph $G$, we have $\sum_{i=1}^n\exp (2 \beta (B_i-A_i))=(1+o(1))n^{g(\beta)}$; see Proposition~\ref{prop:con} in Section~\ref{sect:struct} for a proof. Combining this with the above analysis, we conclude that $\dist(\sigma,X)=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ when $\beta\le\beta^\ast$. This completes the sketched proof of Theorem~\ref{thm:wt3}.
See Sections~\ref{sect:theta} and Sections~\ref{sect:struct} for the rigorous proof of the above arguments.

\section{Samples are concentrated around $\Gamma$ or $\Theta$} \label{sect:aln}
In this section, we show that for $a>b$, if $\alpha>b\beta$, then all the samples produced by $\SIBM(n, k,\linebreak[4]
a\log(n)/n, b\log(n)/n,\alpha,\beta,m)$ are very close to one element from $\Gamma$. More precisely, they differ from the ground truth $\Theta$ in at most $kn/\log^{1/3}(n)$ coordinates.
On the other hand, if $\alpha<b\beta$, then the samples differ from  $\pm \Theta$ in at most $kn/\log^{1/3}(n)$ coordinates.
For the latter case, we prove that the number of samples needed for exact recovery of $X$ is at least $\Omega(\log^{1/4}(n))$.

Let $A=A(G)$ be the adjacency matrix of $G$. Then \eqref{eq:isingma} can be written as
\begin{align*}
& P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{\beta}{2} \sum_{i,j} A_{ij} I(\bar{\sigma}_i,\bar{\sigma}_j)
-\frac{\alpha\log(n)}{2n} \sum_{i,j} (J_n-I_n-A)_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)
\Big)  \\
= & \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j} \Big( \big(\beta+\frac{\alpha\log(n)}{n} \big) A
-\frac{\alpha\log(n)}{n} (J_n-I_n) \Big)_{ij} I(\bar{\sigma}_i,\bar{\sigma}_j)
\Big),
\end{align*}
where $J_n$ is the all one matrix and $I_n$ is the identity matrix, both of size $n\times n$.
Conditioned on the ground truth $X$,
$A-E[A|X]$ is a symmetric matrix whose upper triangular part consists of independent entries. According to Theorem~5 in \cite{Hajek16}, the spectral norm of $A-E[A|X]$ is upper bounded by $O(\sqrt{\log(n)})$.
\begin{theorem}[Theorem~5 in \cite{Hajek16}] \label{thm:a2}
For any $r>0$, there exists $c>0$ such that the spectral norm of $A-E[A|X]$ satisfies
$$
P\big(\|A-E[A|X]\| \le c\sqrt{\log(n)} \big)\ge 1-n^{-r} .
$$
\end{theorem}
Define a matrix
$$
M:= \big(\beta+\frac{\alpha\log(n)}{n} \big) E[A|X]
-\frac{\alpha\log(n)}{n} (J_n-I_n).
$$
Then we can further write \eqref{eq:isingma}
as
\begin{equation} \label{eq:M}
P_{\sigma|G}(\sigma=\bar{\sigma})
= \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j}M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) + \frac{1}{2}\big(\beta+\frac{\alpha\log(n)}{n} \big)  \sum_{i,j}  (A-E[A|X])_{ij}
 I(\bar{\sigma}_i, \bar{\sigma}_j)  
\Big)  .
\end{equation}


By definition, all the diagonal entries of $M$ are $0$.
For $i\neq j$, 
$$
M_{ij}=\left\{ 
\begin{array}{cc}
 \big(a\beta-\alpha+\frac{a\alpha\log(n)}{n} \big) \frac{\log(n)}{n} = a' + O(\frac{\log^2 n}{n^2})
   & \text{~if~} X_i=X_j \\
  \big(b\beta-\alpha+\frac{b\alpha\log(n)}{n} \big) \frac{\log(n)}{n} = b' + O(\frac{\log^2 n}{n^2})
   & \text{~if~} X_i\neq X_j
\end{array}
\right. .
$$
where $a' =(a\beta - \alpha) \frac{\log n }{n}, b'=(b\beta - \alpha) \frac{\log n}{n}$.
Given $\bar{\sigma}\in\{1, \omega, \dots, \omega^{k-1}\}^n$, define a $k\times k$ matrix $\Xi$ with 
$\Xi_{ij} = |\{k \in [n]: X_k = \omega^{i-1}, \sigma_k = \omega^{j-1}\}|$. It can be seen that each row of $\Xi$ sums to $\frac{n}{k}$. Define $Q_{ij} = \sum_{r,s=1, r<s}^k (\Xi_{ir} - \Xi_{is})(\Xi_{jr} - \Xi_{js})$
Therefore,
\begin{equation} \label{eq:sMs}
\begin{aligned}
 \sum_{i,j}M_{ij}I(\bar{\sigma}_i, \bar{\sigma}_j)
= & a'\sum_{i=1}^k \big( (k-1)(\sum_{r=1}^k \Xi_{ir}^2) - \sum_{r,s=1,r \neq s}^k \Xi_{ir}\Xi_{is} \big) \\
+ & b'\sum_{i,j=1, i\neq j}^k  \big( (k-1)  (\Xi_{ir} \Xi_{jr}) - \sum_{r,s=1,r \neq s}^k \Xi_{ir}\Xi_{js}  \big) + O(\log^2 n)\\
= & a' \sum_{i=1}^k Q_{ii} + b' \sum_{i,j=1,i\neq j}^k Q_{ij} + O(\log^2 n ) \\
 = & ( a' - b') \sum_{i=1}^k Q_{ii} + b' \sum_{i,j=1}^k Q_{ij} + O(\log^2 n ) 
\end{aligned}
\end{equation}
Further we have:
$$
\sum_{i,j=1}^k Q_{ij} = \sum_{r,s=1, r<s}^k (\sum_{i=1}^k \Xi_{ir} - \sum_{i=1}^k \Xi_{is})^2
$$
Therefore, both the coefficients of $(a'-b')$ and $b'$ are non-negative.
According to \eqref{eq:M}, the configuration $\bar{\sigma}$ that maximizes $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ is (roughly) the most likely output of the Ising model.
Since we assume $a>b$, $a'-b' = (a-b) \beta > 0$ and its coefficient term $\sum_{i=1}^k Q_{ii}$ takes maximum when there is only one non-zero term $\frac{n}{k}$ on each row of the matrix $\Xi$. Below we discuss the sign of $b'$.

If $b\beta<\alpha$, then $b' < 0$ and we should let $\sum_{i,j=1}^k Q_{ij} = 0$ to make the whole expression larger. This is equivalent to say the summation of each column of the matrix $\Xi$ are all the same. Combining the two conditions together we can see the maximum value is
the representation of $S_k$ in $k\times k$ matrix form multiplied by a coefficient $\frac{n}{k}$. And it is further equivalent to the statement $\sigma \in \Gamma$.

If $b\beta>\alpha$, then $b' > 0$ and we should take the maximum value of $\sum_{i,j=1}^k Q_{ij}$ to make the whole expression larger. We can show that in this case, the maximizer is taken when one column of $\Xi$ is $\frac{n}{k}  \mathbf{1}_k$ and other columns are all zero. And it is further equivalent to the statement $\sigma \in \Theta$.

To summarize, if $b\beta<\alpha$, then $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ takes maximum at $\sigma \in \Gamma$.
If $b\beta>\alpha$, then the maximum is at$\sigma \in \Theta$.
Taking into account the effect of the error term $ \sum_{i,j}  (A-E[A|X])_{ij}
 I(\bar{\sigma}_i, \bar{\sigma}_j)  $ in \eqref{eq:M}, we have the following proposition:

\begin{proposition} \label{prop:1}
Let $a>b>0$ and $\alpha,\beta>0$ be constants. Let $m$ be a positive integer that is upper bounded by some polynomial of $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n, \alpha,\beta, m) .
$$
If $b\beta <\alpha$, then for any (arbitrarily large) $r>0$, there exists $n_0(r)$ such that for all even integers $n>n_0(\delta, r)$,
$$
P_{\SIBM} \Big(\dist(\sigma^{(i)}, \Gamma)< kn/\log^{1/3}(n)
\text{~~for all~} i\in[m] \Big) \ge 1- n^{-r} .
$$
If $b\beta >\alpha$, then for any (arbitrarily large) $r>0$, there exists $n_0(r)$ such that for all even integers $n>n_0(\delta, r)$,
$$
P_{\SIBM} \Big(\dist(\sigma^{(i)},\pm \Theta)< kn/\log^{1/3}(n)
\text{~~for all~} i\in[m] \Big) \ge 1- n^{-r} .
$$
\end{proposition}
\begin{proof}
We only prove the case of $b\beta <\alpha$ as the proof of the other case is virtually identical.
Define
$$
\gamma^{r,s}_i = \begin{cases}
1 & \sigma_i = \omega^r \\
-1 & \sigma_i = \omega^s \\
0 & \textrm{ otherwise}
\end{cases}
$$
Then we can show that
$$
\sum_{i,j} I(\sigma_i, \sigma_j) (A-\mathbb{E}[A|X]) = \sum_{r,s} (\gamma^{r,s})^T (A-\mathbb{E}[A|X])\gamma^{r,s}
$$
Since $A-E[A|X]$ is a symmetric matrix,
$$
|\bar{\gamma}  (A-E[A|X]) \bar{\gamma}^T|\le
\|A-E[A|X]\| \bar{\gamma} \bar{\gamma}^T=n \|A-E[A|X]\|
$$
for every $\bar{\gamma}\in\{\pm 1\}^n$.
Therefore, by Theorem~\ref{thm:a2}, for any $r>0$, there is $c>0$ such that
\begin{equation} \label{eq:cnA}
\left|\frac{1}{2} \big(\beta+\frac{\alpha\log(n)}{n} \big)\sum_{i,j} I(\sigma_i, \sigma_j) (A-\mathbb{E}[A|X]) 
\right| \le c\binom{k}{2} n \sqrt{\log(n)}
  \text{~~for all~} \bar{\sigma}\in W^n
\end{equation}
with probability at least $1-n^{-2r}$.

Define a small neighborhood of $\Xi^*$ as
$$
N(\Xi^*) := \{\Xi |\, |\Xi_{i,j} - \Xi_{i,j}| \leq \frac{n}{\log^{1/3} n}, 1\leq i,j\leq 3\}
$$
We use the $L_{\infty}$ norm for the matrix.
 When there is only one non-zero term $\frac{n}{k}$ on each row of the matrix $\Xi$, we call this matrix a ''Corner Matrix''.
 We use $\Gamma'$ to denote all corner matrices, then $|\Gamma'|=k^k$ and $\Gamma \subset \Gamma'$. The neighborhood of all corner matrice is denoted by $N(\Gamma')$. Similary we can define $N(\Gamma)$. 

$N(\Gamma)$ consists of $\bar{\sigma}$'s that differ from $\Gamma$ in at most $kn/\log^{1/3}(n)$ coordinates.
Given a graph $G$ whose adjacency matrix satisfies \eqref{eq:cnA}, we will show that $P_{\sigma|G}(\sigma\notin N(\Gamma))< e^{-n}$. Since $P_{\sigma|G}(\sigma=X)<1$, it suffices to prove that
$\frac{P_{\sigma|G}(\sigma\notin N(\Gamma))}{P_{\sigma|G}(\sigma=X)}<e^{-n}$. 

Since  $N(\Gamma)^c=N(\Gamma')^c\cup N(\Gamma' \backslash \Gamma)$.
Next we prove that for every $G$ satisfying \eqref{eq:cnA} and every
$\bar{\sigma}\in N(\Gamma')^c\cup N(\Gamma' \backslash \Gamma)$, 
\begin{equation} \label{eq:xmb}
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}{P_{\sigma|G}(\sigma=X)}<k^{-n}e^{-n} .
\end{equation}
Together with the trivial upper bound $|N(\Gamma')^c\cup N(\Gamma' \backslash \Gamma)|<k^n$, this implies that $\frac{P_{\sigma|G}(\sigma\notin N(\Gamma))}{P_{\sigma|G}(\sigma=X)}<e^{-n}$.

We first prove \eqref{eq:xmb} for $\bar{\sigma}\in N(\Gamma')^c$.
Observe that $\sigma=X$ corresponds to $\Xi=\frac{n}{k} I_k$. By \eqref{eq:sMs}, we have
\begin{equation} \label{eq:X}
\begin{aligned}
P_{\sigma|G}(\sigma=X) & =\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{1}{2} X M X^T + O(n\sqrt{\log(n)}) \Big)  \\
& =\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{(a-b)\beta(k-1)}{2k}n\log(n) + O(n\sqrt{\log(n)}) \Big) .
\end{aligned}
\end{equation}
On the other hand, if $\bar{\sigma}\in N(\Gamma')^c$, using the definition of $\Xi$ we can find an 1-1 correspondence
between $\bar{\sigma}$ and $\bar{\Xi}$. For $\bar{\Xi}$ we can find one place where 
$\frac{n}{\log^{1/3} n} < |\bar{\Xi}_{ij}| < \frac{n}{k}- \frac{n}{\log^{1/3} n}$. Then
$$
Q_{ii} < (\frac{n}{k}-\frac{2n}{\log^{1/3} n})^2 + (k-2)\Big( (\frac{n}{k} - \frac{n}{\log^{1/3} n})^2 + (\frac{n}{\log^{1/3} n})^2 \Big)
$$

$$
\sum_{r=1}^k Q_{rr} < \frac{k-1}{2k}n^2 - \frac{2n^2}{\log^{1/3} n}
+O(n^2/\log^{2/3}(n)).
$$
Since $b' = b\beta-\alpha<0$ and $\sum_{i,j=1}^k Q_{ij} \geq 0$, by \eqref{eq:sMs} we have
$$
\frac{1}{2}\bar{\sigma} M \bar{\sigma}^T  
\le \frac{a\beta-b\beta}{4}n\log(n)
-(a\beta-b\beta)n\log^{2/3}(n)
+O(n\log^{1/3}(n)) ,
$$
and so
\begin{align*}
P_{\sigma|G}(\sigma=\bar{\sigma}) \le \frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{a\beta-b\beta}{4}n\log(n) 
-(a\beta-b\beta)n\log^{2/3}(n)
+ O(n\sqrt{\log(n)}) \Big) .
\end{align*}
Combining this with \eqref{eq:X}, we have
$$
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}{P_{\sigma|G}(\sigma=X)}
\le \exp\Big(-(a\beta-b\beta)n\log^{2/3}(n)
+ O(n\sqrt{\log(n)}) \Big)
<k^{-n} e^{-n}
$$
for all $\bar{\sigma}\in N(\Gamma')^c$ and all $G$ satisfying \eqref{eq:cnA}. 

For $\bar{\sigma}\in N(\Gamma' \backslash \Gamma)$, we can find $r,s$ such that
$\sum_{i=1}^k \Xi_{ir} \geq \frac{2n}{k} - \frac{2n}{\log^{1/3} n }$ and 
$\sum_{i=1}^k \Xi_{is} \leq \frac{2n}{\log^{1/3} n }$, Therefore
$$
\sum_{i,j=1}^k Q_{ij} \geq (\sum_{i=1}^k \Xi_{ir} - \sum_{i=1}^k \Xi_{is})^2 \geq \frac{n^2}{k^2}
$$
for large $n$. Since $b' = b\beta-\alpha<0$, this implies that
$$
\frac{1}{2}\sum_{i,j=1}^k Q_{ij}
\frac{(b\beta-\alpha)\log(n)}{n}
< - \frac{\alpha-b\beta}{2k^2} n\log(n) ,
$$
so
$$
\frac{1}{2}\bar{\sigma} M \bar{\sigma}^T  
\le \frac{(a-b)\beta(k-1)}{2k}n\log(n)
- \frac{\alpha-b\beta}{2k^2} n\log(n) .
$$
Therefore,
\begin{align*}
P_{\sigma|G}(\sigma=\bar{\sigma}) \le \frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\frac{(a-b)\beta(k-1)}{2k}n\log(n) 
-\frac{\alpha-b\beta}{2k^2} n\log(n)
+ O(n\sqrt{\log(n)}) \Big) .
\end{align*}
Combining this with \eqref{eq:X}, we have
$$
\frac{P_{\sigma|G}(\sigma=\bar{\sigma})}{P_{\sigma|G}(\sigma=X)}
\le \exp\Big(-\frac{\alpha-b\beta}{2k^2} n\log(n)
+ O(n\sqrt{\log(n)}) \Big)
<k^{-n} e^{-n}
$$
for all $\bar{\sigma}\in N(\Gamma' \backslash \Gamma)$ and all $G$ satisfying \eqref{eq:cnA}.

Now we have shown that for a single sample $\sigma$ produced by the SIBM, $P_{\sigma|G}(\sigma\in\Gamma)\ge 1- e^{-n}$ provided that $G$ satisfies \eqref{eq:cnA}. By the union bound, for $m$ independent samples $\sigma^{(1)},\dots,\sigma^{(m)}$ produced by the SIBM, $P_{\sigma|G}(\sigma^{(1)},\dots,\sigma^{(m)}\in\Gamma)\ge 1- m e^{-n}$ provided that $G$ satisfies \eqref{eq:cnA}.
We also know that $G$ satisfies \eqref{eq:cnA} with probability at least $1-n^{-2r}$, and by assumption $m$ is upper bounded by some polynomial of $n$. Therefore, the overall probability of $\sigma^{(1)},\dots,\sigma^{(m)}\in\Gamma$ is at least $1-n^{-r}$ when $n$ is large enough. This completes the proof of the proposition.
\end{proof}
\begin{proposition}  \label{prop:ab}
Let $a>b>0$ and $\alpha,\beta>0$ be constants. Let $m$ be a positive integer that is upper bounded by some polynomial of $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
$$
If $\alpha<b\beta$, then it is not possible to recover $X$ from the samples when $m=O(\log^{1/4}(n))$.
\end{proposition}

\begin{proof}
First observe that there are $\frac{n!}{(n/k)^k}$ balanced partitions, so one needs at least $\log_k \frac{n!}{(n/k)^k}=\Theta(n)$ bits to recover $X$.
By Proposition~\ref{prop:1}, with probability $1-o(n^{-4})$, $\dist(\sigma^{(i)}, \Theta)< kn/\log^{1/3}(n)$
for all $i\in[m]$. Therefore, each $\sigma^{(i)}$ takes at most
$$
T:=\sum_{j=0}^{kn/\log^{1/3}(n)} \binom{n}{j}(k-1)^j
$$
values, so each $\sigma^{(i)}$ contains at most $\log_k T$ bits of information. Next we prove that $\log_k T=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$, so we need at least $\Omega(\frac{\log^{1/3}(n)}{\log\log(n)})$ samples to recover $X$, which proves the proposition.

In order to upper bound $T$, we define a binomial random variable $Y\sim\Binom(n,\frac{k-1}{k})$. Then
$$
T=k^n P(Y\le kn/\log^{1/3}(n))
= k^n P(Y\ge n- kn/\log^{1/3}(n)).
$$
The moment generating function of $Y$ is $(\frac{1}{k}+\frac{k-1}{k}e^s)^n$. By Chernoff bound, for any $s>0$,
$$
P(Y\ge n- kn/\log^{1/3}(n)) \le
(\frac{1}{k}+\frac{k-1}{k}e^s)^n e^{-sn}
e^{ksn/\log^{1/3}(n)}
= k^{-n} (k-1+e^{-s})^n e^{ksn/\log^{1/3}(n)} .
$$
As a consequence, for any $s>0$,
$$
\log_k T\le n\Big(\log_k(k-1+e^{-s})
+\frac{ks}{\log^{1/3}(n)}\log_k e \Big) .
$$
Taking $s=\log\log(n)$ into this bound, we obtain that $\log_2 T=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$.
\end{proof}

\section{$\sigma \in \Gamma$ with probability $1-o(1)$ when $\beta>\beta^\ast$} \label{sect:equal}

Recall the definition of $\beta^\ast$ in \eqref{eq:defstar}.
\begin{proposition} \label{prop:tt}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\beta>\beta^\ast$ and $\alpha>b\beta$. 
Let 
$
(X,G,\sigma) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, 1) .
$
Then
$$
P_{\SIBM}(\sigma \in \Gamma)=1-o(1) .
$$
\end{proposition}

We have proved in Proposition~\ref{prop:1} that if $\alpha>b\beta$, then $\dist(\sigma \in \Gamma) \le kn/\log^{1/3}(n)$
 with probability $1-o(1)$.
Then Proposition~\ref{prop:1} tells us that
$$
\sum_{\cI\subseteq[n],~
kn/\log^{1/3}(n)<|\cI|<n-kn/\log^{1/3}(n)} P_{\SIBM}(\sigma=X^{(\sim\cI)})  = o(1) .
$$
By definition, $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$. Therefore, 
$$
\sum_{\cI\subseteq[n],1\le |\cI|\le kn/\log^{1/3}(n)} P_{\SIBM}(\sigma=f(X^{(\sim\cI)})) =\sum_{\cI\subseteq[n],1\le |\cI|\le kn/\log^{1/3}(n)} P_{\SIBM}(\sigma=X^{(\sim\cI)}).
$$
As a consequence, to prove Proposition~\ref{prop:tt}, we only need to show that
$$
\sum_{\cI\subseteq[n],1\le |\cI|\le kn/\log^{1/3}(n)} P_{\SIBM}(\sigma=X^{(\sim\cI)}) 
= o(1) .
$$
This is further equivalent to proving that there exists a set $\cG$ such that

\noindent (i)
$P(G\in\cG)=1-o(1)$, where the probability is calculated according to the $\SSBM(n, k,a\log(n)/n, \linebreak[4] b\log(n)/n)$.

\noindent (ii)
For every $G\in\cG$,
$$
\sum_{\cI\subseteq[n],1\le |\cI|\le kn/\log^{1/3}(n)}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} =o(1) .
$$
In order to prove the existence of such a set $\cG$, we define two functions
\begin{equation}  \label{eq:gbt}
\begin{aligned}
g(\beta) & := \frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1 , \\
\tilde{g}(\beta) & :=\left\{
\begin{array}{cc}
  g(\beta)   & \text{~if~} \beta< \frac{1}{2k}\log\frac{a}{b} \\
  g(\frac{1}{2k}\log\frac{a}{b})=1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}  & \text{~if~} \beta\ge \frac{1}{2k}\log\frac{a}{b}
\end{array}
\right. ,
\end{aligned}
\end{equation}
and we will prove in Lemma~\ref{lm:ele} below (see the end of Section~\ref{sect:k=1}) that $\tilde{g}(\beta)<0$ under the conditions of Proposition~\ref{prop:tt} (i.e., $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\beta>\beta^\ast$).
The existence of $\cG$ is guaranteed by the following proposition:
\begin{proposition} \label{prop:big}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\beta>\beta^\ast$ and $\alpha>b\beta$. 
Let 
$
(X,G,\sigma) \sim \SIBM(n,k,a\log(n)/n, b\log(n)/n,\alpha,\beta, 1) .
$
There is an integer $n_0$ such that for every even integer $n>n_0$ and  every integer $1\le r\le kn/\log^{1/3}(n)$, there is a set $\cG^{(r)}$ for which

\noindent (i)
$P(G\in\cG^{(r)}) \ge 1- 2(k-1)^r n^{r\tilde{g}(\beta)/8}$ ,

\noindent (ii) For every $G\in\cG^{(r)}$,
$$
\sum_{\cI\subseteq[n],|\cI|=r}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} <
n^{r \tilde{g}(\beta) /2} .
$$
\end{proposition}
With the $\cG^{(r)}$'s given by Proposition~\ref{prop:big}, we
define 
$$
\cG:=\bigcap_{r=1}^{kn/\log^{1/3}(n)} \cG^{(r)} .
$$
By the union bound,
$$
P(G\in\cG)\ge 1-2 \sum_{r=1}^{kn/\log^{1/3}(n)} (k-1)^rn^{r\tilde{g}(\beta)/8}
> 1- \frac{2 (k-1)n^{\tilde{g}(\beta)/8}}{1-(k-1)n^{\tilde{g}(\beta)/8}}
=1-o(1),
$$
where the last equality follows from $\tilde{g}(\beta)<0$. Moreover, for every $G\in\cG$,
\begin{align*}
& \sum_{\cI\subseteq[n],1\le |\cI|\le kn/\log^{1/3}(n)}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} =
\sum_{r=1}^{kn/\log^{1/3}(n)}
\hspace*{0.05in}
\sum_{\cI\subseteq[n],|\cI|=r}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)}  \\
& < \sum_{r=1}^{kn/\log^{1/3}(n)}
n^{r \tilde{g}(\beta) /2}
< \frac{n^{\tilde{g}(\beta)/2}}{1-n^{\tilde{g}(\beta)/2}} =o(1) .
\end{align*}
Thus we have shown that Proposition~\ref{prop:tt} is implied by Proposition~\ref{prop:big}. In the rest of this section, we will prove the latter proposition. In Section~\ref{sect:k=1}, we will prove Proposition~\ref{prop:big} for the special case of $r=1$ to illustrate the basic idea of the proof. Then we prove Proposition~\ref{prop:big} for general $k$ in Section~\ref{sect:gen}.


\subsection{Proof of Proposition~\ref{prop:big} for $r=1$} \label{sect:k=1}

Given the ground truth $X$, a graph $G$ and a vertex $i\in[n]$, define
\begin{equation} \label{eq:defAB}
A^r_i=A^r_i(G):=\{j\in[n]\setminus\{i\}:\{i,j\}\in E(G), X_j= \omega^r \cdot X_i\} 
\end{equation}
Next we give an upper bound on $P(A^1_i-A^0_i\ge t\log(n))$ for $t\in [\frac{1}{k}(b-a), 0]$. We take the left boundary to be $\frac{1}{k}(b-a)$ because $\frac{E[A^1_i-A^0_i]}{\log(n)}\to\frac{1}{k}(b-a)$ as $n\to\infty$.
\begin{proposition}  \label{prop:cher}
For $t\in [\frac{1}{k}(b-a), 0]$,
\begin{equation} \label{eq:upba}
\begin{aligned}
& P(A^1_i-A^0_i\ge t\log(n))  \\
\le &  \exp\Big(\frac{\log(n)}{k}
\Big(\sqrt{k^2t^2+4ab} -kt\log\frac{\sqrt{k^2t^2+4ab}+kt}{2b} -a-b + O\big(\frac{\log(n)}{n}\big) \Big)\Big) .
\end{aligned}
\end{equation}
\end{proposition}

\begin{proof}
When $t=\frac{1}{k}(b-a)$, the right-hand side of \eqref{eq:upba} is equal to $1$, so we only need to prove \eqref{eq:upba} for $t\in (\frac{1}{k}(b-a), 0]$.
By definition,
$A^0_i\sim \Binom(\frac{n}{k}-1,\frac{a\log(n)}{n})$ and $A^1_i\sim \Binom(\frac{n}{k}, \frac{b\log(n)}{n})$, and they are independent.
The moment generating function of $A^1_i-A^0_i$ is
\begin{align*}
E[e^{s(A^1_i-A^0_i)}]
& =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^s \Big)^{n/k}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-s} \Big)^{n/k-1}  \\
& = 
\exp\Big(\frac{\log(n)}{k} \Big( b e^s-b +O\big(\frac{\log(n)}{n}\big) \Big)\Big)
\exp\Big(\frac{\log(n)}{k} \Big( a e^{-s}-a + O\big(\frac{\log(n)}{n}\big) \Big) \Big)
 \\
& = 
\exp\Big(\frac{\log(n)}{k} \Big( a e^{-s}+b e^s-a-b + O\big(\frac{\log(n)}{n}\big) \Big)\Big) ,
\end{align*}
where we use the Taylor expansion $\log(1+x)=x+O(x^2)$ to obtain the second equality.
By Chernoff bound, for any $s>0$, we have
\begin{equation} \label{eq:mmd}
\begin{aligned}
& P(A^1_i-A^0_i\ge t\log(n))\le
\frac{E[e^{s(A^1_i-A^0_i)}]}{e^{st\log(n)}}  \\
 \le & \exp\Big(\frac{\log(n)}{k} \Big( a e^{-s}+b e^s -kst -a-b + O\big(\frac{\log(n)}{n}\big) \Big)\Big)  .
 \end{aligned}
\end{equation}
Let $h(s):=a e^{-s}+be^s-kst$. We want to find $\min_{s>0} h(s)$ to plug into the above upper bound. Since 
$h'(s)=-ae^{-s}+be^s-kt$ and 
$f''(s)=ae^{-s}+be^s>0$, $f(s)$ is a convex function and takes global minimum at $s^\ast$ such that $h'(s^\ast)=0$. Next we show that $s^\ast>0$ for all $t>\frac{1}{k}(b-a)$, so $\min_{s>0}h(s)=h(s^\ast)$. Indeed, this follows directly from the facts that $h'(0)=b-a-kt<0=h'(s^\ast)$ and that $h'(s)$ is an increasing function. Taking $s^\ast=\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b)$ into \eqref{eq:mmd}, we obtain \eqref{eq:upba} for all $t\in(\frac{1}{2}(b-a), 0]$ and large enough $n$.
\end{proof}

Note that $A^r_i$ are functions of the underlying graph $G$.
Given a graph $G$, define 
$$
\tilde{D}_r(G):=|\{i\in[n]:A^r_i- A^0_i\ge 0\}|
\text{~~and~~}
\tilde{D}_{ri}(G):=\mathbbm{1}[A^r_i- A^0_i\ge 0] ,
$$
where $\mathbbm{1}[\cdot]$ is the indicator function. Then
$\tilde{D}_r(G)=\sum_{i=1}^n \tilde{D}_{ri}(G)$ and
$$
E[\tilde{D}_r(G)]=\sum_{i=1}^n E[\tilde{D}_{ri}(G)]
=\sum_{i=1}^n P( A^r_i- A^0_i\ge 0).
$$
Taking $t=0$ into \eqref{eq:upba}, we have
$P(A^r_i- A^0_i\ge 0)\le \exp\big(\log(n)(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big)$. Therefore,
$$
E[\tilde{D}_r(G)]\le n \exp\Big(\log(n)\big(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1) \big) \Big)
= n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)}.
$$
By Markov inequality,
\begin{equation} \label{eq:tD}
P\big(\tilde{D}_r(G)=0 \big) = 1-
P\big(\tilde{D}_r(G)\ge 1\big) \ge 1- E[\tilde{D}_r(G)]
\ge 1- n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)} .
\end{equation}
Since $\sqrt{a}-\sqrt{b} > \sqrt{k}$, we have
$P\big(\tilde{D}(G)=0 \big)= 1-o(1)$.


Next we calculate the ratio
$$
\frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)} .
$$
By \eqref{eq:isingma}, we have
\begin{align*}
\frac{P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)}
& = \sum_{r=1}^{k-1}\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i)
-\frac{2(k-1)\alpha\log(n)}{n} \Big) \\
& \le \sum_{r=1}^{k-1}\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i) \Big)  .
\end{align*}
Since $A^r_i-A^0_i$ takes integer value between $-n/k$ and $n/k$,
we can use indicator functions to write
\begin{align*}
& \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i) \Big) \\
= & \sum_{t\log(n)=-n/k}^{n/k}
\mathbbm{1}[A^r_i-A^0_i=t \log(n)] \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t \log(n) \Big) ,
\end{align*}
where the quantity $t\log(n)$ ranges over all integer values from $-n/k$ to $n/k$ in the summation on the second line.
Define $D_r(G,t):=|\{i\in[n]:A^r_i-A^0_i= t\log(n)\}|$ and notice that $D_r(G,t)=\sum_{i=1}^n \mathbbm{1}[A^r_i-A^0_i=t \log(n)]$.
Therefore,
\begin{equation}  \label{eq:fd}
\begin{aligned}
 & \frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)}
\le \sum_{i=1}^n \sum_{r=1}^{k-1} \exp\Big(2\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i) \Big) \\
= & \sum_{i=1}^n\sum_{r=1}^{k-1}
\hspace*{0.05in}
\sum_{t\log(n)=-n/k}^{n/k}
\mathbbm{1}[A^r_i-A^0_i=t \log(n)] \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t \log(n) \Big) \\
= & \sum_{r=1}^{k-1}\sum_{t\log(n)=-n/k}^{n/k}
D_r(G,t) \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t \log(n) \Big)
\end{aligned}
\end{equation}
Define a set 
$$
\cG_1:=\bigcap_{r=1}^{k-1} \{G:\tilde{D}_r(G)=0\}.
$$
By \eqref{eq:tD}, $P(G\in\cG_1)= 1-o(1)$. By definition of $\tilde{D}_r(G)$, $G\in\cG_1$ implies that $D_r(G,t)=0$ for all $t\ge 0$ and $r=1,\dots,k-1$. Therefore, for $G\in\cG_1$, we have
\begin{equation} \label{eq:lq}
\begin{aligned}
& \frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)}
\le \sum_{r=1}^{k-1}\sum_{t\log(n)=-n/k}^{-1}
D_r(G,t) \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t \log(n) \Big)   \\
\overset{(a)}{\le} & \sum_{r=1}^{k-1}\sum_{t\log(n)=-n/k}^{-1}
D_r(G,t) \exp\big(k\beta t \log(n) \big) \\
= & \sum_{r=1}^{k-1}\sum_{t\log(n)=-n/k}^{\lfloor\frac{b-a}{k}\log(n) \rfloor} D_r(G,t)
\exp\big(k\beta t \log(n) \big)  +  \sum_{r=1}^{k-1}\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1} D_r(G,t)
\exp\big(k\beta t \log(n) \big) \\
\le &  \sum_{r=1}^{k-1}\sum_{t\log(n)=-n/k}^{\lfloor\frac{b-a}{k}\log(n) \rfloor} D_r(G,t)
\exp\big(\beta(b-a)\log(n) \big)  +  \sum_{r=1}^{k-1}\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1} D_r(G,t)
\exp\big(k\beta t \log(n) \big)  \\
\overset{(b)}{\le} & n(k-1)
\exp\big(\beta(b-a)\log(n) \big)  + \sum_{r=1}^{k-1}\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1} D_r(G,t)
\exp\big(k\beta t \log(n) \big) ,
\end{aligned}
\end{equation}
where inequality $(a)$ holds because $t\log(n)$ only takes negative values in the summation, and inequality $(b)$ follows from the trivial upper bound
$\sum_{t\log(n)=-n/k}^{\lfloor\frac{b-a}{k}\log(n) \rfloor} D_r(G,t)\le n$.
Define a function
\begin{equation} \label{eq:gt}
f_{\beta}(t):=\frac{1}{k}\sqrt{k^2t^2+4ab} -t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big) -\frac{a+b}{k} +1 +k\beta t.
\end{equation}
Then for $t\in [\frac{1}{k}(b-a), 0]$, we have 
\begin{align*}
& E[D_r(G, t)
\exp\big(k\beta t \log(n) \big)] \\
= & \sum_{i=1}^n E[\mathbbm{1}[A^r_i-A^0_i =t \log(n)]] \exp\big(k\beta t \log(n) \big) \\
= & \sum_{i=1}^n P\big(A^r_i-A^0_i=t \log(n) \big) \exp\big(k\beta t \log(n) \big) \\
\le & \sum_{i=1}^n P\big(A^r_i-A^0_i \ge t \log(n) \big) \exp\big(k\beta t \log(n) \big) \\
\le &  \sum_{i=1}^n \exp\Big(\log(n)
\Big(\frac{1}{k}\sqrt{k^2t^2+4ab} -t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big) -\frac{a+b}{k} +k\beta t +o(1)\Big)\Big) \\
= &  n^{f_{\beta}(t) +o(1)} ,
\end{align*}
where the second inequality follows from \eqref{eq:upba}.
For $\epsilon>0$, define a set
$$
\cG^r(\epsilon):=\left\{
\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1} D_r(G,t)
\exp\big(k\beta t \log(n) \big)
\le \sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1}
 n^{f_{\beta}(t) + \epsilon}
\right\} .
$$
Then by Markov inequality,
\begin{equation} \label{eq:mk}
P(G\in \cG^r(\epsilon))\ge 1-n^{-(\epsilon-o(1))} >
1-n^{-\epsilon/2}
\end{equation}
for large $n$ and positive $\epsilon$.
Using \eqref{eq:lq}, we obtain that for $G\in\cG_1\cap \bigcap_{r=1}^{k-1}\cG^r(\epsilon)$,
\begin{equation}  \label{eq:zz}
\begin{aligned}
\frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)} 
& \le n(k-1)
\exp\big(\beta(b-a)\log(n) \big)  + (k-1)\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1}  n^{f_{\beta}(t) + \epsilon} \\
& = (k-1)n^{f_{\beta}((b-a)/k)} + (k-1)\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1}  n^{f_{\beta}(t) + \epsilon} ,
\end{aligned}
\end{equation}
where the equality follows from the fact that $f_{\beta}(\frac{1}{k}(b-a))=\beta(b-a)+1$.
Recall the definitions of the functions $g(\beta)$ and $\tilde{g}(\beta)$ in \eqref{eq:gbt}.
By Lemma~\ref{lm:tus} below, we have
$f_{\beta}(t)\le \tilde{g}(\beta)<0$ for all $t\le 0$.
Combining this with \eqref{eq:zz}, we obtain that for
$G\in\cG_1\cap\cG(\epsilon)$,
\begin{equation} \label{eq:lh}
\begin{aligned}
& \frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)} 
\le (k-1)n^{\tilde{g}(\beta)}
+ (k-1)\sum_{t\log(n)=\lceil\frac{b-a}{k}\log(n) \rceil}^{-1}  n^{\tilde{g}(\beta) + \epsilon}  \\
& \le (k-1)n^{\tilde{g}(\beta) + \epsilon}
\big(\frac{a-b}{k}\log(n)+1 \big)
< n^{\tilde{g}(\beta) + 2\epsilon} 
\end{aligned}
\end{equation}
for large $n$ and positive $\epsilon$.
Let $\epsilon=-\tilde{g}(\beta)/4>0$ and define
$$
\cG^{(1)}:=\cG_1\cap \bigcap_{r=1}^{k-1}\cG(-\tilde{g}(\beta)/4) .
$$
By \eqref{eq:lh}, for $G\in\cG^{(1)}$ we have
$$
\frac{\sum_{i=1}^n P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)} <
n^{\tilde{g}(\beta) /2} .
$$
By \eqref{eq:tD} and \eqref{eq:mk}, 
$$
P(G\in\cG^{(1)})\ge 1-(k-1)n^{\tilde{g}(\beta)/8}- (k-1)n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}+o(1)}> 1- 2(k-1)n^{\tilde{g}(\beta)/8},
$$
where the last inequality follows from the fact that $1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}\le
\tilde{g}(\beta)<\tilde{g}(\beta)/8< 0$.
This completes the proof of Proposition~\ref{prop:big} for the special case of $r=1$. 
Next we prove the two auxiliary lemmas used above.
\begin{lemma}[Elementary properties of $\beta^\ast$ defined in \eqref{eq:defstar}] \label{lm:ele}
Let $g(\beta)$ and $\tilde{g}(\beta)$ be the functions defined in \eqref{eq:gbt}. Assume that $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
Then,

\begin{enumerate}[label=(\roman*)]
\item The equation $g(\beta) = 0$ has two roots, and the smaller one of them is $\beta^\ast$.

\item Denote the other root as $\beta'$. Then
$\beta^\ast< \frac{1}{2k}\log\frac{a}{b} <\beta'$.

\item $g(\beta)<0$ for all $\beta^\ast< \beta \le \frac{1}{2k}\log\frac{a}{b}$.

\item $\tilde{g}(\beta)<0$ for all $\beta>\beta^\ast$.

\item $\tilde{g}(\beta)$ is a decreasing function in $[0,+\infty)$. 

\item $\tilde{g}(\beta)<1$ for all $\beta>0$.
\end{enumerate}
\end{lemma}
\begin{proof}
{\bf Proof of (i)}:
We write $x=e^{k\beta}$. Then $g(\beta)=0$ can be written as $bx^2-(a+b-k)x+a=0$. This quadratic equation has two roots if and only if $(a+b-k)^2-4ab>0$, which is guaranteed by the assumption $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
The two roots of $bx^2-(a+b-k)x+a=0$ are
$x^\ast=\frac{a+b-k - \sqrt{(a+b-k)^2-4ab}}{2b}$ and $x'=\frac{a+b-k + \sqrt{(a+b-k)^2-4ab}}{2b}$.
Therefore, $\beta^\ast=\frac{1}{k}\log(x^\ast)$ and $\beta'=\frac{1}{k}\log(x')$.
{\bf Proof of (ii)}:
Since $x^\ast x'=\frac{a}{b}=(\sqrt{\frac{a}{b}})^2$, we have $x^\ast<\sqrt{\frac{a}{b}}<x'$, so $\beta^\ast< \frac{1}{2k}\log\frac{a}{b} <\beta'$.
{\bf Proof of (iii)}:
Since $b>0$, $bx^2-(a+b-k)x+a<0$ if and only if $x^\ast<x<x'$. Therefore, $g(\beta)<0$ if and only if $\beta^\ast< \beta <\beta'$. This implies (iii). 
{\bf Proof of (iv)}: (iv) follows directly from (iii).
{\bf Proof of (v)}: $g'(\beta)=b e^{k\beta} - a e^{-k\beta}$, so $g'(\beta)<0$ for $0\le \beta<\frac{1}{2k}\log\frac{a}{b}$, and $g(\beta)$ takes minimum value at $\beta=\frac{1}{2k}\log\frac{a}{b}$. This implies (v).
{\bf Proof of (vi)}: (vi) follows directly from (v) and the fact that $\tilde{g}(0)=1$.
\end{proof}









\begin{lemma} \label{lm:tus}
Let $f_{\beta}(t)$ be the function defined in \eqref{eq:gt}. If $a>b>0$, then $f_{\beta}(t)\le \tilde{g}(\beta)$ for all $t\le 0$.
If $\sqrt{a}-\sqrt{b}>\sqrt{k}$
and $\beta>\beta^\ast$, then we further have $f_{\beta}(t)\le \tilde{g}(\beta)<0$ for all $t\le 0$.
\end{lemma}
\begin{proof}
The first and second derivatives are
$f_{\beta}'(t)=
-\log(\sqrt{k^2 t^2+4ab}+kt)+\log(2b) +k\beta$ and $f_{\beta}''(t)=-\frac{k}{\sqrt{k^2 t^2+4ab}}<0$.
Therefore $f_{\beta}(t)$ is a concave function and takes global maximum at $t^\ast$ such that $f_{\beta}'(t^\ast)=0$.
Simple calculation shows that 
$$
t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k} \quad \text{~and~} \quad
f_{\beta}(t^\ast)=\frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1
=g(\beta) .
$$
We divide the proof into two cases. {\bf Case 1}: If $\beta\ge \frac{1}{2k}\log\frac{a}{b}$, then $t^\ast\ge 0$. Since $f_{\beta}(t)$ is an increasing function for $t\le t^\ast$, we have $f_{\beta}(t)\le f_{\beta}(0)=1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}= \tilde{g}(\beta)$ for all $t\le 0$.
{\bf Case 2}: If $\beta< \frac{1}{2k}\log\frac{a}{b}$, then we simply use the global maximum $f_{\beta}(t^\ast)$ to upper bound $f_{\beta}(t)$, i.e., $f_{\beta}(t)\le f_{\beta}(t^\ast)=g(\beta)$ for all $t$.
Combining these two cases, we have $f_{\beta}(t)\le \tilde{g}(\beta)$ for all $t\le 0$ as long as $a>b>0$.
If $\sqrt{a}-\sqrt{b}>\sqrt{k}$ and $\beta>\beta^\ast$, then by property (iv) of Lemma~\ref{lm:ele} we further have $f_{\beta}(t)\le \tilde{g}(\beta)<0$ for all $t\le 0$.
\end{proof}
\subsection{Proof of Proposition~\ref{prop:big} for general $r$} \label{sect:gen}
We want to bound the ratio
$$
\sum_{\cI\subseteq[n]:|\cI|=r}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)}
$$
for all $r\le kn/\log^{1/3}(n)$.
To that end,
for $\cI\subseteq[n]$ and an index vector $v$ of $\cI$,
define the different parts of $\cI$ as
$$
\cI_j:=\{i\in\cI:X_i= \omega^j\}, j=0, 1, \dots, k-1 \text{ and } \cI_{j_1,j_2}:=\{i\in\cI:X_i= \omega^{j_1}, v_{\mathrm{index}(i)} = \omega^{j_2}\}
$$
and define 
$$
\nabla \cI:=\{\{i,j\}: \{i,j\} \not\subseteq [n]\setminus\cI\} .
$$
We further define
$$
\nabla\cI_+:=\{\{i,j\}\in\nabla\cI:X_i=X_j\}
\quad \text{and} \quad
\nabla\cI_v:=\{\{i,j\}\in\nabla\cI:X_j= v_{\mathrm{index}(i)} \cdot X_i\}.
$$
Then 
\begin{equation} \label{eq:partial}
\begin{aligned}
& |\nabla\cI_+|=\sum_{j=0}^{k-1}\Big(|\cI_j|(\frac{n}{k}-|\cI_j|) + \frac{|\cI_j| (|\cI_j|-1)}{2}\Big)
=\frac{n}{k}|\cI|- \frac{1}{2}\sum_{j=0}^{k-1}|\cI_j|(|\cI_j|+1) , \\
& |\nabla\cI_v|=\sum_{j_1}^{k-1}\sum_{j_2=1}^{k-1}|\cI_{j_1, j_2}|\frac{n}{k} =\frac{n}{k}|\cI|
.
\end{aligned}
\end{equation}
Given a graph $G$,
define
\begin{align*}
& A_{\cI}=A_{\cI}(G):=|\{\{i,j\}\in\nabla\cI\cap E(G):X_i=X_j\}| ,  \\
& B_{\cI,v}=B_{\cI,v}(G):=|\{\{i,j\}\in\nabla\cI\cap E(G):X_j= v_{\mathrm{index}(i)} \cdot X_i\}| .
\end{align*}




\begin{proposition}
For $t\in [\frac{1}{k}(b-a), 0]$
and $|\cI|\le kn/\log^{1/3}(n),v$
\begin{equation} \label{eq:upmpt}
\begin{aligned}
& P(B_{\cI,v}-A_{\cI}\ge t|\cI|\log(n))  \\
\le & \exp\Big(|\cI|\log(n)
\Big(\frac{1}{k}\sqrt{k^2 t^2 + 4ab} -t\big(\log(\sqrt{k^2 t^2 + 4ab}+t)-\log(2b) \big) -\frac{a+b}{k} 
+ O(\log^{-1/3}(n)) \Big)\Big) .
\end{aligned}
\end{equation}
\end{proposition}
\begin{proof}
By definition, $A_{\cI}\sim\Binom(|\nabla\cI_+|,\frac{a\log(n)}{n})$ and $B_{\cI,v}\sim\Binom(|\nabla\cI_v|,\frac{b\log(n)}{n})$, and they are independent. For $s>0$, the moment generating function of $B_{\cI,v}-A_{\cI}$ for $|\cI|\le kn/\log^{1/3}(n)$ can be bounded from above as follows:
\begin{align*}
& E[e^{s(B_{\cI,v}-A_{\cI})}] \\
& =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^s \Big)^{n|\cI|/k
}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-s} \Big)^{n |\cI|/k-\frac{1}{2}\sum_{j=0}^{k-1}(|\cI_j|+1)|\cI_j| }  \\
& \le 
\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^s \Big)^{n|\cI|/k}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-s} \Big)^{n |\cI|/k-|\cI|^2}
 \\
& \le
\exp\Big(\frac{|\cI|\log(n)}{k}(a e^{-s}+b e^s-a-b +\frac{ka|\cI|}{n})
+|\cI|O(\frac{\log^2(n)}{n})\Big) \\
& =
\exp\Big(\frac{|\cI|\log(n)}{k}(a e^{-s}+b e^s-a-b +
O(\log^{-1/3}(n))) \Big),
\end{align*}
where the first inequality follows from $1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^s>1$ and $1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-s}<1$; in the second inequality we use the Taylor expansion $\log(1+x)=x+O(x^2)$; the last equality follows from the assumption that $|\cI|\le kn/\log^{1/3}(n)$.
By Chernoff bound, for $s>0$, we have
\begin{align*} 
& P(B_{\cI,v}-A_{\cI}\ge t|{\cI}|\log(n))\le
\frac{E[e^{s(B_{\cI,v}-A_{\cI})}]}{e^{st|{\cI}|\log(n)}}  \\
\le & \exp\Big(\frac{{\cI}\log(n)}{k} \big(a e^{-s}+b e^s -kst -a-b
 + O(\log^{-1/3}(n)) \big)\Big)  .
\end{align*}
The rest of the proof is to find $s^\ast$ to minimize $a e^{-s}+b e^s -kst$ and take $s^\ast$ into the above bound. This is exactly the same as the proof of \eqref{eq:upba}, and we do not repeat it here.
\end{proof}


Given a graph $G$, define 
$$
\tilde{D}^{(r,v)}(G):=|\{\cI\subseteq[n],|\cI|=r:B_{\cI,v}- A_{\cI}\ge 0\}|
\text{~~and~~}
\tilde{D}^v_{\cI}(G):=\mathbbm{1}[B_{\cI,v}-A_{\cI}\ge 0] .
$$
 Then
$\tilde{D}^{(r,v)}(G)=\sum_{\cI\subseteq[n],|\cI|=r} \tilde{D}^v_{\cI}(G)$ and
$$
E[\tilde{D}^{(r, v)}(G)]=\sum_{\cI\subseteq[n],|\cI|=r} E[\tilde{D}^v_{\cI}(G)]
=\sum_{\cI\subseteq[n],|\cI|=r} P(B_{\cI,v}- A_{\cI}\ge 0).
$$
Taking $t=0$ into \eqref{eq:upmpt}, we have
$P(B_{\cI,v}- A_{\cI}\ge 0)\le \exp\big(|\cI|\log(n)(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) ) \big)$. Therefore,
\begin{align*}
& E[\tilde{D}^{(r,v)}(G)]\le \binom{n}{r} \exp\Big(r \log(n)\big(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) \big) \Big) \\
\le & n^r \exp\Big(r \log(n)\big(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) \big) \Big)
= n^{r( 1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) )}.
\end{align*}
By Markov inequality,
\begin{equation} \label{eq:Dk}
P\big(\tilde{D}^{(r,v)}(G)=0 \big) = 1-
P\big(\tilde{D}^{(r,v)}(G)\ge 1\big) \ge 1- E[\tilde{D}^{(r,v)}(G)]
\ge 1-  n^{r (1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) )} .
\end{equation}
Since $\sqrt{a}-\sqrt{b} > \sqrt{k}$, we have
$P\big(\tilde{D}^{(r,v)}(G)=0 \big)= 1-o(1)$.


By \eqref{eq:isingma}, we have
\begin{equation} \label{eq:ts}
\begin{aligned}
& \frac{P_{\sigma|G}(\sigma=X^{(\sim\cI, v)})}{P_{\sigma|G}(\sigma=X)} \\
= &\exp\Big(\beta\sum_{\{i,j\}\in E(G)} (I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)})
-I(X_i,X_j))
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} (I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)})
-I(X_i,X_j)) \Big) \\
=& \exp\Big((\beta+ \frac{\alpha \log(n)}{n})\sum_{\{i,j\}\in E(G) \cap \nabla \cI} (I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)})
-I(X_i,X_j))
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\} \in \nabla \cI}  (I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)})
-I(X_i,X_j))\Big) \\
\overset{(a)}{=}  & \exp\Big( k(\beta+ \frac{\alpha \log(n)}{n})( B_{\cI,v} - A_{\cI})
-\frac{k\alpha\log(n)}{n} \big( |\nabla \cI_v | - | \nabla \cI_+ | \big)\Big) \\
\overset{(b)}{=} & \exp\Big( k\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI,v}-A_{\cI})
- \frac{k\alpha\log(n)}{2n}\sum_{j=0}^{j-1} | \cI_j | ( |\cI_j| + 1)
\Big) \\
\le & \exp\Big( k\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI,v}-A_{\cI})
\Big) ,
\end{aligned}
\end{equation}
where $(a)$ follows from the fact that:
\begin{align*}
\sum_{\{i,j\}\in E(G) \cap \nabla \cI} I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)}) &= (k-1) B_{\cI, v} - \sum_{v'\neq v} B_{\cI, v'} - A_{\cI} \\
\sum_{\{i,j\}\in E(G) \cap \nabla \cI} I(X_i, X_j) &= (k-1)A_{\cI} - \sum_{v'} B_{\cI, v'} \\
\sum_{\{i,j\}\in \nabla \cI} I(X_i^{(\sim\cI, v)}, X_j^{(\sim\cI, v)}) &= (k-1) |\nabla I_v| - \sum_{v'\neq v} | \nabla I_{v'}| - | \nabla I_{+}| \\
\sum_{\{i,j\}\in \nabla \cI} I(X_i, X_j) &= (k-1) | \nabla I_{+} | - \sum_{v'} | \nabla I_{v'} |
\end{align*}
and $(b)$ follows from \eqref{eq:partial}.
Since $B_{\cI,v}-A_{\cI}$ takes integer value between $-|\cI|n/k$ and $|\cI|n/k$,
we can use indicator functions to write
\begin{align*}
& \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI,v}-A_{\cI}) \Big) \\
= & \sum_{t|\cI|\log(n)=-|\cI|n/k}^{|\cI|n/k}
\mathbbm{1}[B_{\cI}-A_{\cI}=t|\cI| \log(n)] \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t|\cI| \log(n) \Big) ,
\end{align*}
where the quantity $t|\cI|\log(n)$ ranges over all integer values from $-|\cI|n/k$ to $|\cI|n/k$ in the summation on the second line.
Define $D^{(r,v)}(G,t):=|\{\cI\subseteq[n],|\cI|=r: B_{\cI,v}-A_{\cI}= tr\log(n)\}|$ and
notice that $D^{(r,v)}(G,t)=\sum_{\cI\subseteq[n],|\cI|=r} \mathbbm{1}[B_{\cI,v}-A_{\cI}= tr\log(n)]$.
Therefore,
\begin{align*}
 & \sum_{\cI\subseteq[n],|\cI|=r}
 \frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)}
\le \sum_{\cI\subseteq[n],|\cI|=r, v} \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI, v}-A_{\cI}) \Big) \\
= & \sum_{\cI\subseteq[n],|\cI|=r, v}
\hspace*{0.05in}
\sum_{tr\log(n)=-rn/k}^{rn/k}
\mathbbm{1}[B_{\cI,v}-A_{\cI}=tr \log(n)] \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) tr \log(n) \Big) \\
= & \sum_{v}\sum_{tr\log(n)=-rn/k}^{rn/k}
D^{(r.v)}(G,t) \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t r\log(n) \Big)
\end{align*}
Define a set 
\begin{equation} \label{eq:g1k}
\cG_1^{(r)}:=\bigcap_{v \in \{\omega, \dots, \omega^{k-1}\}^r}\{G:\tilde{D}^{(r,v)}(G)=0\}.
\end{equation}
By \eqref{eq:Dk}, $P(G\in\cG_1^{(r)})= 1-o(1)$. By definition of $\tilde{D}^{(r,v)}(G)$,
$G\in\cG_1^{(r)}$ implies that $D^{(r,v)}(G,t)=0$ for all $t\ge 0$.
Therefore, for $G\in\cG_1^{(r)}$, we have
\begin{equation} \label{eq:bk}
\begin{aligned}
& \sum_{\cI\subseteq[n],|\cI|=r}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} \\
\le & \sum_{v}\sum_{tr\log(n)=-rn/k}^{-1}
D^{(r,v)}(G,t) \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) t r \log(n) \Big)   \\
\overset{(a)}{\le} & \sum_v\sum_{tr\log(n)=-rn/k}^{-1}
D^{(r,v)}(G,t) \exp\big(k\beta t r \log(n) \big) \\
= & \sum_v \sum_{tr\log(n)=-rn/k}^{\lfloor\frac{b-a}{k}r\log(n) \rfloor} D^{(r,v)}(G,t) \exp\big(k\beta t r \log(n) \big) \\
& \hspace*{1.5in} + \sum_v \sum_{tr\log(n)=\lceil\frac{b-a}{k}r\log(n) \rceil}^{-1} D^{(r,v)}(G,t) \exp\big(k\beta t r \log(n) \big) \\
\le & \sum_v\sum_{tr\log(n)=-rn/k}^{\lfloor\frac{b-a}{k}r\log(n) \rfloor} D^{(r,v)}(G,t)
\exp\big(\beta(b-a)r\log(n) \big) \\
& \hspace*{1.5in} +\sum_v \sum_{tr\log(n)=\lceil\frac{b-a}{k}r\log(n) \rceil}^{-1} D^{(r,v)}(G,t) \exp\big(k\beta t r \log(n) \big)  \\
\overset{(b)}{\le} & (k-1)^r\binom{n}{k}
\exp\big(\beta(b-a) r \log(n) \big)  + \sum_v \sum_{tr\log(n)=\lceil\frac{b-a}{k}r\log(n) \rceil}^{-1} D^{(r,v)}(G,t) \exp\big(k\beta t r \log(n) \big),
\end{aligned}
\end{equation}
where inequality $(a)$ holds because $tk\log(n)$ only takes negative values in the summation, and inequality $(b)$ follows from the trivial upper bound
$\sum_{tr\log(n)=-rn/k}^{\lfloor\frac{b-a}{k}r\log(n) \rfloor} D^{(r,v)}(G,t)\le \binom{n}{r}$.




Recall the function $f_{\beta}(t)$ defined in \eqref{eq:gt}.
Then for $t\in [\frac{1}{k}(b-a), 0]$, we have 
\begin{align*}
& E[D^{(r,v)}(G,t) \exp\big(k\beta t r \log(n) \big)] \\
= & \sum_{\cI\subseteq[n],|\cI|=r} E[\mathbbm{1}[B_{\cI,v}-A_{\cI}=t r \log(n)]] \exp\big(k\beta t r \log(n) \big) \\
= & \sum_{\cI\subseteq[n],|\cI|=r} P\big(B_{\cI,v}-A_{\cI}=t r \log(n) \big) \exp\big(k\beta t r \log(n) \big) \\
\le & \sum_{\cI\subseteq[n],|\cI|=r} P\big(B_{\cI,v}-A_{\cI} \ge t r \log(n) \big) \exp\big(k\beta t r \log(n) \big) \\
\le &  \sum_{\cI\subseteq[n],|\cI|=r} \exp\Big( r \log(n)
\Big(\frac{1}{k}\sqrt{k^2t^2+4ab} -t\big(\log(\sqrt{k^2t^2+4ab}+t)-\log(2b) \big) -\frac{a+b}{k} +k\beta t +o(1) \Big)\Big) \\
= & \binom{n}{r} n^{r(f_{\beta}(t)-1+o(1))} ,
\end{align*}
where the second inequality follows from \eqref{eq:upmpt}.
For $\epsilon>0$, define a set
\begin{equation} \label{eq:gep}
\begin{aligned}
\cG^{(r,v)}(\epsilon):=\left\{
\sum_{tr\log(n)=\lceil\frac{b-a}{k}r\log(n) \rceil}^{-1} D^{(r,v)}(G,t) \exp  \big(k\beta t r \log(n) \big) \hspace*{1.5in} \right. \\
\left.
 \le  \sum_{tr\log(n)=\lceil\frac{b-a}{k}r\log(n) \rceil}^{-1}
\binom{n}{r} n^{r(f_{\beta}(t)-1+\epsilon)}
\right\} .
\end{aligned}
\end{equation}
Then by Markov inequality,
\begin{equation} \label{eq:Gk}
P(G\in \cG^{(r,v)}(\epsilon))\ge 1-n^{-r(\epsilon-o(1))} >1-n^{-r\epsilon/2}
\end{equation}
for large $n$ and positive $\epsilon$.
Using \eqref{eq:bk}, we obtain that for $G\in\cG_1^{(r)}\cap \bigcap_{v} \cG^{(r,v)}(\epsilon)$,
\begin{equation}  \label{eq:3l}
\begin{aligned}
& \sum_{\cI\subseteq[n],|\cI|=r}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)}  \\
\le & (k-1)^r\binom{n}{k}
\exp\big(\beta(b-a) r \log(n) \big)  + \sum_v\sum_{tr\log(n)=\lceil\frac{b-a}{k}r\log(n) \rceil}^{-1}
\binom{n}{r} n^{r(f_{\beta}(t)-1+\epsilon)} \\
= & (k-1)^r\binom{n}{r} n^{r(f_{\beta}((b-a)/k)-1)} + (k-1)^r\sum_{tr\log(n)=\lceil\frac{b-a}{k}r\log(n) \rceil}^{-1}
\binom{n}{r} n^{r(f_{\beta}(t)-1+\epsilon)} ,
\end{aligned}
\end{equation}
where the equality follows from the fact that $f_{\beta}(\frac{1}{k}(b-a))=\beta(b-a)+1$.
Recall the function $\tilde{g}(\beta)$ defined in \eqref{eq:gbt}, and recall from  Lemma~\ref{lm:tus} that
$f_{\beta}(t)\le \tilde{g}(\beta)<0$ for all $t\le 0$.
Using this in \eqref{eq:3l} together with the fact $\binom{n}{r}<n^r$, we obtain that for
$G\in\cG_1^{(r)}\cap \bigcap_{v} \cG^{(r,v)}(\epsilon)$,
\begin{equation} \label{eq:wuh}
\begin{aligned}
& \sum_{\cI\subseteq[n],|\cI|=r}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} 
\le (k-1)^rn^{k\tilde{g}(\beta)}
+ (k-1)^r\sum_{tr\log(n)=\lceil\frac{b-a}{k}r\log(n) \rceil}^{-1}  n^{r(\tilde{g}(\beta) + \epsilon)}  \\
& \le(k-1)^r n^{r(\tilde{g}(\beta) + \epsilon)}
\Big(\frac{a-b}{k}\log(n^r)+1 \Big)
< n^{r(\tilde{g}(\beta) + 2\epsilon)} 
\end{aligned}
\end{equation}
for large $n$ and positive $\epsilon$.
Let $\epsilon=-\tilde{g}(\beta)/4>0$ and define
$$
\cG^{(r)}:=\cG_1^{(r)}\cap \bigcap_{v} \cG^{(r,v)}(-\tilde{g}(\beta)/4) .
$$
By \eqref{eq:wuh}, for $G\in\cG^{(r)}$ we have
$$
\sum_{\cI\subseteq[n],|\cI|=k}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} <
n^{r \tilde{g}(\beta) /2} .
$$
By \eqref{eq:Dk}, \eqref{eq:g1k} and \eqref{eq:Gk}, 
$$
P(G\in\cG^{(r)})\ge 1-(k-1)^rn^{r\tilde{g}(\beta)/8}- (k-1)^rn^{r (1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) )}> 1- 2(k-1)^r n^{k\tilde{g}(\beta)/8},
$$
where the last inequality follows from the fact that $1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}\le
\tilde{g}(\beta)<\tilde{g}(\beta)/8< 0$.

\section{Samples differ from $\Gamma$ in $O(n^{\theta})$ coordinates for some $\theta<1$ when $\beta\le\beta^\ast$}
\label{sect:theta}



\begin{proposition}[Refinement of Proposition~\ref{prop:1}] \label{prop:43}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\alpha>b\beta$ and $\beta\le\beta^\ast$. Let $m$ be a constant integer that is independent of $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
$$
Then for any (arbitrarily small) $\delta>0$ and any (arbitrarily large) $r>0$, there exists $n_0(\delta, r)$ such that for all even integers $n>n_0(\delta, r)$,
$$
P_{\SIBM} \Big(\dist(\sigma^{(i)},\Gamma)<n^{g(\beta)+\delta}
\text{~for all~} i\in[m] \Big) \ge 1- n^{-r} .
$$
\end{proposition}

By Lemma~\ref{lm:ele} (vi), we know that $g(\beta)<1$ for all $0<\beta\le\beta^\ast$, so we can always choose a $\delta>0$ such that $g(\beta)+\delta<1$.
Then Proposition~\ref{prop:43} implies that when $\beta\le\beta^\ast$, with probability $1-o(1)$ all samples differ from $\Gamma$ in $O(n^\theta)$ coordinates for some $\theta<1$.


Since we assume that $m$ is a constant that is independent of $n$,
we only need to prove Proposition~\ref{prop:43} for the special case of $m=1$, and the case of general values of $m$ follows immediately from this special case.
Proposition~\ref{prop:1} tells us that if $\alpha>b\beta$, then $\dist(\sigma,\pm X) \le kn/\log^{1/3}(n)$
 with probability $1-n^{-2r}$ for any given $r>0$ and large enough $n$.
Also note that
\begin{align*}
 \sum_{\cI\subseteq[n],~
n^{g(\beta)+\delta}
\le |\cI|\le kn/\log^{1/3}(n)}
\frac{P_{\sigma|G}(\sigma=f(X)^{(\sim\cI)})}{P_{\sigma|G}(\sigma=f(X))} 
=  \sum_{\cI\subseteq[n],~
n^{g(\beta)+\delta}
\le |\cI|\le kn/\log^{1/3}(n)}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} .
\end{align*}
Therefore, to prove Proposition~\ref{prop:43}, we only need to show that given $r>0$, there exists a set $\cG_{\delta}$ such that the following two conditions hold for large enough $n$:  (i)
$P(G\in\cG_{\delta})\ge 1-n^{-2r}$, and (ii)
For every $G\in\cG_{\delta}$,
$$
\sum_{\cI\subseteq[n],~
n^{g(\beta)+\delta}
\le |\cI|\le kn/\log^{1/3}(n)}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} \le n^{-2r} .
$$
The existence of $\cG_{\delta}$ is guaranteed by the following proposition:
\begin{proposition} \label{prop:xz}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$.
Let 
$$
(X,G,\sigma) \sim \SIBM(n, k, a\log(n)/n, b\log(n)/n,\alpha,\beta, 1) .
$$
For any $\delta>0$, there exists $n_0(\delta)$ such that
for every even integer $n>n_0(\delta)$ and
every integer $n^{g(\beta)+\delta} \le s \le kn/\log^{1/3}(n)$,
there is a set $\cG_{\delta}^{(s)}$ for which

\noindent (i)
$P(G\in\cG_\delta^{(s)}) \ge 1- 2(k-1)^s n^{-s\delta'}$,
where $\delta':=\min(\frac{\delta}{8},\frac{(\sqrt{a}-\sqrt{b})^2}{2k} - \frac{1}{2}) >0$.

\noindent (ii) For every $G\in\cG_\delta^{(s)}$,
$$
\sum_{\cI\subseteq[n],|\cI|=s}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} <
n^{-s \delta /4} .
$$
\end{proposition}
With the $\cG_\delta^{(s)}$'s given by Proposition~\ref{prop:xz}, we
define 
$$
\cG_\delta:=\bigcap_{s=n^{g(\beta)+\delta}}^{kn/\log^{1/3}(n)} \cG_\delta^{(s)} .
$$
By the union bound,
$$
P(G\in\cG_\delta)\ge 1-2 \sum_{s=n^{g(\beta)+\delta}}^{kn/\log^{1/3}(n)} (k-1)^sn^{-s\delta'}
> 1-2 \sum_{s=\lceil 4r/\delta' \rceil}^{+\infty} (k-1)^sn^{-s\delta'}
\ge 1- (k-1)^{\lceil 4r/\delta' \rceil}\frac{2 n^{-4r}}{1-n^{-\delta'}}
>1-n^{-2r}
$$
for large enough $n$.
 Moreover, for every $G\in\cG_\delta$ and large enough $n$,
\begin{align*}
& \sum_{\cI\subseteq[n],~~
n^{g(\beta)+\delta}\le |\cI|\le kn/\log^{1/3}(n)}
\frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} =
\sum_{s=n^{g(\beta)+\delta}}^{2n/\log^{1/3}(n)}
\hspace*{0.05in}
\sum_{\cI\subseteq[n],|\cI|=s}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)}  \\
& < \sum_{s=n^{g(\beta)+\delta}}^{2n/\log^{1/3}(n)}
n^{-s \delta /4}
< \sum_{s=\lceil 16r/\delta \rceil}^{+\infty}
n^{-s \delta /4}
\le \frac{n^{-4r}}{1-n^{-\delta /4}} < n^{-2r} .
\end{align*}
Thus we have shown that Proposition~\ref{prop:43} is implied by Proposition~\ref{prop:xz}. Now we are left to prove the latter proposition.
It turns out that all we need for the proof of Proposition~\ref{prop:xz} is a tighter inequality than \eqref{eq:wuh}.
Recall that we obtain \eqref{eq:wuh} from \eqref{eq:3l} by using a coarse upper bound $\binom{n}{s}<n^s$.
Here we use a tighter upper bound $\binom{n}{s}<n^s/(s!)$ in \eqref{eq:3l} and obtain that for
$G\in\cG_1^{(s)}\cap\cG^{(s)}(\epsilon)$,
(see the definitions of $\cG_1^{(s)}$ and $\cG^{(s)}(\epsilon)$
in \eqref{eq:g1k} and \eqref{eq:gep})
\begin{equation} \label{eq:zm}
\begin{aligned}
& \sum_{\cI\subseteq[n],|\cI|=s}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} 
< (k-1)^s(s!)^{-1} n^{sf_{\beta}((b-a)/k)} +
(k-1)^s\sum_{ts\log(n)=\lceil\frac{b-a}{k}s\log(n) \rceil}^{-1}
(r!)^{-1} n^{r(f_{\beta}(t)+\epsilon)} \\
& \le (k-1)^s(s!)^{-1} n^{sg(\beta)}
+ (k-1)^s\sum_{ts\log(n)=\lceil\frac{b-a}{k}s\log(n) \rceil}^{-1}  (s!)^{-1} n^{s(g(\beta) + \epsilon)}  \\
& \le (s!)^{-1} (k-1)^s n^{s(g(\beta) + \epsilon)}
\Big(\frac{a-b}{k}\log(n^s)+1 \Big) \\
& < (s!)^{-1} n^{s(g(\beta) + 2\epsilon)}  ,
\end{aligned}
\end{equation}
where the second inequality holds because
$f_{\beta}(t)\le \tilde{g}(\beta) \le g(\beta)$ for all $t\le 0$
(see  Lemma~\ref{lm:tus}), and the last inequality holds for positive $\epsilon$ and large $n$.
It is well known that\footnote{We know from the Taylor expansion that $e^x>x^k/(k!)$ for any $x>0$. Taking $x=k$ gives us $k!>(k/e)^k$.}
$s!>(s/e)^s$
for all positive integer $s$.
Therefore, for $s>n^{g(\beta)+\delta}$, we have
$$
s!>(s/e)^s
=\exp(s\log(s)-s)
>\exp(s(g(\beta)+\delta)\log(n)-s)
=n^{s(g(\beta)+\delta-o(1))}
$$
Taking this into \eqref{eq:zm}, we obtain that $G\in\cG_1^{(s)}\cap\cG^{(s)}(\epsilon)$,
$$
\sum_{\cI\subseteq[n],|\cI|=s}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} 
< n^{s(2\epsilon-\delta+o(1))}
< n^{s(3\epsilon-\delta)}
$$
for positive $\epsilon$ and large $n$.
Let $\epsilon=\delta/4$ and define 
$$
\cG_{\delta}^{(s)}
:=\cG_1^{(s)}\cap\cG^{(s)}(\delta/4) .
$$
Then for $G\in\cG_{\delta}^{(s)}$ we have
$$
\sum_{\cI\subseteq[n],|\cI|=s}
\frac{P_{\sigma|G}(\sigma=X^{(\sim \cI)} )}
{P_{\sigma|G}(\sigma=X)} <
n^{-s \delta /4} .
$$
By \eqref{eq:Dk}, \eqref{eq:g1k} and \eqref{eq:Gk}, 
\begin{align*}
P(G\in\cG_{\delta}^{(k)})
& \ge 1-(k-1)^s n^{-s\delta/8}- (k-1)^s n^{s (1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} + o(1) )} \\
& \ge 1-(k-1)^s n^{-s\delta/8}- (k-1)^sn^{s(\frac{1}{2}-\frac{(\sqrt{a}-\sqrt{b})^k}{2k} )}
> 1- 2 (k-1)^s n^{-s\delta'},
\end{align*}
where the second inequality follows from  $1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}< 0$, and the last inequality follows from the definition $\delta'=\min(\frac{\delta}{8},\frac{(\sqrt{a}-\sqrt{b})^2}{2k} - \frac{1}{2})$.
\section{Exact recovery in $O(n)$ time when $\lfloor \frac{m+1}{2} \rfloor \beta>\beta^\ast$}
\label{sect:direct}
\section{Samples differ from $\Gamma$ in $\Theta(n^{g(\beta)})$ coordinates when $\beta\le\beta^\ast$}  \label{sect:struct}
Recall the definitions of $A^r_i$ in previous sections; see the beginning of Section~\ref{sect:k=1}. 
By definition,
$A^0_i\sim \Binom(\frac{n}{k}-1,\frac{a\log(n)}{n})$ and $A^r_i\sim \Binom(\frac{n}{k}, \frac{b\log(n)}{n})$ for $r>0$, and they are independent. 
Also note that $A^r_i$ are functions of the underlying graph $G$.

\begin{proposition}  \label{prop:con}
Let $(X,G)\sim \SSBM(n,k, a\log(n)/n, b\log(n)/n)$, where $\sqrt{a}-\sqrt{b} > \sqrt{k}$.
Suppose that $0< \beta\le \beta^\ast$.
Then there is a set $\cG_{\con}$ such that (i) $P(G \in \cG_{\con}) = 1-o(1)$ and (ii) for every $G\in \cG_{\con}$, 
$$
\sum_{i=1}^n \exp\big(k\beta (A^r_i-A^0_i) \big)
=(1+o(1)) n^{g(\beta)} .
$$
\end{proposition}

\begin{proof}
Let $\cG_1:=\{G:A^r_i-A^0_i<0\text{~for all~}i\in[n] \text{ and } 1\leq r \leq k-1 \}$. By \eqref{eq:tD}, we have $P(G\in\cG_1)=1-o(1)$. We will prove that
\begin{align}
& E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big]
= (1+o(1)) n^{g(\beta)}  , \label{eq:cg} \\
& \Var \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big]
= o (n^{g(\beta)} ) .  \label{eq:sw}
\end{align}
Then the proposition follows immediately from  Chebyshev's inequality and the fact that $g(\beta)\ge 0$ when $0<\beta\le\beta^\ast$ (see Lemma~\ref{lm:ele}).


In Proposition~\ref{prop:df} (see Appendix~\ref{ap:um}), we prove \eqref{eq:cg} for $0<\beta<\frac{1}{2k}\log\frac{a}{b}$. By Lemma~\ref{lm:ele}, $\beta^\ast<\frac{1}{2k}\log\frac{a}{b}$, so \eqref{eq:cg} holds for $0< \beta\le \beta^\ast$.
Now we are left to prove \eqref{eq:sw}. Observe that
\begin{equation} \label{eq:mh1}
\begin{aligned}
& \Var \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big] \\
= & \sum_{i=1}^n  \Var \big[\exp\big(k\beta (A^r_i-A^0_i) \big) ~\big|~ G\in\cG_1 \big] \\
& \hspace*{1.2in} + \sum_{i,j\in[n],i\neq j}
\Cov(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j) \big) ~\big|~ G\in\cG_1 ) \\
\le & \sum_{i=1}^n E \big[ \exp\big(2k\beta (A^r_i-A^0_i) \big) ~\big|~ G\in\cG_1 \big] \\
& \hspace*{1.2in} + \sum_{i,j\in[n],i\neq j}
\Cov(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j) \big) ~\big|~ G\in\cG_1) .
\end{aligned}
\end{equation}
By Corollary~\ref{cr:yy} in Appendix~\ref{ap:um}, for all $\beta>0$ we have
$$
\sum_{i=1}^n E[ \exp\big(2k\beta (A^r_i-A^0_i) \big) \big| G\in\cG_1]
= O ( n^{\tilde{g}(2\beta)} ) .
$$
By Lemma~\ref{lm:ele}, $\tilde{g}(\beta)$ is a decreasing function, and it is strictly decreasing when $\beta\le\beta^\ast<\frac{1}{2k}\log\frac{a}{b}$. Therefore, $g(\beta)=\tilde{g}(\beta)>\tilde{g}(2\beta)$ whenever $\beta\le\beta^\ast$. As a consequence, 
\begin{equation} \label{eq:mh2}
\sum_{i=1}^n E[ \exp\big(2k\beta (A^r_i-A^0_i) \big) \big| G\in\cG_1]
< o ( n^{g(\beta)} ) .
\end{equation}



Now we are left to bound the covariance of $\exp\big(k\beta (A^r_i-A^0_i) \big)$ and $\exp\big(k\beta (A^r_j-A^0_j) \big)$ for $i\neq j$.
Define $\xi_{ij}=\xi_{ij}(G):=\mathbbm{1}[\{i,j\}\in E(G)]$ as the indicator function of the edge $\{i,j\}$ connected in graph $G$.
Now suppose that\footnote{The case of $X_i=X_j$ can be handled in the same way.} $X_i\neq X_j$. Then we can decompose $A'^r_i$ and $A'^r_j$ as $A^r_i=A'^r_i+\xi_{ij}$ and $A^r_j=A'^r_j+\xi_{ij}$, where both $A'^r_i$ and $A'^r_j$ have distribution $\Binom(\frac{n}{k} - 1 , \frac{b\log(n)}{n})$,
and the five random variables $A^0_i, A^0_j, A'^r_i,A'^r_j$ and $\xi_{ij}$ are independent.
Therefore,
\begin{align*}
& \Cov(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j) \big) ) \\
= & E[\exp\big(k\beta (A^r_i-A^0_i+A^r_j-A^0_j) \big)] 
-E[\exp\big(k\beta (A^r_i-A^0_i) \big)] E[\exp\big(k\beta (A^r_j-A^0_j) \big)]   \\
= & E[\exp\big(k\beta (A'^r_i-A^0_i) \big)] E[\exp\big(k\beta (A'^r_j-A^0_j) \big)]  \Big( E[\exp(2k\beta \xi_{ij})] -
\big(E[\exp(k\beta \xi_{ij})]\big)^2 \Big) \\
= & E[\exp\big(k\beta (A'^r_i-A^0_i) \big)] E[\exp\big(k\beta (A'^r_j-A^0_j) \big)] \\
& \hspace*{1.2in}
\Big( 1-\frac{b\log(n)}{n} + \frac{b\log(n)}{n} e^{2k\beta} -
\Big(1-\frac{b\log(n)}{n} + \frac{b\log(n)}{n} e^{k\beta} \Big)^2 \Big) \\
= & \Theta\Big( \frac{\log(n)}{n} \Big) E[\exp\big(k\beta (A'^r_i-A^0_i) \big)] E[\exp\big(k\beta (A'^r_j-A^0_j) \big)] \\
= & \Theta\Big( \frac{\log(n)}{n} \Big) E[\exp\big(k\beta (A^r_i-A^0_i) \big)] E[\exp\big(k\beta (A^r_j-A^0_j) \big)]  ,
\end{align*} 
where the last equality holds because $\exp\big(k\beta (A'^r_i-A^0_i) \big)$ differs from $\exp\big(k\beta (A^r_i-A^0_i) \big)$  by a factor of at most $e^{k\beta}$. 
By \eqref{eq:pl} in Appendix~\ref{ap:um}, $E \big[  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\big|~ G\in\cG_1 \big] 
= (1+o(1)) E \big[  \exp\big(k\beta (A^r_i-A^0_i) \big) \big]$ when $0<\beta\le\beta^\ast$. By Lemma \ref{lem:BijG} in Appendix~\ref{ap:um} we have $E[\exp\big(k\beta (A^r_i-A^0_i+A^r_j-A^0_j) ~\big|~ G\in\cG_1 \big)] = (1+o(1)) E[\exp\big(k\beta (A^r_i-A^0_i+A^r_j-A^0_j) \big)]$ when $0<\beta\le\beta^\ast$.
Therefore,
\begin{align*}
& \Cov \big(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j)  \big) ~\big|~ G\in\cG_1 \big) \\
= & (1+o(1)) \Cov \big(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j)  \big) \big) \\
= & \Theta\Big( \frac{\log(n)}{n} \Big) E[\exp\big(k\beta (A^r_i-A^0_i) \big)] E[\exp\big(k\beta (A^r_j-A^0_j) \big)] .
\end{align*}
As a consequence,
\begin{align*}
& \sum_{i,j\in[n],i\neq j}
\Cov \big(\exp\big(k\beta (A^r_i-A^0_i) \big), \exp\big(k\beta (A^r_j-A^0_j) \big) ~\big|~ G\in\cG_1 \big) \\
= & \Theta\Big( \frac{\log(n)}{n} \Big) \sum_{i,j\in[n],i\neq j} \Big( E[\exp\big(k\beta (A^r_i-A^0_i) \big)] E[\exp\big(k\beta (A^r_j-A^0_j) \big)] \Big) \\
\le & \Theta\Big( \frac{\log(n)}{n} \Big)
\Big(\sum_{i=1}^n  E[\exp\big(k\beta (A^r_i-A^0_i) \big)] \Big)^2 \\
\overset{(a)}{=} & \Theta(n^{2g(\beta)-1} \log(n)) \\
\overset{(b)}{=} & o(n^{g(\beta)})
\end{align*}
where equality $(a)$ follows from \eqref{eq:lb}, and $(b)$ follows from $g(\beta)<1$ when $0<\beta\le\beta^\ast$; see Lemma~\ref{lm:ele} (vi).
Finally, \eqref{eq:sw} follows immediately from this bound and \eqref{eq:mh1}--\eqref{eq:mh2}.
\end{proof}

\begin{remark}
One might wonder why we use the conditional expectation and variance to prove Proposition~\ref{prop:con} instead of using the unconditional ones. By \eqref{eq:lb} in Appendix~\ref{ap:um},
$$
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big]
= (1+o(1)) n^{g(\beta)}  
$$
for all $\beta$. This gives us the same estimate as the conditional expectation in \eqref{eq:cg}. However, the unconditional variance can be much larger than the conditional one in \eqref{eq:sw}. Recall from \eqref{eq:mh1} that we use $\sum_{i=1}^n E[ \exp\big(2k\beta (A^r_i-A^0_i) \big)]$ to bound the variance. 
By Corollary~\ref{cr:yy} in Appendix~\ref{ap:um},
for the unconditional case this sum is of order $\Theta(n^{g(2\beta)})$ while for the conditional case it is of order $O(n^{\tilde{g}(2\beta)})$. One can show that if $\beta\le\beta^\ast$, then we always have $g(2\beta)>\tilde{g}(2\beta)$. Therefore the order of the unconditional variance is much larger than the conditonal one.
\end{remark}

\begin{theorem}  \label{thm:dist}
	Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\alpha>b\beta$ and $0<\beta\le \beta^\ast$. Let 
	$
	(X,G,\sigma) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta, 1) .
	$
	Then
	$$
	P_{\SIBM}(\dist(\sigma, \Gamma) = \Theta(n^{g(\beta)}) ) = 1-o(1) .
	$$
\end{theorem}
\begin{proof}
	We prove the following equivalent form:
	$$
	P_{\SIBM}(\dist(\sigma, X) = \Theta(n^{g(\beta)}) ~\big|~ \dist(\sigma, X) \le n/k) = 1-o(1) .
	$$
	Corollary~\ref{cr:1} together with Proposition~\ref{prop:43} implies that there is an integer $z>0$ and a set $\cG_{\good}$ such that
	
	\noindent (i)
	$P(G\in\cG_{\good}) \ge 1- O(n^{-4})$.
	
	\noindent (ii) For every $G\in\cG_{\good}$, 
	\begin{equation}  \label{eq:hs}
	P_{\sigma|G} \big( \sigma \in  \Lambda(G, z)
	\text{~and~} \dist(\sigma, X) \le n^\theta ~\big|~ \dist(\sigma, X) \le n/k \big) 
	=1- O(n^{-4}) ,
	\end{equation}
	where we can choose $\theta$ to be any constant in the open interval $(\tilde{g}(\beta), 1)$.
	
	We now define 
	$\phi_i := \mathbbm{1}[\sigma_i \neq X_i]$ for $i\in[n]$, and
	we want to estimate $\dist(\sigma,X)=\sum_{i=1}^n \phi_i$. 
	Given a fixed graph $G$ and a random sample $\sigma$, we will define {\bf Bernoulli} random variables $\underline{S}_1,\dots, \underline{S}_n$ and $\overline{S}_1,\dots,\overline{S}_n$ satisfying the following conditions:
	\begin{enumerate}
		\item $\underline{S}_1,\dots, \underline{S}_n$ are conditionally independent given the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$. $\overline{S}_1,\dots,\overline{S}_n$ are also conditionally independent given the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$.
		\item $\underline{S}_i\le \phi_i\le \overline{S}_i$ for all $i\in[n]$.
		\item Conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$, $P(\underline{S}_i=1)=\underline{C}
		\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)$ and $P(\overline{S}_i=1)=\overline{C}
		\sum_{r=1}^{k-1}\exp\big(k\beta (B_i-A_i) \big)$ for all $i\in[n]$. The constants $\bar{C}$ and $\underline{C}$ are defined in Equation \eqref{eq:qke}.
	\end{enumerate}
	We will postpone the definition of random variables $\underline{S}_1,\dots, \underline{S}_n$ and $\overline{S}_1,\dots,\overline{S}_n$ to the end of this proof. For now let us assume the existence of these random variables and prove the theorem.
	By property 3), conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$, we have
	\begin{align*}
	& E[\underline{S}_1 + \dots + \underline{S}_n] = \underline{C}
	\sum_{r=1}^{k-1}\sum_{i=1}^n \exp\big(2\beta (A^r_i-A^0_i) \big) , \quad
	\Var(\underline{S}_1 + \dots + \underline{S}_n) \le \underline{C}
	\sum_{r=1}^{k-1}\sum_{i=1}^n \exp\big(2\beta (A^r_i-A^0_i) \big)
 	, \\
	& E[\overline{S}_1 + \dots + \overline{S}_n] = \overline{C}
	\sum_{r=1}^{k-1}\sum_{i=1}^n \exp\big(2\beta (A^r_i-A^0_i) \big) , \quad
	\Var(\overline{S}_1 + \dots + \overline{S}_n) \le \overline{C}
	\sum_{r=1}^{k-1}\sum_{i=1}^n \exp\big(2\beta (A^r_i-A^0_i) \big) ,
	\end{align*}
	where we use the fact that the variance of a Bernoulli random variable is always upper bounded by its expectation.
	By Proposition~\ref{prop:con}, for all $G\in\cG_{\good}\cap\cG_{\con}$, we have 
	\begin{align*}
	& E[\underline{S}_1 + \dots + \underline{S}_n] = \Theta(n^{g(\beta)}) , \quad
	\Var(\underline{S}_1 + \dots + \underline{S}_n) = O(n^{g(\beta)}) , \\
	& E[\overline{S}_1 + \dots + \overline{S}_n] = \Theta(n^{g(\beta)}) , \quad
	\Var(\overline{S}_1 + \dots + \overline{S}_n) = O(n^{g(\beta)}) 
	\end{align*}
	conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$. Since $g(\beta)\ge 0$ for all $0<\beta\le \beta^\ast$, by Chebyshev's inequality we know that both $\underline{S}_1 + \dots + \underline{S}_n=\Theta(n^{g(\beta)})$ and $\overline{S}_1 + \dots + \overline{S}_n=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$.
	Since $\underline{S}_i\le \phi_i\le \overline{S}_i$ for all $i\in[n]$, we also have $\dist(\sigma,X)=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$. Combining this with \eqref{eq:hs}, we obtain that for every $G\in\cG_{\good}\cap\cG_{\con}$, $\dist(\sigma,X)=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ conditioning on $\{\dist(\sigma, X) \le n/k\}$. Finally, the theorem follows from $P(G\in\cG_{\good}\cap\cG_{\con})=1-o(1)$.
	
	Now we are left to define {\bf Bernoulli} random variables $\underline{S}_1,\dots, \underline{S}_n$ and $\overline{S}_1,\dots,\overline{S}_n$.
	First we define some auxiliary random variables.
	For $i\in[n]$, let 
	$\underline{R}_i=\underline{R}_i(\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n,G)$ be a Bernoulli random variable with parameter 
	$$
	\min \Big( \frac{\underline{C}
		\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)}{P_{\sigma|G}(\phi_i=1|\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n)} , 1 \Big) ,
	$$
	and let 
	$\overline{R}_i=\overline{R}_i(\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n,G)$ be a Bernoulli random variable with parameter 
	$$
	\min \Big(\frac{1- \overline{C}
		\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)}{P_{\sigma|G}(\phi_i=0|\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n)} , 1 \Big) .
	$$
	Furthermore, let both $\underline{R}_i$ and $\overline{R}_i$ be independent of $\phi_i$.
	For $i\in[n]$, define
	$$
	\underline{S}_i=\phi_i \underline{R}_i \quad
	\text{and} \quad
	\overline{S}_i = 1- (1-\phi_i) \overline{R}_i .
	$$
	Property 2) above directly follows from this definition.
	Now condition on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$.
	By \eqref{eq:soon} we have $\underline{R}_i =\frac{\underline{C}
		\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)}{P_{\sigma|G}(\phi_i=1|\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n)} $, one can see that no matter what values $\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n$ take, the conditional probability of $\underline{S}_i=1$ given these variables is always $\underline{C}
	\sum_{r=1}^{k-1}\exp\big(k\beta (A^r_i-A^0_i) \big)$. Therefore, conditioning on the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$, $\underline{S}_i$ is independent of $\phi_1,\dots,\phi_{i-1},\phi_{i+1},\dots,\phi_n$ and it only depends on $\phi_i$. As a consequence, $\underline{S}_1,\dots, \underline{S}_n$ are conditionally independent given the event $\{\sigma\in \Lambda(G,z) ,\dist(\sigma, X) \le n^{\theta}\}$. Thus we have verified the three properties above for $\underline{S}_1,\dots, \underline{S}_n$. The arguments for $\overline{S}_1,\dots, \overline{S}_n$ are the same.
\end{proof}
\section{Exact recovery is not solvable when $\lfloor \frac{m+1}{2} \rfloor \beta < \beta^\ast$}\label{sect:converse}

Given the ground truth $X$, the graph $G$ and a vertex $i\in[n]$, define the neighbors of $i$ in $G$ as 
\begin{equation} \label{eq:ngbi}
\cN_i(G) := \{j\in[n]\setminus\{i\}:
\{i,j\}\in E(G) \} .
\end{equation}
Given a sample $\sigma\in W^n$ and a graph $G$,  we define the set of ``bad" neighbors of the vertex $i$ in $G$ as
$$
\Omega_i(\sigma,G):=\{j\in \cN_i(G): 
\sigma_j \neq X_j \} .
$$
Given a vertex $i\in[n]$, a graph $G$ and an integer $z>0$, we also define
$$
\Lambda_i(G, z):=\{ \sigma\in W^n: |\Omega_i(\sigma,G)| < z \} ,
$$
i.e., $\Lambda_i(G, z)$ consists of all the samples for which the number of ``bad" neighbors of the vertex $i$ in $G$ is less than $z$.

\begin{lemma} \label{lm:us}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$, $\alpha>b\beta$ and $\beta\le \beta^\ast$.
Given any $r>0$, there exists an integer $z>0$ such that for large enough $n$ and every $i\in[n]$, 
\begin{equation} \label{eq:zr}
P_{\SIBM} (\sigma\in \Lambda_i(G, z)
| \dist(\sigma, X) \le n/k)
> 1 - n^{-r} .
\end{equation}
\end{lemma}
\begin{proof}
The event $\{\sigma\notin \Lambda_i(G, z) \}$ can be written as
\begin{equation} \label{eq:lei}
\{\sigma\notin \Lambda_i(G, z) \}
=\{|\Omega_i(\sigma,G)| \ge z \}
=\bigcup_{\tilde{\cI}\subseteq[n]\setminus\{i\}, |\tilde{\cI}| = z} \{\tilde{\cI} \subseteq \Omega_i(\sigma,G)\} .
\end{equation}
Given a subset $\tilde{\cI}\subseteq[n]\setminus\{i\}$,
the event $\{\tilde{\cI} \subseteq \Omega_i(\sigma,G) \}$ is the intersection of two events $\{\sigma_j \neq X_j \text{~for all~} j\in \tilde{\cI}\}$ and $\{i\in\cN_j(G) \text{~for all~} j\in \tilde{\cI}\}$.

We start with the analysis of the first event.
In Proposition~\ref{prop:43}, we have shown that
for any $\delta>0$ and any $r'>0$, there exists $n_0(\delta, r')$ such that for all even integers $n>n_0(\delta, r')$,
$$ 
P_{\SIBM} \Big(\dist(\sigma,\pm X) \le n^{g(\beta)+\delta}
 \Big) \ge 1- n^{-r'} .
$$
By Lemma~\ref{lm:ele} (vi), we know that $g(\beta)<1$ for all $0<\beta\le\beta^\ast$, so we can always choose a $\delta>0$ such that $g(\beta)+\delta<1$.
Let $\theta:=g(\beta)+\delta<1$. Then for large enough $n$,
$$
P_{\SIBM} \Big(\dist(\sigma, \Gamma) \le n^{\theta} \Big) \ge 1- n^{-r'} .
$$
This in particular implies that
$$
P_{\SIBM} \Big(\dist(\sigma, X) \le n^{\theta} | \dist(\sigma, X) \le n/k
\Big) \ge 1- n^{-r'} .
$$
Moreover, Lemma~\ref{lm:bq} in Appendix~\ref{ap:6} (see inequality \eqref{eq:l2}) tells us that
$$
 P_{\SIBM}(\sigma_j \neq X_j \text{~for all~}  j\in\tilde{\cI}
~\big| \dist(\sigma,X) \le n^\theta) \le \Big(\frac{n^\theta}{n/k-|\tilde{\cI}|}
\Big)^{|\tilde{\cI}|}
\quad \text{for all~} \tilde{\cI}\subseteq [n] .
$$
Therefore, for any subset $\tilde{\cI}\subseteq[n]\setminus\{i\}$ with size $|\tilde{\cI}|=z$, we have
$$
P_{\SIBM}(\sigma_j \neq X_j \text{~for all~} j\in \tilde{\cI}
| \dist(\sigma, X) \le n/k )
\le O(n^{-(1-\theta) z}) + O(n^{-r'}) . 
$$
Taking $r'>(1-\theta) z$ gives us
\begin{equation} \label{eq:gr}
P_{\SIBM}(\sigma_j \neq X_j \text{~for all~} j\in \tilde{\cI}
| \dist(\sigma, X) \le n/k )
\le O(n^{-(1-\theta) z}) . 
\end{equation}

Given $r>0$, we will prove \eqref{eq:zr} for any integer $z\ge \frac{r+1}{1-\theta}$. 
Now condition on the event $\{\sigma_j \neq X_j \text{~for all~} j\in \tilde{\cI}\}$.
Taking a specific $\sigma'$ which satistifies  $\{\sigma_j \neq X_j \text{~for all~} j \in \tilde{\cI}, \dist(\sigma, X) \le n/k\}$.
Using Lemma \ref{lem:post_independent}, we can show that
\begin{align*}
P(i\in\cN_j(G) \text{~for all~} j\in \tilde{\cI} | \sigma_j \neq X_j \text{~for all~} j \in \tilde{\cI}, \dist(\sigma, X) \le n/k) &\leq P(i\in\cN_j(G) \text{~for all~} j\in \tilde{\cI} | \sigma =\sigma')\\
&=\prod_{j\in \tilde{\cI}}\Pr((i,j)\in E(G) | \sigma = \sigma') =  O\Big( \frac{\log^z(n)}{n^z} \Big) .
\end{align*}
Combining this with \eqref{eq:gr}, we have
$$
P_{\SIBM} (\tilde{\cI} \subseteq \Omega_i(\sigma,G))
= O (n^{-(2-\theta)z}\log^z(n)) .
$$
Finally, combining this with \eqref{eq:lei} and the union bound, we have
\begin{align*}
P_{\SIBM} \big(\sigma\notin \Lambda_i(G, z)
| \dist(\sigma, X) \le n/k \big)
& \le n^z O (n^{-(2-\theta)z}\log^z(n))
= O (n^{-(1-\theta)z}\log^z(n))  \\
& \le O (n^{-r-1}\log^z(n)) <n^{-r}
\end{align*}
for large enough $n$. This completes the proof of the lemma.
\end{proof}
We further define 
\begin{align*}
\Lambda(G, z) :=
\bigcap_{i=1}^n
\Lambda_i(G, z) .
\end{align*}
The following corollary follows immediately from Lemma~\ref{lm:us} and the union bound.
\begin{corollary} \label{cr:1}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$.
Given any $r>0$, there exists an integer $z>0$ such that for large enough $n$, 
$$
P_{\SIBM} (\sigma\in \Lambda(G, z)
| \dist(\sigma, X) \le n/k)
> 1 - n^{-2r} .
$$
Equivalently, for any $r>0$, there is an integer $z>0$ and a set $\cG_{\good}$ such that

\noindent (i)
$P(G\in\cG_{\good}) \ge 1- O(n^{-r})$.

\noindent (ii) For every $G\in\cG_{\good}$,
$$
P_{\sigma|G} \big( \sigma \in  \Lambda(G, z)  \big| \dist(\sigma, X) \le n/2 \big) =1- O(n^{-r}).
$$
\end{corollary}
\begin{proof}
We only explain why the first statement implies the second one.
Define a set 
$$
\cG_{\bad}:= \big\{ G: P_{\sigma|G} \big( \sigma \notin  \Lambda(G, z)  \big| \dist(\sigma, X) \le n/k \big)
>n^{-r} \big\} .
$$
Then by the union bound and Lemma~\ref{lm:us}, $P(G\in\cG_{\bad})\le n^{-r}$. Therefore, the second statement follows by taking $\cG_{\good}$ to be the complement of $\cG_{\bad}$.
\end{proof}

We define subsets of $\cN_i(G)$ with the same labeling and the opposite labeling as
$$
\cN_{i,r}(G):=\{j\in \cN_i(G): X_j=\omega^r \cdot X_i \}
$$
respectively.
Recall that in \eqref{eq:defAB}, we defined $A^r_i=A^r_i(G):=|\cN_{i,r}(G)|$.
By definition, $\sigma = X^{(\sim\cI)}\in  \Lambda_i(G,z)$ if and only if $|\cI\cap \cN_i(G)| < z$.


\begin{lemma} \label{lm:et}
Let $0<\theta<1$ be some constant and $1\leq r \leq k-1$. Then for large enough $n$ we have
\begin{align*}
\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i - 2z) \Big) & \le 
\frac{P_{\sigma|G}(\sigma_i= \omega^r \cdot X_i, \sigma\in \Lambda_i(G,z) ,\dist(\sigma, X) \le n^{\theta} ) } 
{ P_{\sigma|G}(\sigma_i=X_i, \sigma\in \Lambda_i(G,z) ,\dist(\sigma, X) \le n^{\theta} ) } \\
& \le \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i + 2z) \Big)
\end{align*}
\end{lemma}

\begin{proof}
We only need to show that for every $\cI\in [n]\setminus\{i\},v$ with size $|\cI|\le n^\theta$ such that $X^{(\sim\cI,v)}\in \Lambda_i(G,z)$,
and $v'$ be an extension of $v$ such that $v'=[v, \omega^r]$:
\begin{equation} \label{eq:qk}
\begin{aligned}
\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i - kz) \Big) & \le 
\frac{P_{\sigma|G}(\sigma= X^{(\sim(\cI\cup\{i\}), v')} ) } { P_{\sigma|G}(\sigma= X^{(\sim\cI,v)} ) } \\
& \le \exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i + kz) \Big) .
\end{aligned}
\end{equation}
Then using 
$$
\frac{P_{\sigma|G}(\sigma_i= \omega^r \cdot X_i, \sigma\in \Lambda_i(G,z) ,\dist(\sigma, X) \le n^{\theta} ) } 
{ P_{\sigma|G}(\sigma_i=X_i, \sigma\in \Lambda_i(G,z) ,\dist(\sigma, X) \le n^{\theta} ) } 
= \frac{\sum_{\cI, v} P_{\sigma|G}(\sigma= X^{(\sim(\cI\cup\{i\}), v')} ) } {\sum_{\cI, v} P_{\sigma|G}(\sigma= X^{(\sim\cI,v)} ) } 
$$
we can get the conclusion.

Define $v^{-r} = |\{i | v_i = \omega^{k-r}\}|$ and
$$
A'^r_i :=A^r_i-|\cI\cap \cN_{i,r}(G)| + \sum_{s=0, s\neq r}^{k-1}|\cI\cap \cN_{i,s}(G) \cap v^{-(s-r)}| .
$$
Since $X^{(\sim\cI, v)}\in \Lambda_i(G,z)$, we have $|\cI\cap \cN_{i,r}(G)| \le z-1$. Therefore,
\begin{equation} \label{eq:oo}
A^r_i-A^0_i-k(z-1) \le A'^r_i-A'^0_i\le A^r_i-A^0_i+k(z-1) .
\end{equation}
By \eqref{eq:isingma}, we have
\begin{align*}
& \frac{P_{\sigma|G}(\sigma= X^{(\sim(\cI\cup\{i\}))} ) } { P_{\sigma|G}(\sigma= X^{(\sim\cI)} ) } \\
= & \exp\Big(k(\beta+\frac{\alpha\log n }{n})(A'^r_i-A'^0_i)
-2\frac{(k-1)\alpha\log(n)}{n}
\Big)
 \\
= & \exp\Big(k(\beta+\frac{\alpha\log n }{n})(A'^r_i-A'^0_i)+o(1) \Big)
\end{align*}
Taking \eqref{eq:oo} into this equation gives us \eqref{eq:qk} and completes the proof.
\end{proof}
\begin{remark}
	From the proof we can see that $\Gamma_i$ can be replaced with $\Gamma$. Since $A_i^r - A_i^0 < 0$ with probability one when
	$n$ is large, we can get there exists $\underline{C}, \bar{C}$ such that
	\begin{equation} \label{eq:qke}
	\underline{C}\sum_{r=1}^{k-1}\exp\Big(k\beta(A^r_i-A^0_i) \Big)  \le 
	P_{\sigma|G}(\sigma \neq X_i | \sigma \in \Lambda(G, z),
	\dist(\sigma, X) \leq n^{\theta})
	\le \bar{C} \sum_{r=1}^{k-1}\exp\Big(k\beta (A^r_i-A^0_i ) \Big) 
	\end{equation}
	where $\underline{C}$ and $\overline{C}$ are constants that are independent of $n$.
	Under the condition that $\bar{\sigma} \in \Lambda(G, z)$ and
	$\dist(\bar{\sigma}, X) \leq n^{\theta}$,
	we can also reformulate Equation \eqref{eq:qk} as follows:
	\begin{equation}\label{eq:soon}
	\underline{C} \sum_{r=1}^{k-1} \exp(k\beta(A^r_i - A^0_i)) \leq P_{\sigma | G}(\sigma_i \neq X_i | \sigma_j = \bar{\sigma}_j \text{ for all } j\neq i ) \leq
	\bar{C} \sum_{r=1}^{k-1} \exp(k\beta(A^r_i - A^0_i))
	\end{equation}
\end{remark}

\appendix
\section{Auxiliary lemmas used in Section~\ref{sect:converse}}\label{ap:6}

\begin{lemma} \label{lm:bq}
For $0<\theta<1$,
\begin{align}
& P_{\SIBM}(\sigma_i \neq X_i
\big| \dist(\sigma,X) \le n^\theta) \le k n^{\theta-1}
\quad \text{for all~} i\in[n] , \label{eq:l1}\\
& P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI}
~\big| \dist(\sigma,X) \le n^\theta) \le \Big(\frac{n^\theta}{n/k - |\tilde{\cI}|}
\Big)^{|\tilde{\cI}|}
\quad \text{for all~} \tilde{\cI}\subseteq [n] .   \label{eq:l2}
\end{align}
\end{lemma}
\begin{proof}
Define $\dist_j(\sigma,X):=|\{i\in[n]:X_i=\omega^j, \sigma_i \neq X_i\}|$ for $j \in \{0, \dots, k-1\}$. Clearly, $\dist(\sigma,X)=\sum_{j=0}^{k-1} \dist_j(\sigma,X)$.

Inequality \eqref{eq:l1} follows immediately from the following equality:
$$
P_{\SIBM}(\sigma_i \neq X_i |
\dist_j(\sigma,X)=u_j,
j=0,\dots,k-1) 
= k u_r / n \textrm{ where } X_i = \omega^r
$$
Without loss of generality,
we only prove the case of $X_i=1 (r=0)$, and
we need the following definition for the proof of this equality:
For $\cI\subseteq[n]$, define $\cI_j:=\{i\in\cI:X_i=\omega^j\}$ Then
\begin{align*}
& P_{\SIBM}(\sigma_i \neq X_i |
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)  \\
= & \frac{P_{\SIBM}(\sigma_i \neq X_i ,
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)}
{P_{\SIBM}(
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)} \\
= & \frac{\sum_{\cI:i\in\cI_0,|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }
{\sum_{\cI: |\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) } \\
\overset{(a)}{=} & \frac{\binom{n/k-1}{u_0 -1}}
{\binom{n/k}{u_0} }
= ku_0/n ,
\end{align*}
where equality (a) follows from Lemma~\ref{lm:cc} below.

Similarly, inequality \eqref{eq:l2} follows from the following inequality:
For $u_j\geq |\tilde{\cI}_j|, j=0, \dots, k-1$,
\begin{align*}
& P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI} ~ \big|
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)  \\
= & \frac{P_{\SIBM}(\sigma_i \neq X_i \text{~for all~}  i\in\tilde{\cI} ,
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)}{P_{\SIBM}(
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)} \\
= & \frac{\sum_{\cI:\tilde{\cI}_j\subseteq\cI_j,
|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }
{\sum_{\cI:|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }  \\
\overset{(a)}{=} & \prod_{j=0}^{k-1} \frac{\binom{n/k-|\tilde{\cI}_j|}{u_j -|\tilde{\cI}_j|} }
{\binom{n/k}{u_j} }
< \prod_{j=0}^{k-1}\Big(\frac{u_j}{n/k- |\tilde{\cI}_j|} \Big)^{|\tilde{\cI}_j|}
\\
< & \Big(\frac{\sum_{j=0}^{k-1} u_j}{n/k- |\tilde{\cI}|} \Big)^{|\tilde{\cI}|}  ,
\end{align*}
where equality (a) again follows from Lemma~\ref{lm:cc} below.
\end{proof}







\begin{lemma} \label{lm:cc}
Let $\cI,\cI'\subseteq[n]\setminus\{i\}$ be two subsets such that $|\cI_j|=|\cI_j'|$ for $j=0,\dots,k-1$. 
Then $P_{\SIBM}(\sigma=X^{(\sim\cI)}) = P_{\SIBM}(\sigma=X^{(\sim\cI')})$.
\end{lemma}

\begin{lemma}\label{lem:post_independent}
Let $p_{ij}=\Pr(\{\{i,j\} \in E(G) \})$ be the prior probability, which equals $\frac{a\log n}{n}$ or $\frac{b\log n}{n}$ depending on $X$.
The event $\{\{i,j\} \in E(G) \}$ are independent given $\sigma$ and the posterior probability is
\begin{equation}
\Pr(\{\{i,j\} \in E(G) \} | \sigma) = \frac{c p_{ij} }{1-p_{ij} + cp_{ij}} \text{ where  } c= \exp\Big((\beta + \frac{\alpha \log n}{n} ) I(\bar{\sigma}_i, \bar{\sigma}_j) \Big)
\end{equation}
\end{lemma}
\begin{proof}
Let $Y_{ij}$ be a Bernoulli random variable with $\Pr(Y_{ij} = 1) = \Pr(\{\{i,j\} \in E(G)\}) = p_{ij}$. $Y_{ij}$ represents whether there is an edge between node $i$ and $j$.
The prior distribution for $Y:=\{Y_{ij}\}$ can be written as:
$$
\Pr(Y) = \prod_{i,j} p_{ij}^{y_{ij}} (1-p_{ij})^{1-y_{ij}}
$$ 
Using Equation \eqref{eq:isingma}, the posterior probability for $Y| \sigma$ can be written as
$$
\Pr(Y|\sigma) = C\prod_{ij} \exp((\beta + \frac{\alpha \log n}{n})I(\bar{\sigma_i}, \bar{\sigma_j} )y_{ij}) \Pr(Y)
$$
where $C$ is a constant irrelevant with $Y$.
We can see from the joint distribution of $Y|\sigma$ that $Y_{ij} | \sigma$ are independent and each marginal distribution has the following form:
$$
\Pr(Y_{ij} | \sigma) = C_{ij} \exp((\beta + \frac{\alpha \log n}{n})I(\bar{\sigma_i}, \bar{\sigma_j} )y_{ij}) p_{ij}^{y_{ij}} (1-p_{ij})^{1-y_{ij}}
$$
Using the normalization condition for $Y_{ij} | \sigma $, we can compute $C_{ij} = \frac{1}{1-p_{ij} + p_{ij}c}$. From $\Pr(\{\{i,j\} \in E(G) \} | \sigma) =\Pr(Y_{ij}=1|\sigma)$ the proof is complete.
\end{proof}


\begin{lemma} \label{lm:bmd}
Let $Y\sim \Binom(n + o(n), a\log(n)/n)$. Then for $r>8$ and large enough $n$, we have
$$
P(Y\ge r a \log(n)) < n^{-r} .
$$
\end{lemma}
\section{Auxiliary propositions used in Section~\ref{sect:theta}}\label{ap:um}

We first prove a tight estimate of $P(A^1_i-A^0_i = t\log(n))$ for $t=\Theta(1)$. Note that \eqref{eq:upba} gives an upper bound on $P(A^1_i-A^0_i \ge t\log(n))$ for $t\in [\frac{1}{k}(b-a), 0]$, so it is also an upper bound of $P(A^1_i-A^0 = t\log(n))$. Below we prove that the upper bound in \eqref{eq:upba} is in fact tight up to a $\Theta(1/ \sqrt{\log(n)})$ factor for all $t=\Theta(1)$ such that $t\log(n)$ is an integer.

We use $f(n)=\Theta(g(n))$ and $f(n)\asymp g(n)$ interchangeably if there is a constant $C>0$ such that $C^{-1}g(n)\le f(n)\le C g(n)$ for large enough $n$.

\begin{proposition}  \label{prop:99}
For any $t$ such that $t\log(n)$ is an integer and $|t|<100a$,
\begin{equation} \label{eq:ly}
\begin{aligned}
& P(A^1_i-A^0_i = t\log(n))  \\
\asymp & \frac{1} {\sqrt{\log(n)}} \exp\Big(\log(n)
\Big(\sqrt{t^2+\frac{4ab}{k}} -t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big) -\frac{a+b}{k}  \Big)\Big) .
\end{aligned}
\end{equation}
\end{proposition}


\begin{proof}
Since
$$
P(A^1_i-A^0_i = t\log(n))
= \sum_{s\log(n)=0}^{n/k}
P(A^1_i = s\log(n)) P(A^0_i=(s-t)\log(n)) ,
$$
we first calculate tight estimates of $P(A^1_i = s\log(n))$ and $P(A^0_i=(s-t)\log(n))$. (The summation $\sum_{s\log(n)=0}^{n/k}$ in the above equation means that the quantity $s\log(n)$ ranges over all integer values from $0$ to $n/k$.)
By Lemma~\ref{lm:bmd} in Appendix~\ref{ap:6}, we only need to focus on the regime where both $|s|$ and $|t|$ are bounded from above by some (large) constants, e.g., $100a$.
Therefore,
\begin{align*}
& P(A^1_i = s\log(n)) \\
= & \binom{n/k}{s\log(n)}
\Big( \frac{b\log(n)}{n} \Big)^{s\log(n)}
\Big( 1- \frac{b\log(n)}{n} \Big)^{n/k-s\log(n)}  \\
= & (1+o(1))
\frac{(n/k)^{s\log(n)}}{(s\log(n))!} \Big( \frac{b\log(n)}{n} \Big)^{s\log(n)}
\exp \Big(-\frac{b\log(n)}{k} \Big)  \\
= & (1+o(1))
\frac{1} {(s\log(n))!} \Big( \frac{b\log(n)}{k} \Big)^{s\log(n)}
\exp \Big(-\frac{b\log(n)}{k} \Big)  \\
\overset{(a)}{=} & (1+o(1))
\frac{1} {\sqrt{2\pi s\log(n)}}
\Big(\frac{e}{s\log(n)} \Big)^{s\log(n)}
\Big( \frac{b\log(n)}{k} \Big)^{s\log(n)}
\exp \Big(-\frac{b\log(n)}{k} \Big)  \\
= & (1+o(1))
\frac{1} {\sqrt{2\pi s\log(n)}}
\Big( \frac{ e b }{ks} \Big)^{s\log(n)}
\exp \Big(-\frac{b\log(n)}{k} \Big) \\
= & (1+o(1))
\frac{1} {\sqrt{2\pi s\log(n)}}
\exp\Big( \log(n) \Big( 
s+ s\log(b)-s\log(k)-s\log(s)-\frac{b}{k}
\Big)\Big) ,
\end{align*}
where $(a)$ follows from Stirling's formula.
Similarly, when $s > t$,
\begin{align*}
& P(A^0_i=(s-t)\log(n)) \\
= & (1+o(1))
\frac{1} {\sqrt{2\pi (s-t)\log(n)}}
\exp\Big( \log(n) \Big( 
 (s-t)(1+\log(a)-\log(k)-\log(s-t))-\frac{a}{k}
\Big)\Big)
\end{align*}
Define a function
$$
h_t(s):=(2s-t)(1-\log(k))
+s\log\frac{ab}{s(s-t)}
+t\log\frac{s-t}{a} -\frac{a+b}{k} .
$$
Then for $s>\max(0,t)$,
$$
P(B_i = s\log(n)) P(A_i=(s-t)\log(n))
= (1+o(1)) \frac{1} {2\pi \log(n)\sqrt{s (s-t)}}
\exp(h_t(s) \log(n)) .
$$
Therefore, for $t$ such that $t\log(n)$ is an integer, we have
\begin{equation} \label{eq:gj}
\begin{aligned}
& P(A^1_i-A^0_i = t\log(n))  \\
= & (1+o(1)) \sum_{s\log(n)=\max(0,t\log(n))}^{n/k}
\frac{1} {2\pi \log(n)\sqrt{s (s-t)}}
\exp(h_t(s) \log(n)) .
\end{aligned}
\end{equation}
In order to estimate this sum, we need to analyze the function $h_t(s)$. Its first and second derivatives are
$h_t'(s)=\log\frac{ab}{k^2s(s-t)}$
and $h_t''(s)=-\frac{1}{s}-\frac{1}{s-t}<0$, so $h_t(s)$ is a concave function and takes maximum at $s^\ast$ such that $h_t'(s^\ast)=0$.
Simple calculations show that
$s^\ast=(t+\sqrt{t^2+4ab/k^2})/2>\max(0,t)$ and
$$
h_t(s^\ast)
= \sqrt{t^2+4ab/k^2} +t\big(\log(\sqrt{t^2+4ab/k^2}-t)-\log(2a) \big) -\frac{a+b}{k} .
$$
By Lemma~\ref{lm:bmd} in Appendix~\ref{ap:6}, both $|s|$ and $|t|$ are upper bounded by some (large) constants
with probability $1-o(n^{-10})$. Therefore, the sum on the right-hand side of \eqref{eq:gj} is concentrated around a small neighborhood of $s^\ast$. In this neighborhood, we have
$$
\frac{1} {2\pi \log(n)\sqrt{s (s-t)}} = \Theta(\frac{1} {\log(n)}) .
$$
Therefore, in order to prove this proposition, we only need to show that
\begin{equation} \label{eq:jh}
\sum \exp(h_t(s) \log(n))
= \Theta\Big(\sqrt{\log(n)} \exp(h_t(s^\ast) \log(n))\Big) ,
\end{equation}
where the summation is taken over this small neighborhood.
We will show that $\exp(h_t(s) \log(n))$ varies by a constant factor within a window of length $\Theta(1/\sqrt{\log(n)})$ around $s^\ast$, and
then drops off geometrically fast beyond that window. First observe that $h_t(s)\approx h_t(s^\ast) - h_t''(s^\ast) (s-s^\ast)^2$ in the neighborhood of $s^\ast$, so when $|s-s^\ast|=\Theta(1/\sqrt{\log(n)})$, we have $h_t(s^\ast) \log(n) - h_t(s) \log(n) = \Theta(1)$.
Also note that when $s$ is in the range $(s^\ast-\Theta(1/\sqrt{\log(n)}) , s^\ast+\Theta(1/\sqrt{\log(n)}))$, the quantity $s\log(n)$ takes $\Theta(\sqrt{\log(n)})$ integer values.
Now pick some constant $c>0$. By the above analysis we have
$$
\sum_{s^\ast\log(n) - c\sqrt{\log(n)} \le s\log(n) \le s^\ast\log(n) + c\sqrt{\log(n)}}
\exp(h_t(s) \log(n))
= \Theta\Big(\sqrt{\log(n)} \exp(h_t(s^\ast) \log(n))\Big) .
$$
For $s>s^\ast+c/\sqrt{\log(n)}$, we use the fact that concave functions are always bounded from above by its tangent lines. Therefore,
\begin{align*}
h_t(s) & \le h_t(s^\ast+c/\sqrt{\log(n)})
+ h_t'(s^\ast+c/\sqrt{\log(n)}) 
(s- s^\ast - c/\sqrt{\log(n)}) \\
& \le h_t(s^\ast) - \frac{c'}{\sqrt{\log(n)}}
(s- s^\ast - c/\sqrt{\log(n)}) ,
\end{align*}
where we use the fact that $h_t'(s^\ast+c/\sqrt{\log(n)}) = \Theta(1/\sqrt{\log(n)})$, and $c'$ is another constant that depends on $c$.
Therefore,
\begin{align*}
& \sum_{s\log(n) > s^\ast\log(n) + c\sqrt{\log(n)}}
\exp(h_t(s) \log(n))  \\
\le & \sum_{s\log(n) > s^\ast\log(n) + c\sqrt{\log(n)}}
\exp \Big( h_t(s^\ast) \log(n) 
-\frac{c'}{\sqrt{\log(n)}}
(s\log(n)- s^\ast \log(n)- c \sqrt{\log(n)}) \Big)   \\
= & \exp(h_t(s^\ast) \log(n))
\sum_{j>0} \exp \Big( 
-\frac{c'}{\sqrt{\log(n)}} j \Big) \\
\le & \exp(h_t(s^\ast) \log(n))
\frac{1}{1-\exp(-c'/ \sqrt{\log(n)})} \\
= & O\Big(\sqrt{\log(n)} \exp(h_t(s^\ast) \log(n))\Big) .
\end{align*}
The sum over $s\log(n) < s^\ast\log(n) - c\sqrt{\log(n)}$ can be bounded in the same way. Thus we have shown \eqref{eq:jh}, and this completes the proof of the proposition.
\end{proof}

Let $\cG_1:=\{G:A^r_i-A^0_i<0\text{~for all~}i\in[n]\text{~and~} 1\leq r \leq k-1\}$. By \eqref{eq:tD}, we have $P(G\in\cG_1)=1-o(1)$. In the proposition below, we will prove that if $0<\beta<\frac{1}{2k}\log\frac{a}{b}$,
then the conditional expectation $E \Big[ \sum_{i=1}^n  \exp\big(2\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big]$ is very close to the unconditional expectation $E \Big[ \sum_{i=1}^n  \exp\big(2\beta (A^r_i-A^0_i) \big) \Big]$. On the other hand, if $\beta\ge\frac{1}{2k}\log\frac{a}{b}$, then the conditional expectation is $O(n^{\tilde{g}(\beta)})$ while the unconditional one is $\Theta(n^{g(\beta)})$. Since $\tilde{g}(\beta)<g(\beta)$ when $\beta>\frac{1}{2k}\log\frac{a}{b}$, the conditional expectation is much smaller than the unconditional one in this case.
\begin{proposition}  \label{prop:df}
Assume that $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
If $0<\beta<\frac{1}{2k}\log\frac{a}{b}$, then
$$
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big] 
= (1+o(1)) E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big]
= (1+o(1)) n^{g(\beta)}  .
$$
If $\beta\ge\frac{1}{2k}\log\frac{a}{b}$, then
\begin{align*}
E \Big[ \sum_{i=1}^n  \exp\big(2\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big] 
= O(n^{\tilde{g}(\beta)})  .
\end{align*}
\end{proposition}
\begin{proof}
We first calculate the unconditional expectation:
By writing $A^r_i$ and $A^0_i$ as sums of independent Bernoulli random variables, we obtain that
\begin{align*}
E[e^{k\beta(A^r_i-A^0_i)}]
& =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^{k\beta} \Big)^{n/k}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-k\beta} \Big)^{n/k-1}  \\
& = 
\exp\Big(\frac{\log(n)}{k} ( a e^{-k\beta}+b e^{k\beta} -a-b )
+o(1) \Big) \\
& = (1+o(1)) n^{g(\beta)-1} .
\end{align*}
Therefore, for all $\beta$ we have
\begin{equation} \label{eq:lb}
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big]
= (1+o(1)) n^{g(\beta)}  .
\end{equation}
Now let us switch to conditional expectation. In light of \eqref{eq:lb}, for the case of $0<\beta<\frac{1}{2k}\log\frac{a}{b}$ we only need to prove that the conditional expectation is very close to the unconditional expectation.
To that end, we first reprove a weaker version of \eqref{eq:lb} using Proposition~\ref{prop:99}. This will help us estimate the difference between the conditional and unconditional expectations.

Define $D_r(G,t):=|\{i\in[n]:A^r_i-A^0_i= t\log(n)\}|$.
Similarly to \eqref{eq:fd}, we have
\begin{equation} \label{eq:s1}
\sum_{i=1}^n \exp\big(k\beta (A^r_i-A^0_i) \big) 
=  \sum_{t\log(n)=-n/k}^{n/k}
D_r(G,t) \exp\big(k\beta t \log(n) \big) .
\end{equation}
By definition, $D(G,t)=\sum_{i=1}^n \mathbbm{1}[A^r_i-A^0_i= t\log(n)]$, so
$E[D(G,t)]=n P(A^r_i-A^0_i= t\log(n))$. Therefore, by Proposition~\ref{prop:99} and the definition of function $f_{\beta}(t)$ in \eqref{eq:gt},
$$
E[D(G,t)
\exp\big(k\beta t \log(n) \big)]
\asymp \frac{1} {\sqrt{\log(n)}} \exp( f_{\beta}(t) \log(n) ) .
$$
As a consequence,
\begin{equation}  \label{eq:wf1}
\begin{aligned}
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big]     
& =  \sum_{t\log(n)=-n/k}^{n/k}
E \big[ D(G,t) \exp\big(k\beta t \log(n) \big) \big]  \\
& \asymp  \frac{1} {\sqrt{\log(n)}} \sum_{t\log(n)=-n/k}^{n/k}
 \exp( f_{\beta}(t) \log(n) ) .
\end{aligned}
\end{equation}
By the proof of Lemma~\ref{lm:tus}, $f_{\beta}(t)$ is a concave function and takes maximum at
$t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k}$. 
Similarly to the analysis of \eqref{eq:jh}, $\exp( f_{\beta}(t) \log(n))$ varies by a constant factor within a window of length $\Theta(1/\sqrt{\log(n)})$ around $t^\ast$, and
then drops off geometrically fast beyond that window. Since $t\log(n)$ takes $\Theta(\sqrt{\log(n)})$ integer values when $t$ takes values in such a window, we have
\begin{equation} \label{eq:wf2}
\sum_{t\log(n)=-n/k}^{n/k}
 \exp( f_{\beta}(t) \log(n) )
 \asymp \sqrt{\log(n)}
 \exp( f_{\beta}(t^\ast) \log(n) ) .
\end{equation}
By the proof of Lemma~\ref{lm:tus}, we have $f_{\beta}(t^\ast)=g(\beta)$. Taking this into \eqref{eq:wf1} and \eqref{eq:wf2}, we obtain that
$$
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big]
= \Theta (n^{g(\beta)}) .
$$
Now let us consider the conditional expectation. When conditioning on the event $G\in\cG_1$, we have $D_r(G,t)=0$ for all $t\ge 0$.
In this case, the range of sum in both \eqref{eq:s1} and \eqref{eq:wf1} reduces from $[-n/k,n/k]$ to
$[-n/k,0)$. By Lemma~\ref{lm:5t} below, we have $P(A^r_i-A^0_i= t\log(n)~|~G\in\cG_1)= (1+o(1))P(A^r_i-A^0_i= t\log(n))$ for $t<0$, and so
$E[D_r(G,t)|G\in\cG_1]=(1+o(1))E[D_r(G,t)]$ for $t<0$. Therefore,
$$
E\Big[ \sum_{i=1}^n \exp\big(2\beta (A^r_i-A^0_i) \big) \Big| G\in\cG_1 \Big]
= (1+o(1)) \sum_{t\log(n)=-n/k}^{-1}
E\big[ D_r(G,t) \exp\big(2\beta t \log(n) \big) \big] .
$$
From the analysis of \eqref{eq:wf1}, we know that 
\begin{align*}
& \sum_{t\log(n)=-n/k}^{n/k}
E\big[ D_r(G,t) \exp\big(k\beta t \log(n) \big) \big]  \\
= & (1+o(1)) \sum_{t\log(n)=t^\ast\log(n)-\Theta(\sqrt{\log(n)})}^{t^\ast\log(n)+\Theta(\sqrt{\log(n)})}
E\big[ D_r(G,t) \exp\big(k\beta t \log(n) \big) \big] .
\end{align*}
Therefore, if $t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k}<0$,
or equivalently $0<\beta<\frac{1}{2k}\log\frac{a}{b}$, then
$$
\sum_{t\log(n)=-n/k}^{n/k}
E\big[ D_r(G,t) \exp\big(k\beta t \log(n) \big) \big] = (1+o(1))
\sum_{t\log(n)=-n/k}^{-1}
E\big[ D_r(G,t) \exp\big(k\beta t \log(n) \big) \big] ,
$$
i.e.,
\begin{equation} \label{eq:bz}
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big] 
= (1+o(1)) E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) \Big] .
\end{equation}
On the other hand,
if $t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k}\ge 0$, or equivalently $\beta\ge\frac{1}{2k}\log\frac{a}{b}$, then
\begin{equation}  \label{eq:wq}
E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big]   
\asymp  \frac{1} {\sqrt{\log(n)}} \sum_{t\log(n)=-n/k}^{-1}
 \exp( f_{\beta}(t) \log(n) ) .
\end{equation}
Since $f_{\beta}(t)$ is concave, it is an increasing function when $t<t^\ast$.
Therefore, $f_{\beta}(t)<f_{\beta}(0)=g(\frac{1}{2k}\log\frac{a}{b})=\tilde{g}(\beta)$ for $\beta\ge\frac{1}{2k}\log\frac{a}{b}$.
Similarly to the analysis of \eqref{eq:jh} and \eqref{eq:wf1},
$\exp( f_{\beta}(t) \log(n))$ varies by a constant factor within a window of length $O(1/\sqrt{\log(n)})$ around $t=0$, and
then drops off geometrically fast beyond that window. As a consequence,
\begin{align*}
\sum_{t\log(n)=-n/k}^{-1}
 \exp( f_{\beta}(t) \log(n) )
=   O(\sqrt{\log(n)}) \exp(\tilde{g}(\beta) \log(n)) .
\end{align*}
Taking this into \eqref{eq:wq} completes the proof of the proposition.
\end{proof}

The following corollary follows immediately from Proposition~\ref{prop:df} and \eqref{eq:lb}:
\begin{corollary} \label{cr:yy}
For all $\beta>0$,
\begin{align*}
& E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big)  \Big] 
= \Theta(n^{g(\beta)}),  \\
& E \Big[ \sum_{i=1}^n  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\Big|~ G\in\cG_1 \Big] 
= O(n^{\tilde{g}(\beta)})  .
\end{align*}
\end{corollary}



\begin{remark}
From the proof of \eqref{eq:bz}, we can see that if $0<\beta<\frac{1}{2k}\log\frac{a}{b}$, then
\begin{equation} \label{eq:pl}
E \big[  \exp\big(k\beta (A^r_i-A^0_i) \big) ~\big|~ G\in\cG_1 \big] 
= (1+o(1)) E \big[  \exp\big(k\beta (A^r_i-A^0_i) \big) \big] ,
\end{equation}
i.e., we can remove the summation in \eqref{eq:bz}. We will use this in the proof of Proposition~\ref{prop:con}.
\end{remark}



\begin{lemma}  \label{lm:5t}
Let $\cG_1:=\{G:A^r_i-A^0_i<0\text{~for all~}i\in[n]\text{~and~}1\leq r \leq k-1\}$. Then $P(A^r_i-A^0_i= t\log(n)~|~G\in\cG_1)= (1+o(1))P(A^r_i-A^0_i= t\log(n))$ for all $t<0$ such that $t\log(n)$ is an integer.
\end{lemma}

\begin{proof}
Note that 
$$
P(A^r_i-A^0_i= t\log(n)~|~G\in\cG_1)
=\frac{P \big(A^r_j-A^0_j<0\text{~for all~} j\in[n]\setminus\{i\}, A^r_i-A^0_i= t\log(n) \big)}{P(G\in\cG_1)} .
$$
By \eqref{eq:tD}, we have $P(G\in\cG_1)=1-o(1)$, so
\begin{align*}
& P(A^r_i-A^0_i= t\log(n)~|~G\in\cG_1) \\
= & (1+o(1)) P \big(A^r_j-A^0_j<0\text{~for all~} j\in[n]\setminus\{i\}, A^r_i-A^0_i= t\log(n) \big) \\
= & (1+o(1)) P \big(A^r_j-A^0_j<0\text{~for all~} j\in[n]\setminus\{i\} ~\big|~ A^r_i-A^0_i= t\log(n) \big) P(A^r_i-A^0_i= t\log(n)) .
\end{align*}
Therefore, to prove the lemma we only need to show that $P \big(A^r_j-A^0_j<0\text{~for all~} j\in[n]\setminus\{i\} ~\big|~ A^r_i-A^0_i= t\log(n) \big)=1-o(1)$. 

For $j\in[n]\setminus\{i\}$, define $\xi_{ij}=\xi_{ij}(G):=\mathbbm{1}[\{i,j\}\in E(G)]$ as the indicator function of the edge $\{i,j\}$ connected in graph $G$. We also define 
$$
A'^r_j=\left\{
\begin{array}{cl}
  A^r_j-\xi_{ij}   & \mbox{if~} X_i\neq X_j \\
  A^r_j   &   \mbox{if~} X_i= X_j
\end{array}
\right.
\quad \text{and} \quad
A'^0_j=\left\{
\begin{array}{cl}
  A^0_j   & \mbox{if~} X_i\neq X_j \\
  A^0_j-\xi_{ij}   &   \mbox{if~} X_i= X_j
\end{array}
\right. .
$$
Then $A'^r_j-A'^0_j$ differs from $A^r_j-A^0_j$ by at most $1$.
Therefore, $A'^r_j-A'^0_j<-1$ implies that $A^r_j-A^0_j<0$,
and so $P \big(A^r_j-A^0_j<0\text{~for all~} j\in[n]\setminus\{i\} ~\big|~ A^r_i-A^0_i= t\log(n) \big) \ge P \big(A'^r_j-A'^0_j<-1 \text{~for all~} j\in[n]\setminus\{i\} ~\big|~ A^r_i-A^0_i= t\log(n) \big)$.
Now we only need to prove that the right-hand side is $1-o(1)$.
Also note that the two sets of random variables $\{A'^r_j,A'^0_j:j\in[n]\setminus\{i\}\}$ and $\{A^r_i,A^0_i\}$ are independent,
so $P \big(A'^r_j-A'^0_j<-1 \text{~for all~} j\in[n]\setminus\{i\} ~\big|~ A^r_i-A^0_i= t\log(n) \big)=P \big(A'^r_j-A^0_j<-1 \text{~for all~} j\in[n]\setminus\{i\}  \big)$.
By definition, we have
$A'^r_j\sim\Binom(\frac{n}{k}-\Theta(1),b\log(n)/n)$
and $A'^0_j\sim\Binom(\frac{n}{k}-\Theta(1),a\log(n)/n)$ for all $j\in[n]\setminus\{i\}$.
Then following exactly the same proof\footnote{First use Chernoff bound as we did in Proposition~\ref{prop:cher} and then use the union bound.} as that of \eqref{eq:tD}, we have
$$
P \big(A'^r_j-A'^0_j <-1 \text{~for all~} j\in[n]\setminus\{i\}  \big)
\ge 1- n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)} = 1-o(1).
$$
This completes the proof of the lemma.
\end{proof}
Using the same techniques as in Lemma \ref{lm:5t}, we have the following corollary:
\begin{corollary}\label{lem:ucBA}
Let $\cG_1:=\{G:A^r_i-A^0_i<0\text{~for all~}i\in[n] \text{ and } 1\leq r \leq k-1\}$. Suppose $i\neq j$, then $P(A^r_i-A^0_i + A^r_j - A^0_j= t\log(n)~|~G\in\cG_1)= (1+o(1))P(A^r_i-A^0_i + A^r_j - A^0_j= t\log(n))$ for all $t<0$ such that $t\log(n)$ is an integer.
\end{corollary}
\begin{lemma}\label{lem:BijG}
\begin{equation} 
E \big[  \exp\big(k\beta (A^r_i-A^0_i + A^r_j - A^0_j) \big) ~\big|~ G\in\cG_1 \big] 
= (1+o(1)) E \big[  \exp\big(k\beta (A^r_i-A^0_i + A^r_j - A^0_j) \big) \big] ,
\end{equation}
\end{lemma}
\begin{proof}
First we have
\begin{align}
E \big[  \exp\big(k\beta (A^r_i - A^0_i + A^r_j - A^0_j) \big) ~\big|~ G\in\cG_1 \big] 
&= \sum_{t=-2\frac{n}{k}}^{-2} P(A^r_i - A^0_i + A^r_j - A^0_j = t\log n | G \in \cG_1) \exp(k\beta t \log n) \notag \\
\text{ Corollary \ref{lem:ucBA} implies }&= (1+o(1))\sum_{t=-2\frac{n}{k}}^{-2} P(A^r_i - A^0_i + A^r_j - A^0_j = t\log n) \exp(k\beta t \log n)
\end{align}
On the other hand, we suppose $X_i \neq X_j$ and we decompose $A^r_j = A'^r_j + \xi_{ij}, A^r_i = A'^r_i + \xi_{ij}$ where $\xi_{ij}$ is an indicator function of $\{i,j\} \in E(G)$. Then $A'^r_j, A'^r_i, A^r_j, A^r_i, \xi_{ij}$ are independent.
$\xi_{ij} \sim Bern(\frac{b\log n}{n})$.
Then we have
\begin{align*}
E \big[  \exp\big(k\beta (A^r_i - A^0_i + A^r_j - A^0_j ) \big) \big] & = E[\exp(2k\beta \xi_{ij})] E[\exp(k\beta (A'^r_i - A^r_i)]
E[\exp(k\beta (A'^r_j - A^r_j)] \\
& = (1+o(1))E[\exp(k\beta(A'^r_i - A^r_i))] E[\exp(k\beta(A'^r_j - A^r_j))]
\end{align*}
From Equation \eqref{eq:pl} we have
$$
E[\exp(k\beta(A^r_i - A^0_i))] = (1+o(1)) \sum_{t=-n/k}^{-1} P(A^r_i - A^0_i = t \log n)E[\exp(k\beta t \log n)]
$$
we have
\begin{align*}
E \big[  \exp\big(k\beta (A^r_i - A^0_i + A^r_j - A^0_j ) \big) \big] & = (1+o(1))
\sum_{t_1=-n/k}^{-1} P(A'^r_i - A^r_i = t_1 \log n)E[\exp(k\beta t_1 \log n)] \\
& \cdot
\sum_{t_2=-n/k}^{-1} P(A'^r_j - A^r_j = t_2 \log n)E[\exp(k\beta t_2 \log n)] \\
& = (1+o(1))  \sum_{t=-2\frac{n}{k}}^{-2} E[\exp(k\beta t \log n)]\\
& \cdot \sum_{\substack{t_1 + t_2 = t \\ t_1 < 0, t_2 < 0}}
P(A'^r_i - A^r_i = t_1 \log n) P(A'^r_j - A^r_j = t_2\log n)
\end{align*}
Since 
\begin{align*}
P(A^r_i - A^0_i + A^r_j - A^0_j = t\log n)
&= \sum_{\substack{t_1 + t_2 + t_3 = t\\ t_3 \in\{0, 1\}}} P(A'^r_i - A^r_i = t_1 \log n) P(A'^r_j - A^r_j = t_2 \log n) P(2\xi_{ij} = t_3 \log n) \\
&=(1+o(1)) \sum_{t_1 + t_2 = t} P(A'^r_i - A^r_i = t_1 \log n) P(A'^r_j - A^r_j = t_2 \log n)  \\
\end{align*}
By Equation \ref{eq:tD}, $P(G\in G_1) = 1-o(1)$. The above summation can be further restricted to $t_1 < 0, t_2 < 0$. Thus Lemma \ref{lem:BijG} follows. 
\end{proof}
\bibliographystyle{plain}
\bibliography{exportlist}
\end{document}

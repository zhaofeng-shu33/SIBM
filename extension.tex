\documentclass{article}
\input{macros.tex}
\title{Extension of SIBM to multiple communities}
\author{Feng Zhao}
\begin{document}
\maketitle

\section{Problem setup and main results} \label{s:Preliminaries}
We first recall the definition of Symmetric Stochastic Block Model (SSBM) with $k$ communities and the definition of Ising model with $k$ state.
\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
Let $n$ be a positive even integer, $W= \{1, \omega, \dots, \omega^{k-1}\}$ is a cyclic group with order $k$ and let $p,q\in[0,1]$ be two real numbers. Let $X=(X_1,\dots,X_n)\in W^n$, and let $G=([n],E(G))$ be a undirected graph with vertex set $[n]$ and edge set $E(G)$. The pair $(X,G)$ is drawn under $\SSBM(n,p,q)$ if 

\noindent
(i) $X$ is drawn uniformly with the constraint that $|\{v \in [n] : X_v = \omega^i\}| = \frac{n}{k}$;

\noindent
(ii) the vertices $i$ and $j$ in $G$ are connected with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$, independently of other pairs of vertices.
\end{definition}
 
From each element $\sigma$ in a symmetric group $S_k$, we can induce a permutation function $f$ on $x=(x_1,\dots,x_n)\in W^n$ as $f(x_i) = \gamma(i)$ ($\gamma$ is a function on $[k]$).
$x$ and $f(x)$ correspond to the same balanced partition, and the conditional distribution $P(G|X=x)$ is the same as $P(G|X=f(x))$ in the above definition. Therefore, we can only hope to recover $X$ from $G$ up to a global permutation.

In this paper, we focus on the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. In this regime, it is well known that exact recovery of $X$ (up to a global sign) from $G$ is possible if and only if $\sqrt{a}-\sqrt{b} > \sqrt{k}$  \cite{abbe2015exact}.
 
Given a partition/labeling $X$ on $n$ vertices, the SBM specifies how to generate a random graph on these $n$ vertices according to the labeling. In some sense, Ising model works in the opposite direction, i.e., given a graph $G=([n],E(G))$, Ising model defines a probability distribution on all possible labelings $W^n$ of these $n$ vertices. 

We define the indicator function $I(x, y)$ as:
\begin{equation}
I(x, y) = \begin{cases}
 k-1 & x = y \\
 -1 & x \neq y
\end{cases}
\end{equation}
% A general Ising model without external fields is a probability distribution on the configurations $\{\pm 1\}^n$ such that
% $$
% P(\sigma)=\frac{1}{Z}
% \exp(\sum_{i\neq j} J_{ij} \sigma_i \sigma_j) 
% \quad\quad
% \text{for every~}
% \sigma=(\sigma_1,\dots,\sigma_n) \in \{\pm 1\}^n ,
% $$
% where $Z=\sum_{\sigma \in \{\pm 1\}^n} \exp(\sum_{i\neq j} J_{ij} \sigma_i \sigma_j)$ is the normalizing constant.
% We incorporate the graphical structure of $G$ into the Ising model by setting $J_{ij}=\beta$ for all  $\{i,j\}\in E(G)$ and $J_{ij}=-\alpha\log(n)/n$ for all $\{i,j\}\notin E(G)$, where $\alpha,\beta>0$ are positive constants. 
% This is summarized in the following definition.

 
 \begin{definition}[Ising model]
Define an Ising model on a graph $G=([n],E(G))$ with parameters $\alpha,\beta>0$ as the probability distribution on the configurations $\sigma\in\{\pm 1\}^n$ such that\footnote{When there is only one sample, we usually denote it as $\bar{\sigma}$. When there are $m$ (independent) samples, we usually denote them as $\sigma^{(1)},\dots,\sigma^{(m)}$.}
\begin{equation} \label{eq:isingma}
P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\beta\sum_{\{i,j\}\in E(G)} I(\bar{\sigma}_i ,\bar{\sigma}_j)
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j)\Big) ,
\end{equation}
where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and 
\begin{equation}  \label{eq:zg}
Z_G(\alpha,\beta)=\sum_{\bar{\sigma} \in W^n} \exp\Big(\beta\sum_{\{i,j\}\in E(G)}I(\bar{\sigma}_i, \bar{\sigma}_j)
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j) \Big) 
\end{equation}
is the normalizing constant.
\end{definition}






By definition we always have $P_{\sigma|G}(\sigma=\bar{\sigma})=P_{\sigma|G}(\sigma=f(\bar{\sigma}))$ in the Ising model. Next we present our new model, the Stochastic Ising Block Model (SIBM), which can be viewed as a natural composition of the SSBM and the Ising model. In SIBM, we first draw a pair $(X,G)$ under $\SSBM(n,p,q)$.  Then we draw $m$ independent samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ from the Ising model on the graph $G$, where $\sigma^{(u)}\in W^n$ for all $u\in[m]$.

\begin{definition}[Stochastic Ising Block Model]
Let $n$ and $m$ be positive integers such that $n$ is even. Let $p,q\in[0,1]$ be two real numbers and let $\alpha,\beta>0$. The triple $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ is drawn under $\SIBM(n,p,q,\alpha,\beta,m)$ if

\noindent
(i) the pair $(X,G)$ is drawn under $\SSBM(n,p,q)$;

\noindent
(ii) for every $i\in[m]$, each sample $\sigma^{(i)}=(\sigma_1^{(i)},\dots,\sigma_n^{(i)}) \in W^n$ is drawn independently according to the distribution \eqref{eq:isingma}.
\end{definition}

Notice that we only draw the graph $G$ once in SIBM, and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ are drawn independently from the Ising model on the {\em same} graph $G$.
Our objective is to recover the underlying partition (or the ground truth) $X$ from the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$, and we would like to use the smallest possible number of samples to guarantee the exact recovery of $X$ up to a global permutation.
Below we use the notation $P_{\SIBM}(A):=E_G[P_{\sigma|G}(A)]$ for an event $A$, where the expectation $E_G$ is taken with respect to the distribution given by SSBM. In other words, $P_{\sigma|G}$ is the conditional distribution of  $\sigma$ given a fixed $G$ while $P_{\SIBM}$ is the joint distribution of both $\sigma$ and $G$.
By definition, we have $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f$ is induced from $S_k$. We use the set $\Gamma = \{f_{\gamma}(X) | \gamma \in S_k\}$ to
represent all vectors with permutations on $X$.

\begin{definition}[Exact recovery in SIBM]
Let $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,p,q,\alpha,\beta,m)$.
We say that exact recovery is solvable for $\SIBM(n,p,q,\alpha,\beta,m)$ if there is an algorithm that takes $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ as inputs and outputs $\hat{X}=\hat{X}(\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ such that
$$
P_{\SIBM}(\hat{X} \in \Gamma) \to 1
\text{~~~as~} n\to\infty ,
$$
and we call $P_{\SIBM}(\hat{X} \in \Gamma)$ the success probability of the recovery/decoding algorithm.
\end{definition}


As mentioned above, we consider the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. By definition, the ground truth $X$, the graph $G$ and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ form a Markov chain $X\to G\to \{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Therefor, if we cannot recover $X$ from $G$, then there is no hope to recover $X$ from $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Thus a necessary condition for the exact recovery in SIBM is $\sqrt{a}-\sqrt{b}> \sqrt{k}$, and we will limit ourselves to this case throughout the paper.

\vspace*{.1in}
\noindent{\bf Main Problem:} \emph{For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, what is the smallest sample size $m^\ast$ such that exact recovery is solvable for $\SIBM(n,a\log(n)/n, b\log(n)/n,\alpha,\beta,m^\ast)$?}

Notation convention: $\dist(\sigma, X) := |\{ i \in [n]: \sigma_i \neq X_i\}|$.
For $\cI \subseteq [n]$, we denote the event $\sigma = X^{(\sim \cI)}$ as $\sigma_i \neq X_i$ for all $i \in \cI$ and $\sigma_i = X_i$ for all $i \not\in \cI$.

\appendix
\section{Auxiliary lemmas used in Section}

\begin{lemma} \label{lm:bq}
For $0<\theta<1$,
\begin{align}
& P_{\SIBM}(\sigma_i=-X_i
\big| \dist(\sigma,X) \le n^\theta) \le k n^{\theta-1}
\quad \text{for all~} i\in[n] , \label{eq:l1}\\
& P_{\SIBM}(\sigma_i=-X_i \text{~for all~}  i\in\tilde{\cI}
~\big| \dist(\sigma,X) \le n^\theta) \le \Big(\frac{n^\theta}{n/2-|\tilde{\cI}|}
\Big)^{|\tilde{\cI}|}
\quad \text{for all~} \tilde{\cI}\subseteq [n] .   \label{eq:l2}
\end{align}
\end{lemma}
\begin{proof}
Define $\dist_j(\sigma,X):=|\{i\in[n]:X_i=\omega^j, \sigma_i \neq X_i\}|$ for $j \in \{0, \dots, k-1\}$. Clearly, $\dist(\sigma,X)=\sum_{j=0}^{k-1} \dist_j(\sigma,X)$.

Inequality \eqref{eq:l1} follows immediately from the following equality:
$$
P_{\SIBM}(\sigma_i \neq X_i |
\dist_j(\sigma,X)=u_j,
j=0,\dots,k-1) 
= k u_r / n \textrm{ where } X_i = \omega^r
$$
Without loss of generality,
we only prove the case of $X_i=1 (r=0)$, and
we need the following definition for the proof of this equality:
For $\cI\subseteq[n]$, define $\cI_j:=\{i\in\cI:X_i=\omega^j\}$ Then
\begin{align*}
& P_{\SIBM}(\sigma_i \neq X_i |
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)  \\
= & \frac{P_{\SIBM}(\sigma_i \neq X_i ,
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)}
{P_{\SIBM}(
\dist_j(\sigma, X) = u_j,j=0,\dots, k-1)} \\
= & \frac{\sum_{\cI:i\in\cI_0,|\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }
{\sum_{\cI: |\cI_j|=u_j,j=0,\dots,k-1}P_{\SIBM}(\sigma=X^{(\sim\cI)}) } \\
\overset{(a)}{=} & \frac{\binom{n/k-1}{u_0 -1}}
{\binom{n/k}{u_0} }
= ku_0/n ,
\end{align*}
where equality (a) follows from Lemma~\ref{lm:cc} below.
Similarly, inequality \eqref{eq:l2} follows from the following inequality:
For $u_+\ge |\tilde{\cI}_+|$ and $u_-\ge |\tilde{\cI}_-|$,
\begin{align*}
& P_{\SIBM}(\sigma_i=-X_i \text{~for all~}  i\in\tilde{\cI} ~ \big|
\dist_+(\sigma,X)=u_+,
\dist_-(\sigma,X)=u_-)  \\
= & \frac{P_{\SIBM}(\sigma_i=-X_i \text{~for all~}  i\in\tilde{\cI} ,
\dist_+(\sigma,X)=u_+,
\dist_-(\sigma,X)=u_-)}{P_{\SIBM}(
\dist_+(\sigma,X)=u_+,
\dist_-(\sigma,X)=u_-)} \\
= & \frac{\sum_{\cI:\tilde{\cI}_+\subseteq\cI_+,
\tilde{\cI}_-\subseteq\cI_-,
|\cI_+|=u_+,|\cI_-|=u_-}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }
{\sum_{\cI:|\cI_+|=u_+,|\cI_-|=u_-}P_{\SIBM}(\sigma=X^{(\sim\cI)}) }  \\
\overset{(a)}{=} & \frac{\binom{n/2-|\tilde{\cI}_+|}{u_+ -|\tilde{\cI}_+|} \binom{n/2 - |\tilde{\cI}_-|}{u_- - |\tilde{\cI}_-|}}
{\binom{n/2}{u_+} \binom{n/2}{u_-}}
< \Big(\frac{u_+}{n/2- |\tilde{\cI}_+|} \Big)^{|\tilde{\cI}_+|}
\Big(\frac{u_-}{n/2- |\tilde{\cI}_-|} \Big)^{|\tilde{\cI}_-|} \\
< & \Big(\frac{u_+ + u_-}{n/2- |\tilde{\cI}|} \Big)^{|\tilde{\cI}|}  ,
\end{align*}
where equality (a) again follows from Lemma~\ref{lm:cc} below.
\end{proof}







\begin{lemma} \label{lm:cc}
Let $\cI,\cI'\subseteq[n]\setminus\{i\}$ be two subsets such that $|\cI_j|=|\cI_j'|$ for $j=0,\dots,k-1$. 
Then $P_{\SIBM}(\sigma=X^{(\sim\cI)}) = P_{\SIBM}(\sigma=X^{(\sim\cI')})$.
\end{lemma}

\bibliographystyle{plain}
\bibliography{exportlist}
\end{document}

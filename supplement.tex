\documentclass{article}
\input{macros.tex}
\title{Supplementary Material to SIBM with 2 communities}
\author{Feng Zhao}
\begin{document}
\maketitle
In this supplementary material, we give some undocumented proof.
\begin{lemma}\label{lem:ucBA}
Let $\cG_1:=\{G:B_i-A_i<0\text{~for all~}i\in[n]\}$. Suppose $i\neq j$, then $P(B_i-A_i + B_j - A_j= t\log(n)~|~G\in\cG_1)= (1+o(1))P(B_i-A_i + B_j - A_j= t\log(n))$ for all $t<0$ such that $t\log(n)$ is an integer.
\end{lemma}
\begin{lemma}\label{lem:BijG}
\begin{equation} 
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) ~\big|~ G\in\cG_1 \big] 
= (1+o(1)) E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] ,
\end{equation}
\end{lemma}
\begin{proof}
First we have
\begin{align}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) ~\big|~ G\in\cG_1 \big] 
&= \sum_{t=-n}^{-2} P(B_i -A_i + B_j - A_j = t\log n | G \in \cG_1) \exp(2\beta t \log n) \notag \\
\text{ Lemma \ref{lem:ucBA} implies }&= (1+o(1))\sum_{t=-n}^{-2} P(B_i -A_i + B_j - A_j = t\log n) \exp(2\beta t \log n)
\end{align}
On the other hand, we suppose $X_i \neq X_j$ and we decompose $B_j = B'_j + \xi_{ij}, B_i = B'_i + \xi_{ij}$ where $\xi_{ij}$ is an indicator function of $\{i,j\} \in E(G)$. Then $B'_j, B'_i, A_j, A_i, \xi_{ij}$ are independent.
$\xi_{ij} \sim Bern(\frac{b\log n}{n})$.
Then we have
\begin{align*}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] & = E[\exp(4\beta \xi_{ij})] E[\exp(2\beta (B'_i - A_i)]
E[\exp(2\beta (B'_j - A_j)] \\
& = (1+o(1))E[\exp(2\beta(B'_i - A_i))] E[\exp(2\beta(B'_j - A_j))]
\end{align*}
Using the conclusion that
$$
E[\exp(2\beta(B_i - A_i))] = (1+o(1)) \sum_{t=-n/2}^{-1} P(B_i - A_i = t \log n)E[\exp(2\beta t \log n)]
$$
we have
\begin{align*}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] & = (1+o(1))
\sum_{t_1=-n/2}^{-1} P(B'_i - A_i = t_1 \log n)E[\exp(2\beta t_1 \log n)] \\
& \cdot
\sum_{t_2=-n/2}^{-1} P(B'_j - A_j = t_2 \log n)E[\exp(2\beta t_2 \log n)] \\
& = (1+o(1))  \sum_{t=-n}^{-2} E[\exp(2\beta t \log n)]\sum_{\substack{t_1 + t_2 = t \\ t_1 < 0, t_2 < 0}} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2\log n)
\end{align*}
Since 
\begin{align*}
P(B_i -A_i + B_j - A_j = t\log n)
&= \sum_{\substack{t_1 + t_2 + t_3 = t\\ t_3 \in\{0, 1\}}} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2 \log n) P(2\xi_{ij} = t_3 \log n) \\
&=(1+o(1)) \sum_{t_1 + t_2 = t} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2 \log n)  \\
\end{align*}
Since $P(G\in G_1) = 1-o(1)$, the above summation can be further restricted to $t_1 < 0, t_2 < 0$. Thus Lemma \ref{lem:BijG} follows. 
\end{proof}
\begin{lemma}\label{lem:post_independent}
	Let $p_{ij}=\Pr(\{\{i,j\} \in E(G) \})$ be the prior probability, which equals $\frac{a\log n}{n}$ or $\frac{b\log n}{n}$ depending on $X$.
	The event $\{\{i,j\} \in E(G) \}$ are independent given $\sigma$ and the posterior probability is
	\begin{equation}
	\Pr(\{\{i,j\} \in E(G) \} | \sigma) = \frac{c p_{ij} }{1-p_{ij} + cp_{ij}} \text{ where  } c= \exp\Big((\beta + \frac{\alpha \log n}{n} ) I(\bar{\sigma}_i, \bar{\sigma}_j) \Big)
	\end{equation}
\end{lemma}
\begin{proof}
	Let $Y_{ij}$ be a Bernoulli random variable with $\Pr(Y_{ij} = 1) = \Pr(\{\{i,j\} \in E(G)\}) = p_{ij}$. $Y_{ij}$ represents whether there is an edge between node $i$ and $j$.
	The prior distribution for $Y:=\{Y_{ij}\}$ can be written as:
	$$
	\Pr(Y) = \prod_{i,j} p_{ij}^{y_{ij}} (1-p_{ij})^{1-y_{ij}}
	$$ 
	Using the conditional distribution for $\sigma|G$, the posterior probability for $Y| \sigma$ can be written as
	$$
	\Pr(Y|\sigma) = C\prod_{ij} \exp((\beta + \frac{\alpha \log n}{n})I(\bar{\sigma_i}, \bar{\sigma_j} )y_{ij}) \Pr(Y)
	$$
	where $C$ is a constant irrelevant with $Y$.
	We can see from the joint distribution of $Y|\sigma$ that $Y_{ij} | \sigma$ are independent and each marginal distribution has the following form:
	$$
	\Pr(Y_{ij} | \sigma) = C_{ij} \exp((\beta + \frac{\alpha \log n}{n})I(\bar{\sigma_i}, \bar{\sigma_j} )y_{ij}) p_{ij}^{y_{ij}} (1-p_{ij})^{1-y_{ij}}
	$$
	Using the normalization condition for $Y_{ij} | \sigma $, we can compute $C_{ij} = \frac{1}{1-p_{ij} + p_{ij}c}$. From $\Pr(\{\{i,j\} \in E(G) \} | \sigma) =\Pr(Y_{ij}=1|\sigma)$ the proof is complete.
\end{proof}

\begin{lemma} \label{lm:qq}
	Let 
	$$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
	$$
	If there is a pair $i,i'\in[n]$ satisfying the following two conditions: (1) $\sigma_i^{(j)}=\sigma_{i'}^{(j)}$ for all $j\in[m]$ and (2) $X_i=-X_{i'}$, then it is not possible to distinguish the case $X_i=-X_{i'}=1$ from the case $X_i=-X_{i'}=-1$. In other words, conditioning on the samples, the posterior probability of the ground truth being $X$ is the same as that of the ground truth being $X^{(\sim\{i,i'\})}$, i.e.
	\begin{equation}
	P(X_i=1 | X_i = -X_{i'}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m]) = P(X_i=-1 | X_i = -X_{i'}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m])
	\end{equation}
\end{lemma}
\begin{proof}
We only need to show
\begin{equation}\label{eq:11}
P(X_i=1, X_{i'} = -1, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m]) = P(X_i=-1, X_{i'} = 1, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m])
\end{equation}
Given $X_j = \bar{X}_j \in \{\pm 1\}$ for $j \neq i,i'$.we have Eq. \eqref{eq:12} $\Rightarrow$ \eqref{eq:11}
\begin{equation}\label{eq:12}
P(X_i=1, X_{i'} = -1, X_j = \bar{X}_j,j \neq i,i', \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m]) = P(X_i=-1, X_{i'} = 1, X_j = -\bar{X}_j,j \neq i,i', \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m])
\end{equation}
Using the Markov property of $X \to G \to \sigma$, we have Eq. \eqref{eq:13} $\Rightarrow$ \eqref{eq:12}
\begin{align}\label{eq:13}
&P_{\sigma | G}(\sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m])P(G | X_i=1, X_{i'} = -1, X_j = \bar{X}_j,j \neq i,i') \notag\\
=&P_{\sigma | G} (\sigma_i^{(j)}= \sigma_{i'}^{(j)}, j\in[m])P(G | X_i=-1, X_{i'} = 1, X_j = -\bar{X}_j,j \neq i,i')
\end{align}
Since the conditional probability for graph is equal, the proof is complete.
\end{proof}
\end{document}

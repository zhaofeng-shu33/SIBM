\documentclass{article}
\input{macros.tex}
\title{Supplementary Material to SIBM with 2 communities}
\author{Feng Zhao}
\begin{document}
\maketitle
In this supplementary material, we give some undocumented proof in \cite{ye2020exact}.
This document is not so useful since we have a general $k$ version. The following lemma assumes $k=2$.
\begin{lemma}\label{lem:ucBA}
Let $\cG_1:=\{G:B_i-A_i<0\text{~for all~}i\in[n]\}$. Suppose $i\neq j$, then $P(B_i-A_i + B_j - A_j= t\log(n)~|~G\in\cG_1)= (1+o(1))P(B_i-A_i + B_j - A_j= t\log(n))$ for all $t<0$ such that $t\log(n)$ is an integer.
\end{lemma}
\begin{lemma}\label{lem:BijG}
\begin{equation} 
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) ~\big|~ G\in\cG_1 \big] 
= (1+o(1)) E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] ,
\end{equation}
\end{lemma}
\begin{proof}
First we have
\begin{align}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) ~\big|~ G\in\cG_1 \big] 
&= \sum_{t=-n}^{-2} P(B_i -A_i + B_j - A_j = t\log n | G \in \cG_1) \exp(2\beta t \log n) \notag \\
\text{ Lemma \ref{lem:ucBA} implies }&= (1+o(1))\sum_{t=-n}^{-2} P(B_i -A_i + B_j - A_j = t\log n) \exp(2\beta t \log n)
\end{align}
On the other hand, we suppose $X_i \neq X_j$ and we decompose $B_j = B'_j + \xi_{ij}, B_i = B'_i + \xi_{ij}$ where $\xi_{ij}$ is an indicator function of $\{i,j\} \in E(G)$. Then $B'_j, B'_i, A_j, A_i, \xi_{ij}$ are independent.
$\xi_{ij} \sim Bern(\frac{b\log n}{n})$.
Then we have
\begin{align*}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] & = E[\exp(4\beta \xi_{ij})] E[\exp(2\beta (B'_i - A_i)]
E[\exp(2\beta (B'_j - A_j)] \\
& = (1+o(1))E[\exp(2\beta(B'_i - A_i))] E[\exp(2\beta(B'_j - A_j))]
\end{align*}
Using the conclusion that
$$
E[\exp(2\beta(B_i - A_i))] = (1+o(1)) \sum_{t=-n/2}^{-1} P(B_i - A_i = t \log n)E[\exp(2\beta t \log n)]
$$
we have
\begin{align*}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] & = (1+o(1))
\sum_{t_1=-n/2}^{-1} P(B'_i - A_i = t_1 \log n)E[\exp(2\beta t_1 \log n)] \\
& \cdot
\sum_{t_2=-n/2}^{-1} P(B'_j - A_j = t_2 \log n)E[\exp(2\beta t_2 \log n)] \\
& = (1+o(1))  \sum_{t=-n}^{-2} E[\exp(2\beta t \log n)]\sum_{\substack{t_1 + t_2 = t \\ t_1 < 0, t_2 < 0}} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2\log n)
\end{align*}
Since 
\begin{align*}
P(B_i -A_i + B_j - A_j = t\log n)
&= \sum_{\substack{t_1 + t_2 + t_3 = t\\ t_3 \in\{0, 1\}}} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2 \log n) P(2\xi_{ij} = t_3 \log n) \\
&=(1+o(1)) \sum_{t_1 + t_2 = t} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2 \log n)  \\
\end{align*}
Since $P(G\in G_1) = 1-o(1)$, the above summation can be further restricted to $t_1 < 0, t_2 < 0$. Thus Lemma \ref{lem:BijG} follows. 
\end{proof}


\begin{lemma} \label{lm:qq}
	Let 
	$$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
	$$
	If there is a pair $i,i'\in[n]$ satisfying the following two conditions: (1) $\sigma_i^{(j)}=\sigma_{i'}^{(j)}$ for all $j\in[m]$ and (2) $X_i=-X_{i'}$, then it is not possible to distinguish the case $X_i=-X_{i'}=1$ from the case $X_i=-X_{i'}=-1$. In other words, conditioning on the samples, the posterior probability of the ground truth being $X$ is the same as that of the ground truth being $X^{(\sim\{i,i'\})}$, i.e.
	\begin{align}
	&P(X_i=1,X_j = \bar{X}_j \in \{\pm 1\},j \neq i,i' | X_i = -X_{i'}, \sigma_i^{(j)}=\sigma_{i'}^{(j)}, j\in[m]) \notag\\
	&  = P(X_i=-1,X_j = \bar{X}_j \in \{\pm 1\},j \neq i,i' | X_i = -X_{i'}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m])
	\end{align}
	Notice the condition $\sigma_i^{(j)}=\sigma_{i'}^{(j)}, j\in[m]$ is actually saying that we have already known $m$ samples $\sigma = \sigma^{(j)}$ and
	it happens that the $i$-th coordinate and $i'$-th coordinate of each sample are the same. 
\end{lemma}
\begin{proof}
Let $\bar{X}_i = 1, \bar{X}_{i'}=-1$ We only need to show
\begin{equation}\label{eq:12}
P(X=\bar{X}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m]) = P(X=\bar{X}^{\sim(i,i')}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m])
\end{equation}
Let $\cG_{[n]}$ be the set consisting of all the graphs with vertex set $[n]$.
A permutation $\pi\in S_n$ on the vertex set $[n]$ also induces a permutation on $\cG_{[n]}$: For $G\in\cG_{[n]}$, define the graph $\pi(G)\in\cG_{[n]}$ as the graph with the edge set $E(\pi(G))$ satisfying that $\{\pi(i),\pi(j)\}\in E(\pi(G))$ if and only if $\{i,j\}\in E(G)$.
It is easy to see that for any $\pi\in S_n$ and any $G\in\cG_{[n]}$,
$$
Z_G(\alpha,\beta)
=Z_{\pi(G)}(\alpha,\beta),
$$
%where $Z_G(\alpha,\beta)$ is defined in \eqref{eq:zg}.
We define a permutation $\pi$ such that $\pi(i) = i', \pi(i') = i$ and $\pi(j) = j$ for $j\neq i,i'$.
We can check that $\bar{X}_i=\bar{X}^{\sim(i,i')}_{\pi(i)}$ for all $i\in[n]$ holds.
$$
P_{\SSBM}(G  | X = \bar{X})=P_{\SSBM}(\pi(G) | X = \bar{X}^{\sim(i,i')})  ,
$$

Using the Markov property of $X \to G \to \sigma$, we have
\begin{align*}
P(X=\bar{X}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m]) & = \sum_{G \in \cG_{[n]}} P(X=\bar{X}) P_{\SSBM}(G  | X = \bar{X})\prod_{j=1}^m P_{\sigma | G}(\sigma =\sigma^{(j)})\\
\end{align*}
If we can show that 
\begin{equation}\label{eq:sigmaEqual}
P_{\sigma | G}(\sigma =\sigma^{(j)}) = P_{\sigma | \pi(G)}(\sigma =\sigma^{(j)})
\end{equation}
Then 
\begin{align*}
P(X=\bar{X}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m]) & = \sum_{G \in \cG_{[n]}} P(X=\bar{X}^{\sim(i,i')}) P_{\SSBM}(\pi(G)  | X = \bar{X}^{\sim(i,i')})\prod_{j=1}^m P_{\sigma | \pi(G)}(\sigma =\sigma^{(j)})\\
& = P(\bar{X}^{\sim(i,i')}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m])
\end{align*}
The key to prove Equation \eqref{eq:sigmaEqual} lies at the property $\sigma^{(j)}_i = \sigma^{(j)}_{i'}$.
Let $c_G(\sigma, i,j) = (\beta + \frac{\alpha \log n}{n})\sigma_i \sigma_j \mathbbm{1}[\{i,j\}\in E(G)]  - \frac{\alpha \log n}{n}$
and $C_G(\sigma) = \frac{1}{Z_G(\alpha, \beta)}\exp(\sum_{j_1, j_2 \not\in \{i,i'\}} c_G(\sigma, j_1, j_2) - c_G(\sigma, i,i'))$
\begin{align*}
P_{\sigma | G}(\sigma =\sigma^{(j)}) & = C_G(\sigma^{(j)})\exp(\sum_{s=1}^n c_G(\sigma^{(j)}, i, s)
+ \sum_{s=1}^n c_G(\sigma^{(j)}, i',s) ) \\
& = C_{\pi(G)}(\sigma^{(j)})\exp(\sum_{j=s}^n c_G(\sigma^{(j)}, i, s)
+ \sum_{s=1}^n c_G(\sigma^{(j)}, i',s) ) \\
& = C_{\pi(G)}(\sigma^{(j)})\exp(\sum_{s=1}^n c_{\pi(G)}(\sigma^{(j)}, i', s)+ \sum_{s=1}^n c_{\pi(G)}(\sigma^{(j)}, i, s) ) \\
& = C_{\pi(G)}(\sigma^{(j)})\exp(\sum_{s=1}^n c_{\pi(G)}(\sigma^{(j)}, \pi(i), s) + \sum_{s=1}^n c_{\pi(G)}(\sigma^{(j)}, \pi(i'), s) ) \\
& = P_{\sigma | \pi(G)}(\sigma =\sigma^{(j)})
\end{align*}
\end{proof}
The following lemma assumes a general $k$.
Some notation: $\Lambda := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k-1\}$.
\begin{lemma}
	When $\dist(\bar{\sigma}, \mathbf{1}_n) \geq \frac{n}{\log^{\delta} n}$ where $0<\delta < 1$ and $\arg\,\min_{\sigma'\in \Lambda} \dist(\bar{\sigma}, \sigma') = \mathbf{1}_n$. Show that
	$P_{\sigma | G}(\sigma = \bar{\sigma} ) > \exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n)$
	happens with probability less than $\exp(-\tau(\alpha,\beta) n \log^{1-\delta} n )$ where $C$ is an arbitrary constant, $\tau(\alpha,\beta)$ is a positive number.
\end{lemma}
\begin{proof}
	Let $n_r = |\{\bar{\sigma}_i = w^r | i\in [n] \}|$. Then $n_0 \geq n_r$ for $r=1, \dots, k-1$.
	WLOG, suppose $n_0 \geq n_1 \dots \geq n_{k-1}$.
	We are concerned with the number $N_w = \frac{1}{2}(n(n-1) - \sum_{r=0}^{k-1} n_r(n_r-1))
	=\frac{1}{2}(n^2 - \sum_{r=0}^{k-1} n_r^2)$.
	Taking the $\log$ on both sides of $P_{\sigma | G}(\sigma = \bar{\sigma} ) > \exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n)$ we can get
	\begin{equation}\label{eq:small}
	(\beta + \frac{\alpha \log n}{n}) \sum_{\bar{\sigma}_i  \neq \bar{\sigma}_j} Z_{ij} \leq \frac{\alpha \log n}{n} N_w + C n
	\end{equation}
	
	Firstly we estimate the order of $N_w$, obviously $N_w \leq \frac{1}{2} n^2$.
	Using the conclusion in Appendix A of \cite{yixin} we have
	\begin{equation}
	\sum_{r=0}^{k-1} n_r^2 \leq
	\begin{cases}
	n n_0 & n_0 \leq \frac{n}{2} \\
	n^2 - 2n_0(n-n_0) & n_0 > \frac{n}{2}
	\end{cases}
	\end{equation}
	We have $n_0 \leq n - \frac{n}{\log^{\delta} n}$ and $n_0 \geq \frac{n}{k}$. When $n_0 > \frac{n}{2}$ we take $n_0 = n - \frac{n}{\log^{1/3} n}$
	and we have $N_w \geq n_0 (n - n_0) = \frac{n^2}{\log^{\delta} n}(1+o(1))$. When $n_0 < \frac{n}{2}$ we take $n_0 = \frac{n}{2}$ such that
	$N_w \geq \frac{n^2}{4}$. So generally we have $\frac{n^2}{\log^{\delta} n}(1+o(1)) \leq N_w \leq \frac{1}{2}n^2$.
	The Bernoulli random variables are independent, taking either $Bern(\frac{a\log n}{n})$ or $Bern(\frac{b \log n}{n})$ depending on whether
	$X_i$ equals to $X_j$.
	Since $\frac{\log n}{n} N_w$ is dominated than $Cn$ we neglect inferior terms to rewrite \eqref{eq:small} as
	\begin{equation}
		\sum_{ \bar{\sigma}_i  \neq \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_w}{n}(1+o(1))
	\end{equation}
	Let $N_1$ be the number of random variables of $Z_{ij} \sim Bern(\frac{a\log n}{n})$.
	and $N_2 = N_w - N_1$ is that of $Z_{ij} \sim Bern(\frac{b\log n}{n})$.
	
	Using Chernoff Inequality we have
	\begin{align*}
	\Pr(\sum_{ \bar{\sigma}_i  = \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_w}{n})& \leq (E[\exp(-s Z_{ij})])^{N_1} (E[-exp(-s Z_{ij})])^{N_2} \exp(\frac{\alpha}{\beta} \frac{\log n N_w s}{n}(1+o(1))) \\
	&= \exp( \frac{\log n}{n}(1+o(1))(\exp(-s)-1)(aN_1 + bN_2)+\frac{\alpha}{\beta} \frac{\log n N_w s}{n}(1+o(1)))
	\end{align*}
	Since $s > 0$, we further have
	\begin{align*}
		\Pr(\sum_{ \bar{\sigma}_i  = \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_w}{n})
		& \leq \exp( \frac{N_w\log n }{n}(b(\exp(-s)-1)+ \frac{\alpha}{\beta}s + o(1))) 
	\end{align*}
	Let $h_b(x) = x - b -x\log \frac{x}{b}$, which satisfies $h_b(x) < 0$ for $0<x<b$,
	and take $s=-\log\frac{\alpha}{b\beta} > 0$, using 
	$N_w \geq \frac{n^2}{\log^{1/3} n}$ we have
	\begin{align*}
\Pr(\sum_{ \bar{\sigma}_i  = \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_w}{n})&\leq \exp( N_w \frac{\log n}{n} h_b(\frac{\alpha}{\beta})(1+o(1))) \\
		& \leq \exp (h_b(\frac{\alpha}{\beta}) n \log^{1-\delta} n (1+o(1)))
	\end{align*}
\end{proof}
\begin{lemma}
	For $SBM(2,n, \frac{a\log n}{n}, \frac{b \log n}{n})$, the exact recovery error rate $P(F)\asymp n^{2-(\sqrt{a} - \sqrt{b})^2}$ when we know that the ground truth community labels are balanced.If we allow unbalanced ground truth, possibly the rate is $ n^{1-(\sqrt{a} - \sqrt{b})^2/2} $.
\end{lemma}
\begin{proof}
	We follow Abbe's proof but enhance the inequality \cite{abbe}.
	Instead of using Equation (16) to scale up. We consider the maximum value of $f(k)= -2\log(2k)
	+4\frac{k}{n}\log n - (\frac{1}{2}- \frac{k}{n})4\epsilon \log n + 2$.
	We can verify that the minimum value of $f(k)$ is actually $f(1)$ for $1\leq k \leq \frac{n}{4}$.
	Therefore we have
	$ P(F) \leq \sum_{k=1}^{n/4} \exp(kf(1)) \leq \exp(f(1))$ where $f(1) < 0 $ for sufficient large $n$.
	$\exp(f(1)) = c n^{-2\epsilon}$ and let $\epsilon = \frac{(\sqrt{a}-\sqrt{b})^2}{2} - 1$.
\end{proof}
\begin{theorem}
	When $\alpha < b \beta $ and $0 < \delta < 1$, it is not solvable for any $m= O(\log^{\delta}(n))$ to
	exactly recover SIBM$(n,k,a \log(n)/n, b\log(n)/n ,\alpha, \beta, m)$.
\end{theorem}
\begin{proposition}
	The ML is a special case of index optimization algorithm with
	$\kappa = (\frac{(a - b)}{\log(a/b)} + o(1))\frac{\log n}{n}$.
	\begin{equation}\label{eq:kappa_opt_function}
	\max \sum_{(i,j) \in E} \delta_{\sigma_i\sigma_j} - \kappa \sum_{(i,j) \not\in E}  \delta_{\sigma_i\sigma_j} 
	\end{equation}
\end{proposition}
\begin{proof}
	Let $z_{ij} \in \{0, 1\}$ to represent whether there is an edge between two nodes in a graph. Then
	$$
	p(z | \sigma) = \prod_{\sigma_i = \sigma_j} p^{z_{ij}}
		(1-p)^{1-z_{ij}} \prod_{\sigma_i \neq \sigma_j} q^{z_{ij}}(1-q)^{1-z_{ij}}
	$$
	The log-likelyhood is
	\begin{align}
	\log p(z | \sigma) &= \log p \sum_{(i,j) \in E} \delta_{\sigma_i\sigma_j}
	+ \log(1-p) \sum_{(i,j)\in E} \delta_{\sigma_i\sigma_j} 
	+ \log q \sum_{(i,j) \not\in E} (1-\delta_{\sigma_i\sigma_j})
	+\log (1-q) \sum_{(i,j) \not\in E} (1-\delta_{\sigma_i\sigma_j}) \\
	& =C + \log\frac{p}{q} \sum_{(i,j) \in E} \delta_{\sigma_i\sigma_j}
	-\frac{1-q}{1-p}\sum_{(i,j) \not\in E} \delta_{\sigma_i\sigma_j}
	\end{align}
	Therefore, to maximize $\log p (z | \sigma)$ is equivalent to maximize \eqref{eq:kappa_opt_function}
	with $\kappa = \frac{\log((1-q)/(1-p))}{\log (p / q)}$ with $p=\frac{a}{b\log n}$ and
	$q = \frac{b\log n}{n}$. After expansion we have $\kappa_{ML} = (\frac{(a - b)}{\log(a/b)} + o(1))\frac{\log n}{n}$.
	Notice that the inequality
	$ \frac{(a - b)}{\log(a/b)}  > b$ holds. This is $\alpha > b \beta$.
	For Maximum Modularity, we have already $\kappa_{MQ} = \frac{a+b}{2}\frac{\log n}{n} > \kappa_{ML}$.
\end{proof}
\begin{proposition}
	Define the exact error rate as
	$r_1(x, \sigma) = \max_{i\in [n]} \delta(x_i \neq \pi(\sigma_i))$.
	The mis-matched ratio is defined in \cite{zhang} as
	$ r(x, \sigma) = \frac{1}{n} \sum_{i=1}^n \delta(x_i \neq \pi(\sigma_i))$
	We then have $ r(x, \sigma) \leq r_1(x, \sigma) \leq n r(x, \sigma)$.
\end{proposition}
\begin{thebibliography}{1}
	

	\bibitem{ye2020exact}
	Min Ye.
	\newblock Exact recovery and sharp thresholds of stochastic ising block model,
	2020.
	\bibitem{yixin}
	Chen, Yuxin, Changho Suh, and Andrea J. Goldsmith. "Information recovery from pairwise measurements." *IEEE Transactions on Information Theory* 62.10 (2016): 5881-5905.
	\bibitem{abbe}
	Abbe, Emmanuel, Afonso S. Bandeira, and Georgina Hall. "Exact recovery in the stochastic block model." *IEEE Transactions on Information Theory* 62.1 (2015): 471-487.
	\bibitem{zhang}
	Zhang, Anderson Y., and Harrison H. Zhou. "Minimax rates of community detection in stochastic block models." The Annals of Statistics 44.5 (2016): 2252-2280.
\end{thebibliography}

\end{document}

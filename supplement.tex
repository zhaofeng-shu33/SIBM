\documentclass{article}
\input{macros.tex}
\title{Supplementary Material to SIBM with 2 communities}
\author{Feng Zhao}
\begin{document}
\maketitle
In this supplementary material, we give some undocumented proof.
\begin{lemma}\label{lem:ucBA}
Let $\cG_1:=\{G:B_i-A_i<0\text{~for all~}i\in[n]\}$. Suppose $i\neq j$, then $P(B_i-A_i + B_j - A_j= t\log(n)~|~G\in\cG_1)= (1+o(1))P(B_i-A_i + B_j - A_j= t\log(n))$ for all $t<0$ such that $t\log(n)$ is an integer.
\end{lemma}
\begin{lemma}\label{lem:BijG}
\begin{equation} 
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) ~\big|~ G\in\cG_1 \big] 
= (1+o(1)) E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] ,
\end{equation}
\end{lemma}
\begin{proof}
First we have
\begin{align}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) ~\big|~ G\in\cG_1 \big] 
&= \sum_{t=-n}^{-2} P(B_i -A_i + B_j - A_j = t\log n | G \in \cG_1) \exp(2\beta t \log n) \notag \\
\text{ Lemma \ref{lem:ucBA} implies }&= (1+o(1))\sum_{t=-n}^{-2} P(B_i -A_i + B_j - A_j = t\log n) \exp(2\beta t \log n)
\end{align}
On the other hand, we suppose $X_i \neq X_j$ and we decompose $B_j = B'_j + \xi_{ij}, B_i = B'_i + \xi_{ij}$ where $\xi_{ij}$ is an indicator function of $\{i,j\} \in E(G)$. Then $B'_j, B'_i, A_j, A_i, \xi_{ij}$ are independent.
$\xi_{ij} \sim Bern(\frac{b\log n}{n})$.
Then we have
\begin{align*}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] & = E[\exp(4\beta \xi_{ij})] E[\exp(2\beta (B'_i - A_i)]
E[\exp(2\beta (B'_j - A_j)] \\
& = (1+o(1))E[\exp(2\beta(B'_i - A_i))] E[\exp(2\beta(B'_j - A_j))]
\end{align*}
Using the conclusion that
$$
E[\exp(2\beta(B_i - A_i))] = (1+o(1)) \sum_{t=-n/2}^{-1} P(B_i - A_i = t \log n)E[\exp(2\beta t \log n)]
$$
we have
\begin{align*}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] & = (1+o(1))
\sum_{t_1=-n/2}^{-1} P(B'_i - A_i = t_1 \log n)E[\exp(2\beta t_1 \log n)] \\
& \cdot
\sum_{t_2=-n/2}^{-1} P(B'_j - A_j = t_2 \log n)E[\exp(2\beta t_2 \log n)] \\
& = (1+o(1))  \sum_{t=-n}^{-2} E[\exp(2\beta t \log n)]\sum_{\substack{t_1 + t_2 = t \\ t_1 < 0, t_2 < 0}} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2\log n)
\end{align*}
Since 
\begin{align*}
P(B_i -A_i + B_j - A_j = t\log n)
&= \sum_{\substack{t_1 + t_2 + t_3 = t\\ t_3 \in\{0, 1\}}} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2 \log n) P(2\xi_{ij} = t_3 \log n) \\
&=(1+o(1)) \sum_{t_1 + t_2 = t} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2 \log n)  \\
\end{align*}
Since $P(G\in G_1) = 1-o(1)$, the above summation can be further restricted to $t_1 < 0, t_2 < 0$. Thus Lemma \ref{lem:BijG} follows. 
\end{proof}
\begin{lemma}\label{lem:post_independent}
	Let $p_{ij}=\Pr(\{\{i,j\} \in E(G) \})$ be the prior probability, which equals $\frac{a\log n}{n}$ or $\frac{b\log n}{n}$ depending on $X$.
	The event $\{\{i,j\} \in E(G) \}$ are independent given $\sigma$ and the posterior probability is
	\begin{equation}
	\Pr(\{\{i,j\} \in E(G) \} | \sigma) = \frac{c p_{ij} }{1-p_{ij} + cp_{ij}} \text{ where  } c= \exp\Big((\beta + \frac{\alpha \log n}{n} ) I(\bar{\sigma}_i, \bar{\sigma}_j) \Big)
	\end{equation}
\end{lemma}
\begin{proof}
	Let $Y_{ij}$ be a Bernoulli random variable with $\Pr(Y_{ij} = 1) = \Pr(\{\{i,j\} \in E(G)\}) = p_{ij}$. $Y_{ij}$ represents whether there is an edge between node $i$ and $j$.
	The prior distribution for $Y:=\{Y_{ij}\}$ can be written as:
	$$
	\Pr(Y) = \prod_{i,j} p_{ij}^{y_{ij}} (1-p_{ij})^{1-y_{ij}}
	$$ 
	Using the conditional distribution for $\sigma|G$, the posterior probability for $Y| \sigma$ can be written as
	$$
	\Pr(Y|\sigma) = C\prod_{ij} \exp((\beta + \frac{\alpha \log n}{n})I(\bar{\sigma_i}, \bar{\sigma_j} )y_{ij}) \Pr(Y)
	$$
	where $C$ is a constant irrelevant with $Y$.
	We can see from the joint distribution of $Y|\sigma$ that $Y_{ij} | \sigma$ are independent and each marginal distribution has the following form:
	$$
	\Pr(Y_{ij} | \sigma) = C_{ij} \exp((\beta + \frac{\alpha \log n}{n})I(\bar{\sigma_i}, \bar{\sigma_j} )y_{ij}) p_{ij}^{y_{ij}} (1-p_{ij})^{1-y_{ij}}
	$$
	Using the normalization condition for $Y_{ij} | \sigma $, we can compute $C_{ij} = \frac{1}{1-p_{ij} + p_{ij}c}$. From $\Pr(\{\{i,j\} \in E(G) \} | \sigma) =\Pr(Y_{ij}=1|\sigma)$ the proof is complete.
\end{proof}
\begin{lemma}\label{lem:xpi}
	Suppose $X_i = - X_i'$. And a permutation $\pi$ satisfying $\pi(i) = i', \pi(i') = i, \pi(j) = j$ for $j\neq i, i'$.
    Then we have $P_{SSBM}(G) = P_{SSBM}(\pi(G))$. Further if we have $m$ samples $\sigma^{(1)}, \dots, \sigma^{(m)}$ such that $\sigma^{(j)}_i = \sigma^{(j)}_{i'}$ then we have
    $P_{SSBM}(G | \sigma=\sigma^{(j)},j\in[m]) = P_{SSBM}(\pi(G) | \sigma=\sigma^{(j)},j\in[m])$
\end{lemma}
\begin{proof}
	We have already known that the random variable $G$ can be regarded as collections of binomial random variables.
	That is, $G$ is described uniquely by the set $\{y_{st}, s,t \in [n], y_{st}=y_{ts}, s\neq t, y_{st} \in \{0, 1\} \}$ and the corresponding probability is
	\begin{equation}
	P(G) = \prod_{s,t} p_{st}^{y_{st}} (1 - p_{st})^{1-y_{st}}
	\end{equation}
	For $\pi(G)$, the difference with $G$ is that $i \to i', i' \to i$.
	Therefore we only need to consider the edges whose endpoints are either $i$ or $i'$. WLOG, we suppose $X_i=1, X'_i=-1$
	and 
	\begin{align*}
	M_1 &= |\{j | y_{ij} = 1, X_j = 1, j \neq i \}| \\
	M_2 &= |\{j | y_{ij} = 1, X_j = -1, j \neq i\}| \\
	N_1 &= |\{j | y_{ij} = 0, X_j = 1, j \neq i\}| \\
	N_2 &= |\{j | y_{ij} = 0, X_j = -1, j \neq i\}|	
	\end{align*}
	Also let $A=\frac{a \log n }{n}, B=\frac{b \log n}{n}$.
	Then for $G$, $i$ contributes to $A^{M_1} B^{M_2}(1-A)^{N1}(1-B)^{N2}$. For $\pi(G)$ we can show that $i'$ contributes
	to $A^{M_1} B^{M_2}(1-A)^{N1}(1-B)^{N2}$. Similarly, the contribution of $i'$ for $G$ is the same with that of $i$ for $\pi(G)$. Therefore
	we conclude that $P_{SSBM}(G) = P_{SSBM}(\pi(G))$. Using Lemma \ref{lem:post_independent}, since $\sigma_i^{j} = \sigma_{i'}^{(j)}$,
	$\Pr(\{\{i,j\} \in E(G)\} | \sigma^{(r)}, r\in[m]) =\Pr(\{\{i',j\} \in E(\pi(G))\} | \sigma^{(r)}, r\in[m])$.
	Then we can show that the contribution of $i$ for $G$ is still the same with that of $i'$ for $\pi(G)$. Therefore
	$P_{SSBM}(G | \sigma=\sigma^{(j)},j\in[m]) = P_{SSBM}(\pi(G) | \sigma=\sigma^{(j)},j\in[m])$
\end{proof}
\begin{lemma} \label{lm:qq}
	Let 
	$$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
	$$
	If there is a pair $i,i'\in[n]$ satisfying the following two conditions: (1) $\sigma_i^{(j)}=\sigma_{i'}^{(j)}$ for all $j\in[m]$ and (2) $X_i=-X_{i'}$, then it is not possible to distinguish the case $X_i=-X_{i'}=1$ from the case $X_i=-X_{i'}=-1$. In other words, conditioning on the samples, the posterior probability of the ground truth being $X$ is the same as that of the ground truth being $X^{(\sim\{i,i'\})}$.
\end{lemma}
\begin{proof}
We show the result for $m=1$, since different samples are independent, for $m>1$ the proof is similar. Let the graph $G'$ be the subgraph of $G$ after removing node $i$ and
$i'$. Denote $C(G) = \frac{1}{Z(\alpha, \beta)} \exp\Big(\beta \sum_{\{s,t\} \in E(G')} I(X_s, X_t) - \frac{\alpha \log n }{n} \sum_{\{s,t\} \not\in E(G')} I(X_s, X_t)\Big)$
and 
$$
\ell_G(s,t) = [\beta I(X_s, X_t) + \frac{\alpha \log n}{n} I(X_s, X_t)] \mathbbm{1}[\{i, j\}\in E(\pi(G))] - \frac{\alpha \log n}{n} I(X_s, X_t).
$$
For conditional probability we have
$ P(\sigma = \bar{\sigma} | \sigma=\sigma^{(1)}) = \sum_{G\in\cG_{[n]}} P_{SSBM}(G) P_{\sigma|G, \sigma=\sigma^{(1)}}(\sigma = \bar{\sigma} | \sigma=\sigma^{(1)}) $
Let the permutation $\pi$ be defined in Lemma \ref{lem:xpi}, 
Then 
\begin{align*}
P(\sigma = X | \sigma=\sigma^{(1)}) & = \sum_{G\in\cG_{[n]}} P_{SSBM}(G) P_{\sigma|G, \sigma=\sigma^{(1)}}(\sigma = X | \sigma=\sigma^{(1)}) \\
& = \sum_{G\in\cG_{[n]}} P_{SSBM}(\pi(G)) P_{\sigma|G, \sigma=\sigma^{(1)}}(\sigma = X | \sigma=\sigma^{(1)}) \\
& = \sum_{G\in\cG_{[n]}} P_{SSBM}(\pi(G)) C(G) \exp\Big(\sum_{s \in N_i(G)} \ell_G(i,s) + \sum_{s \in N_{i'}(G)} \ell_G(i',s) - \ell_G(i,i') \Big)\\
& = \sum_{G\in\cG_{[n]}} P_{SSBM}(\pi(G)) C(\pi(G)) \exp\Big(\sum_{s \in N_{\pi(i)}(\pi(G))} \ell_{\pi(G)}(\pi(i),s) + \sum_{s \in N_{i'}(G)} \ell(\pi(i),s) - \ell_{\pi(G)}(\pi(i'),\pi(i)) \Big)\\
&= \sum_{G\in\cG_{[n]}} P_{SSBM}(\pi(G)) P_{\sigma|\pi(G), \sigma=\sigma^{(1)}}(\sigma = X | \sigma=\sigma^{(1)}) \\
& = P(\sigma = X^{(\sim\{i,i'\})} | \sigma=\sigma^{(1)})
\end{align*}
\end{proof}
\end{document}

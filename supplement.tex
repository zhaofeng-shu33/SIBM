\documentclass{article}
\input{macros.tex}
\title{Supplementary Material to SIBM with 2 communities}
\author{Feng Zhao}
\begin{document}
\maketitle
In this supplementary material, we give some undocumented proof in \cite{ye2020exact}.
This document is not so useful since we have a general $k$ version. The following lemma assumes $k=2$.
\begin{lemma}\label{lem:ucBA}
Let $\cG_1:=\{G:B_i-A_i<0\text{~for all~}i\in[n]\}$. Suppose $i\neq j$, then $P(B_i-A_i + B_j - A_j= t\log(n)~|~G\in\cG_1)= (1+o(1))P(B_i-A_i + B_j - A_j= t\log(n))$ for all $t<0$ such that $t\log(n)$ is an integer.
\end{lemma}
\begin{lemma}\label{lem:BijG}
	Let $\beta < \beta^*$,
then \begin{equation} 
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) ~\big|~ G\in\cG_1 \big] 
= (1+o(1)) E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] ,
\end{equation}
\end{lemma}
\begin{proof}
First we have
\begin{align}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) ~\big|~ G\in\cG_1 \big] 
&= \sum_{t=-n}^{-2} P(B_i -A_i + B_j - A_j = t\log n | G \in \cG_1) \exp(2\beta t \log n) \notag \\
\text{ Lemma \ref{lem:ucBA} implies }&= (1+o(1))\sum_{t=-n}^{-2} P(B_i -A_i + B_j - A_j = t\log n) \exp(2\beta t \log n)
\end{align}
On the other hand, we suppose $X_i \neq X_j$ and we decompose $B_j = B'_j + \xi_{ij}, B_i = B'_i + \xi_{ij}$ where $\xi_{ij}$ is an indicator function of $\{i,j\} \in E(G)$. Then $B'_j, B'_i, A_j, A_i, \xi_{ij}$ are independent.
$\xi_{ij} \sim Bern(\frac{b\log n}{n})$.
Then we have
\begin{align*}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] & = E[\exp(4\beta \xi_{ij})] E[\exp(2\beta (B'_i - A_i)]
E[\exp(2\beta (B'_j - A_j)] \\
& = (1+o(1))E[\exp(2\beta(B'_i - A_i))] E[\exp(2\beta(B'_j - A_j))]
\end{align*}
Using the conclusion that
$$
E[\exp(2\beta(B_i - A_i))] = (1+o(1)) \sum_{t=-n/2}^{-1} P(B_i - A_i = t \log n)E[\exp(2\beta t \log n)]
$$
we have
\begin{align*}
E \big[  \exp\big(2\beta (B_i-A_i + B_j - A_j) \big) \big] & = (1+o(1))
\sum_{t_1=-n/2}^{-1} P(B'_i - A_i = t_1 \log n)E[\exp(2\beta t_1 \log n)] \\
& \cdot
\sum_{t_2=-n/2}^{-1} P(B'_j - A_j = t_2 \log n)E[\exp(2\beta t_2 \log n)] \\
& = (1+o(1))  \sum_{t=-n}^{-2} E[\exp(2\beta t \log n)]\sum_{\substack{t_1 + t_2 = t \\ t_1 < 0, t_2 < 0}} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2\log n)
\end{align*}
Since 
\begin{align*}
P(B_i -A_i + B_j - A_j = t\log n)
&= \sum_{\substack{t_1 + t_2 + t_3 = t\\ t_3 \in\{0, 1\}}} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2 \log n) P(2\xi_{ij} = t_3 \log n) \\
&=(1+o(1)) \sum_{t_1 + t_2 = t} P(B'_i - A_i = t_1 \log n) P(B'_j - A_j = t_2 \log n)  \\
\end{align*}
Since $P(G\in G_1) = 1-o(1)$, the above summation can be further restricted to $t_1 < 0, t_2 < 0$. Thus Lemma \ref{lem:BijG} follows. 
\end{proof}


\begin{lemma} \label{lm:qq}
	Let 
	$$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})\sim \SIBM(n,a\log(n)/n, b\log(n)/n,\alpha,\beta, m) .
	$$
	If there is a pair $i,i'\in[n]$ satisfying the following two conditions: (1) $\sigma_i^{(j)}=\sigma_{i'}^{(j)}$ for all $j\in[m]$ and (2) $X_i=-X_{i'}$, then it is not possible to distinguish the case $X_i=-X_{i'}=1$ from the case $X_i=-X_{i'}=-1$. In other words, conditioning on the samples, the posterior probability of the ground truth being $X$ is the same as that of the ground truth being $X^{(\sim\{i,i'\})}$, i.e.
	\begin{align}
	&P(X_i=1,X_j = \bar{X}_j \in \{\pm 1\},j \neq i,i' | X_i = -X_{i'}, \sigma_i^{(j)}=\sigma_{i'}^{(j)}, j\in[m]) \notag\\
	&  = P(X_i=-1,X_j = \bar{X}_j \in \{\pm 1\},j \neq i,i' | X_i = -X_{i'}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m])
	\end{align}
	Notice the condition $\sigma_i^{(j)}=\sigma_{i'}^{(j)}, j\in[m]$ is actually saying that we have already known $m$ samples $\sigma = \sigma^{(j)}$ and
	it happens that the $i$-th coordinate and $i'$-th coordinate of each sample are the same. 
\end{lemma}
\begin{proof}
Let $\bar{X}_i = 1, \bar{X}_{i'}=-1$ We only need to show
\begin{equation}\label{eq:12}
P(X=\bar{X}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m]) = P(X=\bar{X}^{\sim(i,i')}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m])
\end{equation}
Let $\cG_{[n]}$ be the set consisting of all the graphs with vertex set $[n]$.
A permutation $\pi\in S_n$ on the vertex set $[n]$ also induces a permutation on $\cG_{[n]}$: For $G\in\cG_{[n]}$, define the graph $\pi(G)\in\cG_{[n]}$ as the graph with the edge set $E(\pi(G))$ satisfying that $\{\pi(i),\pi(j)\}\in E(\pi(G))$ if and only if $\{i,j\}\in E(G)$.
It is easy to see that for any $\pi\in S_n$ and any $G\in\cG_{[n]}$,
$$
Z_G(\alpha,\beta)
=Z_{\pi(G)}(\alpha,\beta),
$$
%where $Z_G(\alpha,\beta)$ is defined in \eqref{eq:zg}.
We define a permutation $\pi$ such that $\pi(i) = i', \pi(i') = i$ and $\pi(j) = j$ for $j\neq i,i'$.
We can check that $\bar{X}_i=\bar{X}^{\sim(i,i')}_{\pi(i)}$ for all $i\in[n]$ holds.
$$
P_{\SSBM}(G  | X = \bar{X})=P_{\SSBM}(\pi(G) | X = \bar{X}^{\sim(i,i')})  ,
$$

Using the Markov property of $X \to G \to \sigma$, we have
\begin{align*}
P(X=\bar{X}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m]) & = \sum_{G \in \cG_{[n]}} P(X=\bar{X}) P_{\SSBM}(G  | X = \bar{X})\prod_{j=1}^m P_{\sigma | G}(\sigma =\sigma^{(j)})\\
\end{align*}
If we can show that 
\begin{equation}\label{eq:sigmaEqual}
P_{\sigma | G}(\sigma =\sigma^{(j)}) = P_{\sigma | \pi(G)}(\sigma =\sigma^{(j)})
\end{equation}
Then 
\begin{align*}
P(X=\bar{X}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m]) & = \sum_{G \in \cG_{[n]}} P(X=\bar{X}^{\sim(i,i')}) P_{\SSBM}(\pi(G)  | X = \bar{X}^{\sim(i,i')})\prod_{j=1}^m P_{\sigma | \pi(G)}(\sigma =\sigma^{(j)})\\
& = P(\bar{X}^{\sim(i,i')}, \sigma_i^{(j)} = \sigma_{i'}^{(j)}, j\in[m])
\end{align*}
The key to prove Equation \eqref{eq:sigmaEqual} lies at the property $\sigma^{(j)}_i = \sigma^{(j)}_{i'}$.
Let $c_G(\sigma, i,j) = (\beta + \frac{\alpha \log n}{n})\sigma_i \sigma_j \mathbbm{1}[\{i,j\}\in E(G)]  - \frac{\alpha \log n}{n}$
and $C_G(\sigma) = \frac{1}{Z_G(\alpha, \beta)}\exp(\sum_{j_1, j_2 \not\in \{i,i'\}} c_G(\sigma, j_1, j_2) - c_G(\sigma, i,i'))$
\begin{align*}
P_{\sigma | G}(\sigma =\sigma^{(j)}) & = C_G(\sigma^{(j)})\exp(\sum_{s=1}^n c_G(\sigma^{(j)}, i, s)
+ \sum_{s=1}^n c_G(\sigma^{(j)}, i',s) ) \\
& = C_{\pi(G)}(\sigma^{(j)})\exp(\sum_{j=s}^n c_G(\sigma^{(j)}, i, s)
+ \sum_{s=1}^n c_G(\sigma^{(j)}, i',s) ) \\
& = C_{\pi(G)}(\sigma^{(j)})\exp(\sum_{s=1}^n c_{\pi(G)}(\sigma^{(j)}, i', s)+ \sum_{s=1}^n c_{\pi(G)}(\sigma^{(j)}, i, s) ) \\
& = C_{\pi(G)}(\sigma^{(j)})\exp(\sum_{s=1}^n c_{\pi(G)}(\sigma^{(j)}, \pi(i), s) + \sum_{s=1}^n c_{\pi(G)}(\sigma^{(j)}, \pi(i'), s) ) \\
& = P_{\sigma | \pi(G)}(\sigma =\sigma^{(j)})
\end{align*}
\end{proof}
The following lemma assumes a general $k$.
Some notation: $\Lambda := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k-1\}$.
\begin{lemma}
	When $\dist(\bar{\sigma}, \mathbf{1}_n) \geq \frac{n}{\log^{\delta} n}$ where $0<\delta < 1$ and $\arg\,\min_{\sigma'\in \Lambda} \dist(\bar{\sigma}, \sigma') = \mathbf{1}_n$. Show that
	$P_{\sigma | G}(\sigma = \bar{\sigma} ) > \exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n)$
	happens with probability less than $\exp(-\tau(\alpha,\beta) n \log^{1-\delta} n )$ where $C$ is an arbitrary constant, $\tau(\alpha,\beta)$ is a positive number.
\end{lemma}
\begin{proof}
	Let $n_r = |\{\bar{\sigma}_i = w^r | i\in [n] \}|$. Then $n_0 \geq n_r$ for $r=1, \dots, k-1$.
	WLOG, suppose $n_0 \geq n_1 \dots \geq n_{k-1}$.
	We are concerned with the number $N_w = \frac{1}{2}(n(n-1) - \sum_{r=0}^{k-1} n_r(n_r-1))
	=\frac{1}{2}(n^2 - \sum_{r=0}^{k-1} n_r^2)$.
	Taking the $\log$ on both sides of $P_{\sigma | G}(\sigma = \bar{\sigma} ) > \exp(-Cn) P_{\sigma | G}(\sigma = \mathbf{1}_n)$ we can get
	\begin{equation}\label{eq:small}
	(\beta + \frac{\alpha \log n}{n}) \sum_{\bar{\sigma}_i  \neq \bar{\sigma}_j} Z_{ij} \leq \frac{\alpha \log n}{n} N_w + C n
	\end{equation}
	
	Firstly we estimate the order of $N_w$, obviously $N_w \leq \frac{1}{2} n^2$.
	Using the conclusion in Appendix A of \cite{yixin} we have
	\begin{equation}
	\sum_{r=0}^{k-1} n_r^2 \leq
	\begin{cases}
	n n_0 & n_0 \leq \frac{n}{2} \\
	n^2 - 2n_0(n-n_0) & n_0 > \frac{n}{2}
	\end{cases}
	\end{equation}
	We have $n_0 \leq n - \frac{n}{\log^{\delta} n}$ and $n_0 \geq \frac{n}{k}$. When $n_0 > \frac{n}{2}$ we take $n_0 = n - \frac{n}{\log^{1/3} n}$
	and we have $N_w \geq n_0 (n - n_0) = \frac{n^2}{\log^{\delta} n}(1+o(1))$. When $n_0 < \frac{n}{2}$ we take $n_0 = \frac{n}{2}$ such that
	$N_w \geq \frac{n^2}{4}$. So generally we have $\frac{n^2}{\log^{\delta} n}(1+o(1)) \leq N_w \leq \frac{1}{2}n^2$.
	The Bernoulli random variables are independent, taking either $Bern(\frac{a\log n}{n})$ or $Bern(\frac{b \log n}{n})$ depending on whether
	$X_i$ equals to $X_j$.
	Since $\frac{\log n}{n} N_w$ is dominated than $Cn$ we neglect inferior terms to rewrite \eqref{eq:small} as
	\begin{equation}
		\sum_{ \bar{\sigma}_i  \neq \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_w}{n}(1+o(1))
	\end{equation}
	Let $N_1$ be the number of random variables of $Z_{ij} \sim Bern(\frac{a\log n}{n})$.
	and $N_2 = N_w - N_1$ is that of $Z_{ij} \sim Bern(\frac{b\log n}{n})$.
	
	Using Chernoff Inequality we have
	\begin{align*}
	\Pr(\sum_{ \bar{\sigma}_i  = \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_w}{n})& \leq (E[\exp(-s Z_{ij})])^{N_1} (E[-exp(-s Z_{ij})])^{N_2} \exp(\frac{\alpha}{\beta} \frac{\log n N_w s}{n}(1+o(1))) \\
	&= \exp( \frac{\log n}{n}(1+o(1))(\exp(-s)-1)(aN_1 + bN_2)+\frac{\alpha}{\beta} \frac{\log n N_w s}{n}(1+o(1)))
	\end{align*}
	Since $s > 0$, we further have
	\begin{align*}
		\Pr(\sum_{ \bar{\sigma}_i  = \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_w}{n})
		& \leq \exp( \frac{N_w\log n }{n}(b(\exp(-s)-1)+ \frac{\alpha}{\beta}s + o(1))) 
	\end{align*}
	Let $h_b(x) = x - b -x\log \frac{x}{b}$, which satisfies $h_b(x) < 0$ for $0<x<b$,
	and take $s=-\log\frac{\alpha}{b\beta} > 0$, using 
	$N_w \geq \frac{n^2}{\log^{1/3} n}$ we have
	\begin{align*}
\Pr(\sum_{ \bar{\sigma}_i  = \bar{\sigma}_j } -Z_{ij} \geq -\frac{\alpha}{\beta}\frac{\log n N_w}{n})&\leq \exp( N_w \frac{\log n}{n} h_b(\frac{\alpha}{\beta})(1+o(1))) \\
		& \leq \exp (h_b(\frac{\alpha}{\beta}) n \log^{1-\delta} n (1+o(1)))
	\end{align*}
\end{proof}
\begin{lemma}
	For $SBM(2,n, \frac{a\log n}{n}, \frac{b \log n}{n})$, the exact recovery error rate $P(F)\asymp n^{2-(\sqrt{a} - \sqrt{b})^2}$ when we know that the ground truth community labels are balanced.If we allow unbalanced ground truth, possibly the rate is $ n^{1-(\sqrt{a} - \sqrt{b})^2/2} $.
\end{lemma}
\begin{proof}
	We follow Abbe's proof but enhance the inequality \cite{abbe}.
	Instead of using Equation (16) to scale up. We consider the maximum value of $f(k)= -2\log(2k)
	+4\frac{k}{n}\log n - (\frac{1}{2}- \frac{k}{n})4\epsilon \log n + 2$.
	We can verify that the minimum value of $f(k)$ is actually $f(1)$ for $1\leq k \leq \frac{n}{4}$.
	Therefore we have
	$ P(F) \leq \sum_{k=1}^{n/4} \exp(kf(1)) \leq \exp(f(1))$ where $f(1) < 0 $ for sufficient large $n$.
	$\exp(f(1)) = c n^{-2\epsilon}$ and let $\epsilon = \frac{(\sqrt{a}-\sqrt{b})^2}{2} - 1$.
\end{proof}
\begin{theorem}
	When $\alpha < b \beta $ and $0 < \delta < 1$, it is not solvable for any $m= O(\log^{\delta}(n))$ to
	exactly recover SIBM$(n,k,a \log(n)/n, b\log(n)/n ,\alpha, \beta, m)$.
\end{theorem}
\begin{proposition}
	The ML is a special case of index optimization algorithm with
	$\kappa = (\frac{(a - b)}{\log(a/b)} + o(1))\frac{\log n}{n}$.
	\begin{equation}\label{eq:kappa_opt_function}
	\max \sum_{(i,j) \in E} \delta_{\sigma_i\sigma_j} - \kappa \sum_{(i,j) \not\in E}  \delta_{\sigma_i\sigma_j} 
	\end{equation}
\end{proposition}
\begin{proof}
	Let $z_{ij} \in \{0, 1\}$ to represent whether there is an edge between two nodes in a graph. Then
	$$
	p(z | \sigma) = \prod_{\sigma_i = \sigma_j} p^{z_{ij}}
		(1-p)^{1-z_{ij}} \prod_{\sigma_i \neq \sigma_j} q^{z_{ij}}(1-q)^{1-z_{ij}}
	$$
	The log-likelyhood is
	\begin{align}
	\log p(z | \sigma) &= \log p \sum_{(i,j) \in E} \delta_{\sigma_i\sigma_j}
	+ \log(1-p) \sum_{(i,j)\in E} \delta_{\sigma_i\sigma_j} 
	+ \log q \sum_{(i,j) \not\in E} (1-\delta_{\sigma_i\sigma_j})
	+\log (1-q) \sum_{(i,j) \not\in E} (1-\delta_{\sigma_i\sigma_j}) \\
	& =C + \log\frac{p}{q} \sum_{(i,j) \in E} \delta_{\sigma_i\sigma_j}
	-\frac{1-q}{1-p}\sum_{(i,j) \not\in E} \delta_{\sigma_i\sigma_j}
	\end{align}
	Therefore, to maximize $\log p (z | \sigma)$ is equivalent to maximize \eqref{eq:kappa_opt_function}
	with $\kappa = \frac{\log((1-q)/(1-p))}{\log (p / q)}$ with $p=\frac{a}{b\log n}$ and
	$q = \frac{b\log n}{n}$. After expansion we have $\kappa_{ML} = (\frac{(a - b)}{\log(a/b)} + o(1))\frac{\log n}{n}$.
	Notice that the inequality
	$ \frac{(a - b)}{\log(a/b)}  > b$ holds. This is $\alpha > b \beta$.
	For Maximum Modularity, we have already $\kappa_{MQ} = \frac{a+b}{2}\frac{\log n}{n} > \kappa_{ML}$.
\end{proof}
\begin{proposition}
	Define the exact error rate as
	$r_1(x, \sigma) = \max_{i\in [n]} \delta(x_i \neq \pi(\sigma_i))$.
	The mis-matched ratio is defined in \cite{zhang} as
	$ r(x, \sigma) = \frac{1}{n} \sum_{i=1}^n \delta(x_i \neq \pi(\sigma_i))$
	We then have $ r(x, \sigma) \leq r_1(x, \sigma) \leq n r(x, \sigma)$.
\end{proposition}
\section{Extra Lemmas used in the proof}
\begin{lemma}
	For given $X, Y \in W^n$, suppose $|\{i | X_i = w^k \}| = \frac{n}{k}$.
	Let $f^*=\arg\min_{f \in \Gamma } d(f(Y), X)$, then $d(X,f^*(Y)) \leq \frac{k-1}{k}n$.
\end{lemma}
\begin{proof}
	We need to show that there exists a function $g\in \Gamma$ such that $\sum_{i=1}^n \mathbbm{1}[X_i=g(Y)_i] \geq \frac{n}{k}$.
	Now we construct the required function $g$ as follows:
	First we define $N_{ij} = |\{s | X_s = w^i, Z_s = w^j\}|$, $(i_1^*,j_1^*) = \arg\max N^{(1)}_{ij}$ and $S^{(1)}_i = \{i_1^*\}, S_j^{(1)}=\{j_1^*\}$.
	Then we give the recursive definition:
	$(i_r^*,j_r^*) = \arg\max_{i \not\in S_i^{(r-1)}, j \not\in S_j^{(r-1)}} N_{ij}$ and $S^{(r)}_i = S^{(r-1)}_i \cup \{i_r^*\}, S_j^{(r)}= S_j^{(r-1)}\cup \{j_r^*\}$ for $r=2, \dots, k$.
	The function $g$ is defined by $g(w^{i_r^*}) = g(w^{j_r^*})$. It is easy to show that $g$ is well-defined and belongs to $\Gamma$.
	On the other hand, $\sum_{i=1}^n \mathbbm{1}[X_i=g(Y)_i] = \sum_{i=1}^k N_{i_r^*,j_r^*}$.
	$N_{i_k^*, j_k^*} = \frac{n}{k} - \sum_{r=1, r\neq i_k^*}^k N_{r,j_k^*} = \frac{n}{k} - \sum_{r=1}^{k-1} N_{i^*_r,j_k^*} \geq \frac{n}{k} - \sum_{r=1}^{k-1} N_{i^*_r,j^*_r}$. Therefore,  $\sum_{i=1}^n \mathbbm{1}[X_i=g(Y)_i] \geq \frac{n}{k}$.
\end{proof}
\begin{lemma}\label{lem:thm23eq}
	\begin{align}
	\lfloor \frac{m+k-1}{k} \rfloor \beta>\beta^\ast \iff  & m\ge k \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor + 1 \label{eq:m2} \\
	\lfloor \frac{m+k-1}{k} \rfloor \beta <\beta^\ast  \iff & m <  m^* \textrm{ if } \beta^*/\beta \textrm{ is not an integer } \notag \\
	& m <  m^* - k \textrm{ if } \beta^*/\beta \textrm{ is an integer } \label{eq:m22}
	\end{align}
	So Theorem~\ref{thm:wt1} and Theorem~\ref{thm:wt2} give the same threshold.
\end{lemma}
\begin{proof}
	First, we give a proof of Equation \eqref{eq:m2}: $\lfloor \frac{m+k-1}{k} \rfloor \beta>\beta^\ast$
	implies that $\frac{\beta^\ast}{\beta}<\lfloor \frac{m+k-1}{k} \rfloor$.
	The smallest integer that is larger than $\frac{\beta^\ast}{\beta}$ is
	$\lfloor \frac{\beta^\ast}{\beta}\rfloor +1$,
	so $\lfloor \frac{\beta^\ast}{\beta} \rfloor + 1 \le \lfloor \frac{m+k-1}{k} \rfloor\le \frac{m+k-1}{k}$,
	and thus $m\ge k \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1$.
	Now assume $m\ge 2 \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1$,
	then $\frac{m-1}{2} \ge \lfloor \frac{\beta^\ast}{\beta} \rfloor$.
	Since the right hand side is an integer,
	we have $\lfloor \frac{m+1}{2} \rfloor = \lfloor \frac{m-1}{2} \rfloor +1 
	\ge \lfloor \frac{\beta^\ast}{\beta} \rfloor +1 >\frac{\beta^\ast}{\beta}$.
	
	Secondly, we show Equation \eqref{eq:m22}. If $\beta^\ast \over \beta$ is not an integer, then
	\begin{align*}
	& \lfloor \frac{m+k-1}{k} \rfloor < {\beta^\ast \over \beta}  \\
	\iff & \frac{m+k-1}{k}  < \lfloor{\beta^\ast \over \beta}\rfloor + 1 \\
	\iff & m < k \lfloor \frac{\beta^\ast}{\beta} \rfloor  + 1 = m^*
	\end{align*}
	If  $\beta^\ast \over \beta$ is an integer, then
	\begin{align*}
	& \lfloor  \frac{m+k-1}{k}  \rfloor < {\beta^\ast \over \beta}  \\
	\iff & \frac{m+k-1}{k}  < \lfloor{\beta^\ast \over \beta}\rfloor \\
	\iff & m <  m^* - k
	\end{align*}
\end{proof}
\begin{lemma}\label{lem:g_v_extension}
	Given a $k-1$ length vector $v=(u_1, \dots, u_{k-1})$, where $u_i$ is a non-negative integer.
	Let $u=\sum_{i=1}^{k-1} u_i$, $\sqrt{a} - \sqrt{b} > \sqrt{k}$.
	Define a function
	\begin{equation}\label{eq:gvbeta}
	g_v(\beta) = \frac{b\sum_{i=1}^{k-1}\exp(u_i \beta) + a \exp(-u\beta) - a - (k-1)b}{k}+1
	\end{equation}
	The function $g_v(\beta)$ has the following property:
	\begin{enumerate}
		\item $g_v(0)=1$ and $g_v(\beta)$ decreases from $[0, \bar{\beta}]$ and increases from
		$[\bar{\beta}, \infty)$, $g(\bar{\beta}) < 0$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	The monotonicity property comes from careful analysis on $g'_v$ and $g''_v>0$.
	We have already known that $\beta' = \frac{\beta^*}{u}$ satisfies:
	$$
	\frac{b e^{u\beta'} + a e^{-u\beta'}-a-b}{k}+1=0
	$$
	We compute $g_v(\beta')$:
	$$
	g_v(\beta') = \frac{b}{k}(2-k + \sum_{i=1}^{k-1}\exp(\frac{u_i}{u} \beta^*) - \exp(\beta^*))
	$$

	We use mathematical induction to show that $g_v(\beta') \leq 0$.
	Let $\lambda_i = \frac{u_i}{u} \in [0,1]$ and $\sum_{i=1}^{k-1} \lambda_i = 1$.
	If only one $\lambda_i = 1$, then $g_v(\beta')$.
	For $\lambda_i = 0, i=r,\dots, k-1$, if $r=2$, we have
	$$
	2-k + \sum_{i=1}^{k-1}\exp(\frac{u_i}{u} \beta^*) - \exp(\beta^*)
	= -(1-\exp(\lambda_1 \beta^*))(1-\exp(\lambda_2 \beta^*))
	$$
	Since $\exp(\lambda_i \beta^*) \geq 1$, the above equation is negative.
	Therefore, the conclusion holds when $r=2$.
	Now suppose it holds for a certain $r$, consider the case for $r+1$.
	We then have
	\begin{align*}
	L(r+1) = 2-k + \sum_{i=1}^{k-1}\exp(\frac{u_i}{u} \beta^*) - \exp(\beta^*)
	& = L(r)- (1-\exp(\sum_{i=1}^r \lambda_i \beta^*)) (1-\exp(\lambda_{r+1}\beta^*)) \leq 0
	\end{align*}
	Therefore, we have proven that $g_v(\beta') \leq 0$.
\end{proof}
\begin{remark}
	The function $g_v(\beta)$ reduces to common $g(\beta)$ if we choose $v=(1,0,\dots, 0)$.
\end{remark}
\begin{lemma}
	Suppose $u\beta > \beta^*$. For every $v=(u_1, \dots, u_{k-1})$,
	there exists a set $\cG_v$ such that $P(\cG_v) = 1-o(1)$ and for every $G\in \cG_v$,
	\begin{equation}
	\sum_{i=1}^n \exp(\beta(\sum_{r=1}^{k-1}u_i A_i^r - u A_i^0)) \leq n^{g_v(\beta)/2}
	\end{equation}
\end{lemma}
\begin{proof}
	We use Chernoff's Inequality to show $P_G(\sum_{i=1}^n \exp(\beta(\sum_{r=1}^{k-1}u_i A_i^r - u A_i^0)) \geq n^{g_v(\beta)/2}) = o(1)$.
	The expectation can be computed as follows:
	\begin{align*}
	\mathbb{E}[\exp(\beta(\sum_{r=1}^{k-1}u_i A_i^r - u A_i^0))] &=\prod_{i=1}^{k-1}\left(1-\frac{b\log n}{n}+\frac{b\log n}{n}\exp(u_i \beta)\right)^{n/k}
	\left(1-\frac{a\log n}{n}+\frac{a\log n}{n}\exp(-u \beta)\right)^{n/k} \\
	&= \exp(\frac{n}{k} \left[\sum_{i=1}^{k-1}\log(1-\frac{b\log n}{n}+\frac{b\log n}{n}\exp(u_i \beta)) + \log(1-\frac{a\log n}{n}+\frac{a\log n}{n}\exp(-u \beta))\right])\\
	& = \exp(\log n (g_v(\beta) - 1) + O(\frac{\log^2 n}{n}))
	\end{align*}
	By Chernoff's Inequality,
	\begin{align*}
	P_G(\sum_{i=1}^n \exp(\beta(\sum_{r=1}^{k-1}u_i A_i^r - u A_i^0)) \geq n^{g_v(\beta)/2}) 
	& \leq \mathbb{E}[\sum_{i=1}^n \exp(\beta(\sum_{r=1}^{k-1}u_i A_i^r - u A_i^0))]/n^{g_v(\beta)/2}\\
	& = n^{g_v(\beta)}
	\end{align*}
	From Lemma \ref{lem:g_v_extension}, we know that the left root of $g_v(\beta)$ is no larger than $\frac{\beta^*}{u}$.
	Therefore, if $\beta > \frac{\beta^*}{u}$, it is also larger than the left root of $g_v(\beta)$ and our conclusion holds.
\end{proof}
\begin{lemma}
	For $ t \in [u\frac{b-a}{k}, 0] $,
	we have
	$$
	P(\sum_{r=1}^{k-1} u_i A_i^r - u A_i^0 \geq t \log n) \leq \exp(\log(n) (f_{v,\beta}(t) - \beta t - 1) + O(\frac{\log n}{n}))
	$$.
	The function $f_{v,\beta}(t)$ satisfies $f_{v, \beta} \leq \tilde{g}_v(\beta)$.
	When $\beta > \beta^*$, we have $f_{v, \beta} \leq \tilde{g}_v(\beta) < 0$.
\end{lemma}
\begin{proof}
	By some detailed analysis, $f_{v,\beta} = \min_{s \geq 0} (g_v(s) - st) + \beta t$.
	Since $ t\leq 0$, the minimum value is taken for $s\leq \bar{\beta}$. Therefore, we can replace $g_v(s)$ with $\tilde{g}_v(s)$ defined
	as follows:
	\begin{equation}
	\tilde{g}_v(s) = \begin{cases}
	g_v(s) & s < \bar{\beta} \\
	g_v(\bar{\beta}) & s \geq \bar{\beta}\\
	\end{cases}
	\end{equation}
	That is, $f_{v,\beta} = \min_{s \geq 0} (\tilde{g}_v(s) - st) + \beta t \leq \tilde{g}_v(\beta) - \beta t + \beta t = \tilde{g}_v(\beta)$.
\end{proof}

\section{Alternative proof for Proposition 4 in \cite{ye2020exact}}
Let
the event $D_r : = \sum_{i=1}^n\exp ( 2\beta(A^r_i-A^0_i)) > s$
and we are going to give an upper bound of $P(D_r)$. We proceed as follows: 
\begin{align*}
&\Pr(D_r) = 
\Pr(D_r| B_i - A_i \geq 0, \exists i\in [n])
\cdot I_1 \\
&+ \Pr(D_r | B_i - A_i  < 0, \forall i\in [n])
\Pr(  B_i - A_i  < 0 , \forall i \in [n] ) \\
& \leq I_1
+ \Pr(D_r | B_i - A_i  < 0, \forall i\in [n])
\end{align*}
where $I_1 = \Pr( B_i - A_i \geq 0, \exists i\in [n])$.
To bound the two terms above, we need the following lemma, which can be proved by standard Chernoff inequality techniques:

Choosing $t=0$ in Proposition 5 of \cite{ye2020exact}, we have
$\Pr(B_i-A_i\ge 0 ) \leq \exp(-\log n \frac{(\sqrt{a}-\sqrt{b})^2}{2})$.
Then
\begin{align}\label{eq:I_1}
I_1 \leq \sum_{i=1}^n \Pr( B_i - A_i \geq 0) \leq n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{2}}
\end{align}
For the second term,
conditioned on $B_i - A_i  < 0, \forall i\in [n]$ we have
\begin{align}
\mathbb{E}[\sum_{i=1}^n\exp (2 \beta(B_i - A_i))|B_i - A_i < 0]
& = \sum_{i=1}^n \sum_{t\log n =-\frac{n}{2}}^{-1}P(B_i - A_i = t \log n) \exp ( 2\beta  t\log n) \label{eq:split_tlogn} \\ 
& \leq
\sum_{i=1}^n \sum_{t\log n =\tau}^{-1}P(B_i - A_i = t \log n)\exp ( 2\beta  t\log n) + n^{1+\beta(b-a)}
\end{align}
where $\tau =\frac{b-a}{2}\log n$ in short. Therefore
\begin{align*}
&\Pr(D_r | B_i - A_i  < 0, \forall i\in [n])  \\
&\leq\Pr(\sum_{i=1}^n\exp (2 \beta(B_i - A_i))  > s | B_i - A_i < 0) \\
& \leq \mathbb{E}[\sum_{i=1}^n\exp (2 \beta(B_i - A_i))|B_i - A_i < 0] / s \\
& \leq (\sum_{i=1}^n \sum_{t\log n =\tau}^{-1}P(B_i - A_i = t \log n)\exp ( 2\beta  t\log n) + n^{1+\beta(b-a)})/ s \\
& \leq (\frac{a-b}{2}\log n \cdot n^{f_{\beta}(t) + o(1)} + n^{(b-a)\beta + 1})/ s
\end{align*}

We have $f_{\beta}(t) \leq \tilde{g}(\beta)$ for all $t\leq 0$ 
and $f_{\beta}(\frac{b-a}{2}) = (b-a)\beta + 1$.

By choosing $s = n^{\tilde{g}(\beta)/2}$ we have
\begin{align*}
\Pr( D_r) \leq  n^{1-\frac{(\sqrt{a}-\sqrt{b})^2}{2}} + O(\log n)  \cdot n^{\tilde{g}(\beta)/2 + o(1)} \leq 2n^{\tilde{g}(\beta)/4}
\end{align*}
\section{A weaker conclusion than Theorem 3}
In section \cite{ye2020exact}, there is some problem related with the proof. We try to prove a weaker version of Theorem 3.
That is, when $\beta < \beta^*$, we will show that $P_{\SIBM}(\dist(\sigma, \pm X)\geq 1) = 1 - o(1)$.
The following discussion assumes $\dist(\sigma, X) \leq \frac{n}{2}$.
Indeed, using proposition 10 in \cite{ye2020exact}, we can find $\cG_1$ such that $P(\cG_1) < n^{1-(\sqrt{a} - \sqrt{b})^2/2}$
and for $G\not\in \cG_1$, for large enough $n$
$$
\sum_{i=1}^n \exp(2\beta(B_i - A_i)) \geq \frac{1}{2}n^{g(\beta)}
$$
For such $G$, we already have
$$
\frac{P_{\sigma | G}(\dist(\sigma, X) = 1)}{P_{\sigma | G}(\sigma = X)} = (1+o(1))\sum_{i=1}^n \exp(2\beta(B_i - A_i)) \geq \frac{1}{4}n^{g(\beta)}
$$
Therefore, we can get
$$
\frac{P_{\sigma | G}(\dist(\sigma, X) \geq 1)}{1-P_{\sigma | G}(\dist(\sigma, X)\geq 1)}\geq \frac{1}{4}n^{g(\beta)}
$$
We then have
$$
P_{\sigma | G}(\dist(\sigma, X) \geq 1) \geq \frac{n^{g(\beta)}}{4+n^{g(\beta)}}
$$
Then
\begin{align*}
P_{\SIBM}(\dist(\sigma, X) = 0) & \leq P(\cG_1) + \sum_{G\not\in cG_1}P(G)(1-P_{\sigma|G}(\dist(\sigma, X)\geq 1))\\
& \leq n^{1-(\sqrt{a} - \sqrt{b})^2/2} + \frac{4}{4+n^{g(\beta)}} \\
& \leq n^{1-(\sqrt{a} - \sqrt{b})^2/2} + 4n^{-g(\beta)} = o(1)
\end{align*}
\begin{thebibliography}{1}
	

	\bibitem{ye2020exact}
	Min Ye.
	\newblock Exact recovery and sharp thresholds of stochastic ising block model,
	2020.
	\bibitem{yixin}
	Chen, Yuxin, Changho Suh, and Andrea J. Goldsmith. "Information recovery from pairwise measurements." *IEEE Transactions on Information Theory* 62.10 (2016): 5881-5905.
	\bibitem{abbe}
	Abbe, Emmanuel, Afonso S. Bandeira, and Georgina Hall. "Exact recovery in the stochastic block model." *IEEE Transactions on Information Theory* 62.1 (2015): 471-487.
	\bibitem{zhang}
	Zhang, Anderson Y., and Harrison H. Zhou. "Minimax rates of community detection in stochastic block models." The Annals of Statistics 44.5 (2016): 2252-2280.
\end{thebibliography}

\end{document}

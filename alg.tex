\documentclass{ctexart}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Var}{Var}
\newcommand{\A}{\frac{a \log(n)}{n}}
\newcommand{\B}{\frac{b \log(n)}{n}}
\begin{document}
	\section{Introduction}
	\section{Stochastic Ising BlockModel}\label{sec:sibm}
	We consider a symmetric SBM model parameterized by $(n, k, p, q)$ where $p=\frac{a\log n}{n}, q=\frac{b\log n }{n}$.
	
	Let $W=\{1, \omega, \dots, \omega^{k-1}\}$ be a cyclic group with order $k$.
	Define an Ising model on a graph $G=([n],E(G))$ with parameters $\alpha,\beta>0$
	as the probability distribution on the configurations $\sigma\in W^n$ such that
	\begin{equation} \label{eq:isingma}
	P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
	\exp\Big(\beta\sum_{\{i,j\}\in E(G)} I(\bar{\sigma}_i ,\bar{\sigma}_j)
	-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j)\Big) ,
	\end{equation}
	where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and 
	$Z_G(\alpha,\beta)$
	is the normalizing constant. The indicator function is defined as
	$$
	I(x ,y) = \begin{cases}
	1 & x = y\\
	0 & x \neq y
	\end{cases}
	$$
	
	\section{Metropolis sampling algorithm for SIBM}
	In this section, we give an algorithm based on SIBM to do community detection tasks.
	For a SBM with $k-$communities, in previous section we have shown that the sample of SIBM is the same with the label $X$ up to some permutation. 
	Therefore, if we can get a sample from SIBM, then we can use this sample as the estimator of $X$.
	
	The difficulty is that we cannot generate the sample exactly from its distribution \eqref{eq:isingma}. Therefore, some approximation is
	necessary. The most common way to generate a Ising sample is using Metropolis sampling \cite{metropolis1953equation}.
	In this article, we use Metropolis sampling to generate SIBM sample.
	
	From equation \eqref{eq:isingma}, we know that for $k$-state Ising model, the Hamiltonian can be written as:
	\begin{equation}
	H(\bar{\sigma}) = \frac{\alpha}{\beta} \frac{\log n}{n} \sum_{\{i,j\}\not\in E(G)} \delta(\bar{\sigma}_i, \bar{\sigma}_j)
		- \sum_{\{i,j\}\in E(G)} \delta(\bar{\sigma}_i, \bar{\sigma}_j)
	\end{equation}
	Note: $\delta$ is the indicator function.
	
	Then Ising model generates sample according to $\exp(-\beta H(\bar{\sigma}))$. The physical meaning of $\beta$ is the inverse temperature.
	
	Suppose one coordinate $\bar{\sigma}_r$ flips to $w^s \cdot \bar{\sigma}_r$, the new state is denoted as $\bar{\sigma}^{(\sim \{r\}, v)}$.
	Then the change of energy is:
	\begin{equation}\label{eq:delta_H}
	 H(\bar{\sigma}^{(\sim \{r\}, [\omega^s])}) - H(\bar{\sigma}) = \sum_{i \in N_r(G)} J_s(\bar{\sigma}_r, \bar{\sigma}_i) - \frac{\alpha \log n}{\beta n} \sum_{i \in V\backslash N_r(G)\cup\{r\}} J_s(\bar{\sigma}_r,\bar{\sigma}_i)
	\end{equation}
	where $J_s(\sigma_i, \sigma_j)$ is defined as:
	\begin{equation}
	J_s(\sigma_i, \sigma_j) = \begin{cases}
	1 & \sigma_i = \sigma_j \\
	-1 & \omega^s \sigma_i = \sigma_j \\
	0 & \textrm {otherwise}
	\end{cases}
	\end{equation}
	\begin{remark}
	If we require $\bar{\sigma}_i \in \{\pm 1\}$ and use $\bar{\sigma}_i \cdot \bar{\sigma}_j$ replacing
	the indicator function  $\delta(\bar{\sigma},\bar{\sigma}_j)$ in \eqref{eq:delta_H}, the energy will be
	$$
	H(\bar{\sigma}) = \frac{\alpha}{\beta} \frac{\log n}{n} \sum_{\{i,j\}\not\in E(G)}\bar{\sigma}_i \cdot \bar{\sigma}_j
		- \sum_{\{i,j\}\in E(G)}\bar{\sigma}_i \cdot \bar{\sigma}_j
	 $$
	 suppose $\bar{\sigma}'$ is a state with one flop from $\bar{\sigma}$ at position $i$. That is
	 $$
	 \begin{cases}
	 \bar{\sigma}'_i &= -\bar{\sigma}_i \\
	 \bar{\sigma}'_j & = \bar{\sigma}_j \textrm{ for } j \neq i
	 \end{cases}
	 $$
	 Then the energy difference:
	 \begin{align*}
	 H(\bar{\sigma}') - H(\bar{\sigma}) &= 2[\sum_{j\in N_i(G)}\bar{\sigma}_i \cdot \bar{\sigma}_j - \frac{\alpha \log n}{\beta n}
	 \sum_{\substack{j\not\in N_i(G) \\ j\neq i}} \bar{\sigma}_i \cdot \bar{\sigma}_j ] \\
	 & = 2 [\sum_{j\in N_i(G)}(1+\frac{\alpha \log n}{\beta n})\bar{\sigma}_i \cdot \bar{\sigma}_j - \frac{\alpha \log n}{\beta n}
	 \sum_{j\neq i} \bar{\sigma}_i \cdot \bar{\sigma}_j ] 
	 \end{align*}
	 We use the conclusion that there are $\frac{n}{2} \bar{\sigma}_i$ takes value $1$ and $\frac{n}{2}\bar{\sigma}_i$ with value
	 $-1$. Therefore, $\sum_{j\neq i}]cdot \bar{\sigma}_i \bar{\sigma}_j = -1$.
	 On the other hand:
	 $$
	 \frac{P_{\sigma | G}(\sigma = \bar{\sigma'})}{P_{\sigma | G}(\sigma = \bar{\sigma})}
	 = \frac{\exp(-\beta H(\bar{\sigma}'))}{\exp(-\beta H(\bar{\sigma}))} = \exp(-\beta(H(\bar{\sigma}')-H(\bar{\sigma})))
	 $$
	 By some simplification we can get the same result as \cite{ye2020exact} in Section III.A:
	 $$
	 \frac{P_{\sigma |G }(\sigma = X^{(\sim i)})}{P_{\sigma |G }(\sigma = X)}
	 = \exp(2(\beta + \frac{\alpha \log n}{n})(B_i - A_i)-\frac{2\alpha \log n}{n})
	 $$
	\end{remark}
	The pseudo code of our algorithm is summarized in \ref{alg:m}. This algorithm requires the number of the communities $k$ to be known and the weight parameter $\frac{\alpha}{\beta}$ is given. We use the annealing techniques (cooling temperature) to increase $\beta$ at each outer loop, thus guarantee that $\beta > \beta^*$
	if $\epsilon \in (0, 1)$ and $N$ are large. 
	The maximum iteration time $N$ should also be specified.
	\begin{algorithm}[H]
		\caption{Metropolis sampling algorithm for SIBM} \label{alg:m}
		Inputs: the graph $G$ \\
		Output: $\hat{X}$
		\begin{algorithmic}[1]
			\STATE random initialize $\sigma \in W^n$
			\WHILE{the iteration time $<N$}
			\FOR{each node $r$}
			\STATE random choose a new flipping state $w^s \cdot \sigma_r$
			\STATE compute $\Delta H(r,s) = H(\bar{\sigma}^{(\sim \{r\}, [w^s])}) - H(\bar{\sigma})$ using Equation \eqref{eq:delta_H}
			\IF{$\Delta H(r,s)<0$}
			\STATE $\sigma_r \leftarrow w^s \cdot \sigma_r$
			\ELSE
			\STATE generate a random number $u$ ranged within $[0,1]$
			\IF{$u < \exp(-\beta \Delta H(r,s))$}
			\STATE $\sigma_r \leftarrow w^s \cdot \sigma_r$
			\ENDIF
			\ENDIF
			\ENDFOR
			\IF{$H(\bar{\sigma})$ does not change}
			\STATE \textbf{break}
			\ENDIF
			\STATE $\beta \leftarrow (1 + \epsilon)\cdot\beta$
			\ENDWHILE
		\end{algorithmic}
	\end{algorithm}
	Since the computation of $\Delta H(r,s)$ needs $O(n)$ time. The time complexity of Algorithm \ref{alg:m} is $Nn^2$.
	\section{Parameter Estimation for SBM}
	In Section \ref{sec:sibm}, we have shown that when $\alpha > b \beta$ and $\beta > \beta^*$. The sample of SIBM aligns with $X$ with probability one
	as $n\to \infty$. 
	% conjecture, both alpha and beta cannot be too large, otherwise the convergence of Metropolis sampling is very slow.
	The critical value $\beta^*$ is a function of $a$ and $b$. Therefore, to give good guarantee of Algorithm \ref{alg:m}, we need to estimate the parameter
	of $a,b$ before estimating the label of $X$.
	This estimation is done by counting the edges and triangles of the input graph $G$.
	Since the graph is random, the number of edges $T_1$ is also a random variable and its expectation is
	\begin{equation}
	E[T_1] = k \frac{n/k(n/k-1)}{2} p + \frac{k(k-1)}{2} (n/k)^2 q \sim \frac{n\log n}{2k} (a + (k-1) b)
	\end{equation}
	When $n \to \infty$ by the law of large numbers we can show that $\frac{T_1}{n \log n}$ converges to $\frac{a+(k-1)b}{2k}$ in probability.
	
	Similarly, for the number of triangles $T_2$ we can compute its expectation as:
	\begin{align}
	E[T_2] &= k \frac{n/k(n/k-1)(n/k-2)}{6}p^3 + 2\frac{k(k-1)}{2}(n/k) \frac{n/k(n/k-1)}{2} pq^2 + \binom{k}{3} \left(\frac{n}{k}\right)^3 q^3 \\
	&\sim \frac{(\log n)^3}{k^2}
	(\frac{a^3}{6} + \frac{k-1}{2}ab^2 + (k-1)(k-2)\frac{b^3}{6})
	\end{align}
	When $n$ is large, we can also show that $\frac{T_2}{(\log n)^3}$ converges to $\frac{1}{k^2}
	(\frac{a^3}{6} + \frac{k-1}{2}ab^2)$. Therefore we can get the equation array about $a$ and $b$:
	\begin{align}
	\frac{a+(k-1)b}{2k} & = e_1 \label{eq:e_1}\\
	\frac{1}{k^2}
	\left(\frac{a^3}{6} + \frac{k-1}{2}ab^2 + (k-1)(k-2)\frac{b^3}{6}\right) & = e_2 \label{eq:e_2}
	\end{align}	
	
	Solving this equation and use the assumption that $ a > b, 0 < a < e_1$ we can get the unique solution of $(a,b)$.
	This is our parameter estimation method.
	\section{Connection with Modularity Based Method}
	To simplify our analysis, we consider $k=2$ in this section. Our conclusion is that for SBM with large $n$, modularity method
	is a special model of SIBM. The $\alpha, \beta$ are fixed values, which is not optimal for the specific SBM problem. We use theoretical
	analysis and simulation to verify our declaration.

	Now we compute modularity for SBM$(n,k, a\frac{\log n}{n}, b\frac{\log n}{n})$.
	The standard formula is
	\begin{equation}
	Q = \frac{1}{2m} \sum_{ij} (A_{ij} - \frac{d_i d_j}{2m}) \delta(C_i, C_j)
	\end{equation}
	The standard way to do community detection is to maximize the modularity $Q$ using greedy method.
	Suppose we use $Q$ in the definition of the numerator of Ising model. When $n$ is very large,
	we have $d_i = \frac{\log n(a+b)}{2}, m = \frac{1}{2}n d_i$. Therefore we have $\beta = \frac{1}{2m}(1-\frac{\log n}{2n}(a+b))
	\sim \frac{1}{2m}$
	and $\alpha = \frac{1}{2m}\frac{a+b}{2}$. Since $a>b$. We have $\alpha > b \beta$, which is a necessary condition for finite sample complexity.
	However, due to the scaling effect, we can not get the conclusion on the critical value $\beta^*$.
	
	When $\alpha > b \beta$, we can do an optimization problem:
	\begin{align}\label{eq:opsig}
	\max_{\sigma} \,\,& \beta \sum_{\{i,j\} \in E(G)} \sigma_i \sigma_j - \frac{\alpha \log n}{n} \sum_{\{i,j\} \in E(G)} \sigma_i \sigma_j\\
	s.t. \,\, & \sigma \in \{\pm 1\}^n
	\end{align}
	If the above equation can be solved exactly, then the solution is the true label with probability 1 as $n\to \infty$.
	
	This fact is easy to show using SIBM theory. we can always scaling the object function such that $\beta > \beta^*$.
	Then the sample from SIBM is $\pm X$ with probability 1 and it is equivalent to do the above optimization problem.
	This conclusion also verifies that Modularity maximization method is asymptotic optimal for SBM.
	
	For the given optimization problem in Equation \eqref{eq:opsig}, we can let $\beta = 1$ as given for modularity maximization method (we omit the scaling term $\frac{1}{2m}$).
	Then our condition is $\alpha > b$. The choice of $\alpha$ for modularity method is $\frac{a+b}{2}$. We want to
	investigate whether this choice is optimal for some approximation solution of modularity maximization.
	
	We can use simulation annealing to search for the optimal value of Equation \eqref{eq:opsig}. This method has been tried
	by \cite{he2016fast}. The earliest literature for adoption is \cite{liu2010detecting}, empirically combined with other techniques to improve
	the performance.
	\section{SDP Algorithms Proofs}
	\begin{lemma}\label{lem:Cabd}
	Suppose $C$ is a following symmetric matrix with dim = $2n$.
	$$C=\begin{bmatrix} d & \text{ } &  a & \vrule & \text{ } & \text{ }  & \text{ } \\
\text{ } & \ddots & \text{ } & \vrule & \text{ }&  b & \text{ } \\
a & \text{ } & d & \vrule & \text{ } & \text{ } & \text{ } \\ \hline
 \text{ } & \text{ }  & \text{ } & \vrule & d & \text{ } & a \\
 \text{ }& b & \text{ }  & \vrule & \text{ } & \ddots & \text{ } \\
\text{ } & \text{ } & \text{ }& \vrule & a & \text{ } & d \\
\end{bmatrix}$$,
then the characteristic polynomial function is $(\lambda - d + a)^{2n-2} (\lambda - d + a - na - nb)
 (\lambda - d + a - na + nb)$. $d + (n-1)a+ nb$ corresponds to the all-one eigenvector while 
 $d +(n-1)a -nb$ corresponds to the eigenvector $g=(1,\dots, 1, -1, \dots, -1)$.
\end{lemma}
\begin{proof}
Let $d' = d-a, e$ is the all-one vector, then the matrix $\lambda I_{2n} - C$ can be written in block form:
$$
\lambda I_{2n} - C = \begin{bmatrix}
(\lambda -d') I_n - a ee^T & -b ee^T \\
-bee^T & (\lambda -d') I_n - a ee^T
\end{bmatrix}
$$
Using Woodbury matrix inverse we can compute
$$
[(\lambda -d') I_n - a ee^T]^{-1} = (\lambda - d')^{-1} [I_n - \frac{a}{na - \lambda + d'} ee^T]
$$
Using the block matrix formula for determinate, i.e.
$$
\begin{vmatrix}
A & B \\
C & D
\end{vmatrix} = |D| \times |A-BD^{-1}C|
$$
we further have
$$
|\lambda I_{2n} - C| = |(\lambda -d') I_n - a ee^T| |(\lambda - d')I_n + \kappa ee^T|
$$
where
\begin{align*}
\kappa &= -a - \frac{b^2}{\lambda - d'} e^T (I_n - \frac{a}{na-\lambda + d'}ee^T)e \\
&= -a  - \frac{b^2 n}{\lambda - na - d'}
\end{align*}
Further for matrix of the form $(x-y)I_n + yee^T$, its determant can be computed explictly
as $|(x-y)I_n + yee^T|=(x-y)^{n-1}(x+(n-1)y)$ (See Example 1.5.9 of \cite{ad_li}.
Therefore, we can finally get the characteristic function:
$$
|\lambda I_{2n} - C| = (\lambda -d')^{2n-2} [(\lambda - d' - na)^2-n^2b^2]
$$
\end{proof}
\section{Proofs of Parameter Estimation}
	\begin{lemma}
		Suppose $e_1, e_2$ is defined from (\ref{eq:e_1}, \ref{eq:e_2}), Then the following equation array has unique solution $x=a, y=b$
		\begin{align}
	\frac{x+(k-1)y}{2k} & = e_1 \\
\frac{1}{k^2}
\left(\frac{x^3}{6} + \frac{k-1}{2}xy^2 + (k-1)(k-2)\frac{y^3}{6}\right) & = e_2	
		\end{align}
	\end{lemma}
\begin{proof}
	Using $x=2ke_1 - (k-1)y$ we can get
	\begin{equation}
	g(y): = (k-1)(y^3 - 6 e_1 y^2 + 12 e_1^2 y) + 6 e_2 - 8 k e_1^3 = 0
	\end{equation}
	This equation has unique real root since $g(y)$ is increasing on $\mathbb{R}$. $g'(y) = 3(k-1)(y-2e_1)^2 \geq 0 $
	Next we show the root lies within $(0, 2e_1)$.
	\begin{align*}
	g(0) &= \frac{1}{k^2}\left(-3(k-1)(k-2)ab^2 + (k-1)((k-2)-(k-1)^2)b^3\right) < 0 \\
	g(2e_1) &= 6e_2 - 8e_1^3 = \frac{(k-1)(a-b)^3}{k^3} > 0
	\end{align*}
	Therefore, we can get a unique solution $y$ within $(0, 2e_1)$. Since $(a,b)$ is a solution for the equation array. The conclusion follows.
	
\end{proof}
	\begin{lemma}\label{lem:ERT}
	In an ER graph parameterized by $(n,p)$ and $p=\frac{a\log n}{n}$, the number of edges is denoted by $T(G)$, then
	$\frac{T}{n \log n} \to \frac{a}{2}$ in probability.
	% Erdős–Rényi
	\end{lemma}
\begin{proof}
	Let $X_{ij}$ represents a Bernoulli random variable with parameter $p$. Then $T(G) = \sum_{i,j} X_{ij}$, $X_{ij}$ are i.i.d.
	$E[T(G)] = \frac{n(n-1)}{2}p = \frac{(n-1)\log n}{2}a$ and $\Var[T(G)] = \frac{n(n-1)}{2} p(1-p) < a\frac{(n-1)\log n}{2}$
	Then by Chebyshev's inequality
	$$
	\Pr(\Big|\frac{T}{n \log n } - \frac{a}{2} \frac{n-1}{n}\Big| > \epsilon) \leq \frac{\Var[T /(n \log n )]}{\epsilon^2} < \frac{a(n-1)}{2n^2\epsilon^2\log n}
	$$
	For a given $\epsilon$, when $n$ is sufficiently large,
	$$
	\Pr(\Big|\frac{T}{n \log n } - \frac{a}{2} \Big| > \epsilon) < \Pr(\Big|\frac{T}{n \log n } - \frac{a}{2} \frac{n-1}{n}\Big| > 2\epsilon)
	\leq \frac{n-1}{8n^2 \epsilon^2 \log n}
	$$
	Therefore, by the definition of convergence in probability, we have $\frac{T}{n \log n} \to \frac{a}{2}$ as $n\to \infty$.
\end{proof}
\begin{lemma}\label{lem:ER_tr_counting}
	In an ER graph parameterized by $(n,p)$ and $p=\frac{a\log n}{n}$, the number of triangles is denoted by $T(G)$, then
	$\frac{T}{(\log n)^3} \to \frac{a^3}{6}$ in probability.
	% Erdős–Rényi
\end{lemma}
\begin{proof}
	Let $X_{ijk}$ represents a Bernoulli random variable with parameter $p^3$.
	Then $T(G) = \sum_{i,j,k} X_{ijk}$.
	It is easy to compute that $E[T] = \binom{n}{3}p^3$.
	From \cite{holland1977method} we know that
	% Mordern version: https://stats.stackexchange.com/questions/338267/distribution-and-variance-of-count-of-triangles-in-random-graph
	$$
	\Var[T] = \binom{n}{3} p^3 + 12 \binom{n}{4} p^5 + 30 \binom{n}{5} p^6 + 20 \binom{n}{6} p^6 - \binom{n}{3}^2 p^6 = O(\log^3 n)
	$$
	Therefore
	by Chebyshev's inequality
	$$
	\Pr(\Big|\frac{T}{\log^3 n } - \frac{a^3}{6} \frac{(n-1)(n-2)}{n^2}\Big| > \epsilon) \leq \frac{\Var[T /\log^3 n ]}{\epsilon^2} 
	= \frac{1}{\epsilon^2}O(\frac{1}{\log^3 n})
	$$
	Similar to Lemma \ref{lem:ERT} we can show that the convergence of $\frac{T}{\log^3 n}$ to $\frac{a^3}{6}$.
\end{proof}
\begin{lemma}\label{lem:SBM_tr_counting_cross}
	Consider a 2 community SBM $(2n, p, q)$ and count the number of triangles $T$ which has a node in $S_1$ and an edge in $S_2$.
	Then the variance of $T$ is
	\begin{equation}\label{eq:SBM_tr_counting_cross}
	\Var[T] = \frac{n^2(n-1)}{2}q^2p + n^2(n-1)(n-2)p^2q^3 +\frac{n^2(n-1)^2}{2}q^4p - \frac{n^2(n-1)(3n-4)}{2}q^4 p^2
	\end{equation}
\end{lemma}
\begin{proof}
	$T=\sum_{i,j,k} Y_{ijk}$ where the summation is over $i \in S_1$ and $j \in S_2$. Therefore, $E[T] = n\binom{n}{2}pq^2$
	To compute the variance of $T$ we need to compute $E[T^2] = \sum_{i,j,k}\sum_{i',j',k'} Y_{ijk}Y_{i'j'k'}$.
	There are 6 cases for $E[Y_{ijk}Y_{i'j'k'}]$
	\begin{enumerate}
		\item $\{i,j,k\} = \{i',j',k'\}$ then $E[Y_{ijk}Y_{i'j'k'}] = pq^2$.
		There are $n\binom{n}{2}$ such terms in the double sum.
		\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 1 element in common and the element $\in S_1$, then $E[Y_{ijk}Y_{i'j'k'}] = p^2q^4$.
		There are $6n\binom{n}{4}$ such terms in the double sum.
		\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 1 element in common and the element $\in S_2$, then $E[Y_{ijk}Y_{i'j'k'}] = p^2q^4$.
		There are $12\binom{n}{2}\binom{n}{3}$ such terms in the double sum.
		\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 2 element in common and the two elements are both $\in S_2$, then $E[Y_{ijk}Y_{i'j'k'}] = pq^4$.
There are $2\binom{n}{2}\binom{n}{2}$ such terms in the double sum.		
\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 2 element in common and one is in $S_1$, the other is in $S_2$, then $E[Y_{ijk}Y_{i'j'k'}] = p^2q^3$.
There are $6n\binom{n}{3}$ such terms in the double sum.		
\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have 0 element in common, then $E[Y_{ijk}Y_{i'j'k'}] = p^2q^4$.
There are $2\binom{n}{2}\binom{n}{4}$ such terms in the double sum.		
	\end{enumerate}
To verify we have covered all cases, note that the sum adds up to $(n\binom{n}{2})^2$:
% Mathematica Code:
% Simplify[n^2 (n-1)/2 + 12 Binomial[n,2] Binomial[n,3] + 6 n Binomial[n,4] + Binomial[n,4] Binomial[n,2]*12 + 6 n Binomial[n,3]+ 2 Binomial[n,2]^2]
$$
n\binom{n}{2} + 6n\binom{n}{4} + 12\binom{n}{2} \binom{n}{3} + 2(\binom{n}{2})^2 + 6n\binom{n}{3} + 2\binom{n}{2}\binom{n}{4} = \left(n\binom{n}{2}\right)^2
$$
\end{proof}
\begin{lemma}\label{lem:SBM_tr_counting_3}
	Consider a 3 community SBM$(3n, p, q)$ and count the number of triangles $T$ which has a node in $S_1$, one node in $S_2$ and one node in $S_3$.
	Then the variance of $T$ is
	\begin{equation}\label{eq:SBM_tr_counting_three}
	\Var[T] = q^3 n^3 + 3q^4 n^3(n-1) + 3q^5 n^3 (n-1)^2 - n^3(3n^2-3n+1)q^6
	\end{equation}
\end{lemma}
\begin{proof}
	Similar to Proof of Lemma \ref{lem:SBM_tr_counting_cross}.
	$E[T] = n^3 q^3$ and there are 4 cases
	\begin{enumerate}
	\item $\{i,j,k\} = \{i',j',k'\}$ then $E[Y_{ijk}Y_{i'j'k'}] = q^3$.
	There are $n^3$ such terms in the double sum.
	\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 1 element in common, then $E[Y_{ijk}Y_{i'j'k'}] = q^5$.
	There are $12n\binom{n}{2}^2$ such terms in the double sum.
	\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 2 element in common, then $E[Y_{ijk}Y_{i'j'k'}] = q^4$.
	There are $6\binom{n}{2}n^2$ such terms in the double sum.
	\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have 0 element in common, then $E[Y_{ijk}Y_{i'j'k'}] = q^6$.
	There are $8\binom{n}{2}^3$ such terms in the double sum.		
\end{enumerate}	
\end{proof}
\begin{theorem}
	For a SBM$(n, k, p, q)$ where $p=\frac{a\log n}{n}, q = \frac{b\log n}{n}$. The number of triangles is $T$.
	Then $\frac{T}{(\log n)^3}$ converges to $\frac{1}{k^2}(\frac{a^3}{6} + \frac{k-1}{2}ab^2 + (k-1)(k-2)\frac{b^3}{6} )$ in probability as $n \to \infty$.
\end{theorem}
\begin{proof}
	We split $T$ into three parts, the first is the number of triangles within community $i$, $T_i$. There are $k$ terms of $T_i$.
	The second is the number of triangles which have one node in community $i$ and one edge in community $j$, $T_{ij}$. There are $k(k-1)$ terms of $T_{ij}$. The third is the number of triangles which have one node in community $i$, one node in community $j$ and one node in community $k$.
	
	We only need to show that
	\begin{align}
	\frac{T_i}{\log ^3 n} &\to \frac{(a/k)^3}{6} \\
	\frac{T_{ij}}{\log^3 n}& \to \frac{1}{2}(a/k)(b/k)^2\\
	\frac{T_{ijk}}{\log^3 n} & \to (b/k)^3
	\end{align}
	The convergence of $\frac{T_i}{\log ^3 n}$ comes from Lemma \ref{lem:ER_tr_counting}.
	For $T_{ij}$ we use the conclusion from Lemma \ref{lem:SBM_tr_counting_cross}.
	We replace $n$ with $n/k$, $p=a\frac{\log n}{n}$, $q=b\frac{\log n}{n}$ in Equation \eqref{eq:SBM_tr_counting_cross}.
	$\Var[T_{ij}] \sim \frac{ab^2}{2k^3} \log^3 n$. Since the expectation of $\frac{T_{ij}}{\log^3 n}$ is $(n/k)\binom{n/k}{2}pq^2/(\log^3 n)
	=\frac{n-1}{n}\frac{ab^3}{k^3}$. By Chebyshev's inequality we can show that 
	$$
	\Pr( \Big|\frac{T_{ij}}{\log^3 n} - \frac{n-1}{n}\frac{ab^3}{k^3} \Big| > \epsilon) \leq \frac{\Var[T_{ij} / \log^3 n]}{\epsilon^2} = \frac{1}{\epsilon^2}
	O(\frac{1}{\log^3 n})
	$$
	Therefore, $\frac{T_{ij}}{\log^3 n} $ converges to $\frac{1}{2}(a/k)(b/k)^2$
	
	To prove $\frac{T_{ijk}}{\log^3 n}\to (b/k)^3$, from Lemma \ref{lem:SBM_tr_counting_3} we can get $\Var[T_{ijk}] = O(\log^5 n)$
$$
\Pr( \Big|\frac{T_{ijk}}{\log^3 n} -\frac{b^3}{k^3} \Big| > \epsilon) \leq \frac{\Var[T_{ijk} / \log^3 n]}{\epsilon^2} = \frac{1}{\epsilon^2}
O(\frac{1}{\log n})
$$
\end{proof}
\section{SDP for two community}
\subsection{ADMM scheme}
Let $B_{ij} = 1$ if $(i,j) \in E(G)$ and $B_{ij} = -\kappa$ if $(i,j) \not \in E(G)$.
We require $0<\kappa \leq 1$. When $\kappa = 1$, the matrix $B$ is the same with
Abbe's formulation in \cite{abbe2015exact}.
To solve $\max Tr(BX)$ s.t. $X_{ii} = 1$ and $X \succeq 0$.
We use ADMM, that is, we solve
\begin{align*}
\min & - \langle B, X\rangle + \delta_{Z \succeq 0} + \delta_{L(X) = b}\\
s.t.& X=Z
\end{align*}
$L$ is a mapping from $\mathbb{R}^{n \times n} \to \mathbb{R}^n$ such that $[L(X)]_i =  \langle X,F_i \rangle $.
$F_i$ is a matrix with element $(i,i)$ equal to 1 and 0 elsewhere.
The projection $\Pi_{A}(Y)$ simply modifies the diagonal elements of $Y$ to $1$.
The scaled form of ADMM \cite{boyd2011distributed} is given by
\begin{align}
X^{k+1} = \Pi_A(Z^k - U^k + \frac{1}{\rho}B) \\
Z^{k+1} = \Pi_{S_+^n}(X^{k+1} + U^{k}) \\
U^{k+1} = U^k + (X^{k+1} - Z^{k+1}) 
\end{align}
\subsection{Therotical Guarantee}
We have already know that for $\kappa = \frac{\alpha \log n}{\beta n}$, the problem to maximize
$\sum_{(i,j)\in E(G)} \sigma_i \sigma_j - \kappa \sum_{(i,j) \not\in E(G)} \sigma_i \sigma_j$ can be written in matrix form
as 
\begin{align}
\max \, & \sigma^T B \sigma \label{eq:sigmaB}\\
s.t. & \sigma_i \in \{\pm 1\} \notag
\end{align}
We can use semi-definite relaxation to solve this integer programming problems.
That is, we consider
\begin{align}
\max \,& Tr(BX) \label{eq:semiD}\\
s.t. & X_{ii} = 1 \notag \\
&  X \succeq 0 \notag
\end{align}
By following the same procedure in Abbe's paper \cite{abbe2015exact}, we can get the sufficient condition that the optimal solution
to \eqref{eq:semiD} is the ground truth vector $g$.
\begin{theorem}
If $(a - b)^2 > 8 (a+b)$, then with high probability \eqref{eq:semiD} has a unique solution $g$.
\end{theorem}
The proof is similar with that of Abbe's paper. We choose the conjugate vector $Y=\diag(Bgg^T)$ and show that 
we need to guarantee:  $Y \succeq B$ and $\lambda_2(Y-B)>0$.
$g$ is the eigenvector for $Y-B$, with the corresponding eigenvalue equal to zero.
The modification of $\alpha_{ij}^+, \alpha_{ij}^-$ is as follows:
\begin{align}
\alpha^+_{ij}=\begin{cases} 1 & \text{wp }\A \\ -\kappa & \text{wp } 1-\A \end{cases}\\
\alpha^-_{ij}=\begin{cases} 1 & \text{wp }\B \\ -\kappa & \text{wp } 1-\B \end{cases}
\end{align}
The expression of $C, \Gamma$ are modified in corresponding way:
\begin{align}
C &=  \sum_{i<j, j \in S(i)}\left(2\A-1\right)\Delta^+_{ij}+\sum_{i<j, j \notin S(i)} \left(2\B-1\right) \Delta^-_{ij}\\
\Gamma &=  \sum_{i<j, j \in S(i)}\left(\left(2\A-1\right) - \alpha^{+}_{ij}\right) \cdot \Delta^+_{ij} + \sum_{i<j, j \notin S(i)} \left(\left(2\B-1\right) - \alpha^{-}_{ij}\right) \cdot \Delta^-_{ij}
\end{align}
$j\in S(i)$ means that $i,j$ belongs to the same community.

There is some minor problems when using projection in Abbe's paper. We can overcome the problem by consider
the event $\forall u \perp g, u^T(C-\Gamma)u \leq 0$, the probability of this event is bounded by
$$P(\min_{u \perp g, ||u||_2=1} u^T C u \leq \max_{||v||_2=1} v^T \Gamma v)$$.
For the determistic matrix $C$, we have
\begin{align}
a &= \kappa - (1+\kappa) \A \\
b &= \kappa - (1+\kappa) \B \\
d & = a + \frac{1+\kappa}{2} (a-b) \log n
\end{align}
Using Lemma \ref{lem:Cabd}, we can get all eigenvalues of $C$ as $\lambda_2=\frac{1+\kappa}{2}(a-b)\log n $ and $\lambda_1=n\kappa - (1+\kappa)b\log n$
(omit the eigenvalue corresponding to $g$). If $\kappa = 0$, then $C$ has negative eigenvalue and it is impossible for
$C-\Gamma \succeq 0 $ to hold
with large probability. The minimum requirement for this is that $\kappa > b\frac{\log n}{n}$.
This condition has been verified previously.
$\min_{u \perp g, ||u||_2=1} u^T C u = \min\{\lambda_1, \lambda_2\} := t$
Using matrix Bernstein inequality (Theorem 5 in \cite{abbe2015exact}, the matrix norm is the spectral norm), we need to computer $\sigma^2$ and $R$. Actually, $R$ is the minor term.
For example, we can take $R = 4\A$. Only $\sigma^2$ needs to be computed carefully.
We have
$$
\sigma^2 = (\kappa+1)^2 \log n(a (1-\A) + b (1-\B))
$$
Therefore,
\begin{align*}
&P(\lambda_{\max}( \Gamma v) \geq t)\\
& \leq n\cdot \exp \left(
\frac{t^2}{(\kappa+1)^2\log n (a(1-\A) + b(1-\B)) + \frac{2a(1+\kappa)(a-b)}{3}\frac{\log^2 n}{n} }
\right)\\
& \leq n^{-\epsilon}
\end{align*}
where $\epsilon > 0$ by choosing $t^2 > (\kappa + 1)^2 (a+b)$.
If $\kappa \geq \frac{a+b}{2}\frac{\log n}{n}$ Then the condition is equivalent to $(a-b)^2 > 8(a+b)$
\bibliographystyle{plain}
	\bibliography{exportlist.bib}
\end{document}
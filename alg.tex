\documentclass{article}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\DeclareMathOperator{\Var}{Var}
\begin{document}
	\section{Introduction}
	\section{Stochastic Ising BlockModel}\label{sec:sibm}
	We consider a symmetric SBM model parameterized by $(n, k, p, q)$ where $p=\frac{a\log n}{n}, q=\frac{b\log n }{n}$.
	
	Let $W=\{1, \omega, \dots, \omega^{k-1}\}$ be a cyclic group with order $k$.
	Define an Ising model on a graph $G=([n],E(G))$ with parameters $\alpha,\beta>0$
	as the probability distribution on the configurations $\sigma\in W^n$ such that
	\begin{equation} \label{eq:isingma}
	P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
	\exp\Big(\beta\sum_{\{i,j\}\in E(G)} I(\bar{\sigma}_i ,\bar{\sigma}_j)
	-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j)\Big) ,
	\end{equation}
	where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and 
	$Z_G(\alpha,\beta)$
	is the normalizing constant. The indicator function is defined as
	$$
	I(x ,y) = \begin{cases}
	k-1 & x = y\\
	-1 & x \neq y
	\end{cases}
	$$
	
	\section{Metropolis sampling algorithm for SIBM}
	In this section, we give an algorithm based on SIBM to do community detection tasks.
	For a SBM with $k-$ communities, in previous section we have shown that the sample of SIBM is the same with the label $X$ up to some permutation. 
	Therefore, if we can get a sample from SIBM, then we can use this sample as the estimator of $X$.
	
	The difficulty is that we cannot generate the sample exactly from its distribution \eqref{eq:isingma}. Therefore, some approximation is
	necessary. The most common way to generate a Ising sample is using Metropolis sampling \cite{metropolis1953equation}.
	In this article, we use Metropolis sampling to generate SIBM sample.
	
	From equation \eqref{eq:isingma}, we know that for $k$-state Ising model, the Hamiltonian can be written as:
	\begin{equation}
	H(\bar{\sigma}) = \frac{\alpha}{\beta} \frac{\log n}{n} \sum_{\{i,j\}\not\in E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j)
		- \sum_{\{i,j\}\in E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j)
	\end{equation}
	Suppose one coordinate $\bar{\sigma}_r$ flips to $w^s \cdot \bar{\sigma}_r$, the new state is denoted as $\bar{\sigma}^{(\sim \{r\}, v)}$.
	Then the change of energy is:
	\begin{equation}\label{eq:delta_H}
	 H(\bar{\sigma}^{(\sim \{r\}, [\omega^s])}) - H(\bar{\sigma}) = k \sum_{i \in N_r(G)} J_s(\bar{\sigma}_r, \bar{\sigma}_i) - k\frac{\alpha \log n}{\beta n} \sum_{i \in V\backslash N_r(G)} J_s(\bar{\sigma}_r,\bar{\sigma}_i)
	\end{equation}
	where $J_s(\sigma_i, \sigma_j)$ is defined as:
	\begin{equation}
	J_s(\sigma_i, \sigma_j) = \begin{cases}
	1 & \sigma_i = \sigma_j \\
	-1 & \omega^s \sigma_i = \sigma_j \\
	0 & \textrm {otherwise}
	\end{cases}
	\end{equation}
	The pseudo code of our algorithm is summarized in \ref{alg:m}. This algorithm requires the number of the communities $k$ to be known and the weight parameter $\alpha, \beta$ is given. The iteration time $N$ should also be specified.
	\begin{algorithm}[H]
		\caption{Metropolis sampling algorithm for SIBM} \label{alg:m}
		Inputs: the graph $G$ \\
		Output: $\hat{X}$
		\begin{algorithmic}[1]
			\STATE random initialize $\sigma \in W^n$
			\WHILE{the iteration time $<N$}
			\FOR{each node $r$}
			\STATE random choose a new flipping state $w^s \cdot \sigma_r$
			\STATE compute $\Delta H(r,s) = H(\bar{\sigma}^{(\sim \{r\}, [w^s])}) - H(\bar{\sigma})$ using Equation \eqref{eq:delta_H}
			\IF{$\Delta H(r,s)<0$}
			\STATE $\sigma_r \leftarrow w^s \cdot \sigma_r$
			\ELSE
			\STATE generate a random number $u$ ranged within $[0,1]$
			\IF{$u < \exp(-\beta H(r,s))$}
			\STATE $\sigma_r \leftarrow w^s \cdot \sigma_r$
			\ENDIF
			\ENDIF
			\ENDFOR
			\ENDWHILE
		\end{algorithmic}
	\end{algorithm}
	Since the computation of $\Delta H(r,s)$ needs $O(n)$ time. The time complexity of Algorithm \ref{alg:m} is $Nn^2$.
	\section{Parameter Estimation for SBM}
	In Section \ref{sec:sibm}, we have shown that when $\alpha > b \beta$ and $\beta > \beta^*$. The sample of SIBM aligns with $X$ with probability one
	as $n\to \infty$. 
	% conjecture, both alpha and beta cannot be too large, otherwise the convergence of Metropolis sampling is very slow.
	The critical value $\beta^*$ is a function of $a$ and $b$. Therefore, to give good guarantee of Algorithm \ref{alg:m}, we need to estimate the parameter
	of $a,b$ before estimating the label of $X$.
	This estimation is done by counting the edges and triangles of the input graph $G$.
	Since the graph is random, the number of edges $T_1$ is also a random variable and its expectation is
	\begin{equation}
	E[T_1] = k \frac{n/k(n/k-1)}{2} p + \frac{k(k-1)}{2} (n/k)^2 q \sim \frac{n\log n}{2k} (a + (k-1) b)
	\end{equation}
	When $n \to \infty$ by the law of large numbers we can show that $\frac{T_1}{n \log n}$ converges to $\frac{a+(k-1)b}{2k}$ in probability.
	
	Similarly, for the number of triangles $T_2$ we can compute its expectation as:
	\begin{align}
	E[T_2] &= k \frac{n/k(n/k-1)(n/k-2)}{6}p^3 + 2\frac{k(k-1)}{2}(n/k) \frac{n/k(n/k-1)}{2} pq^2 + \binom{k}{3} \left(\frac{n}{k}\right)^3 q^3 \\
	&\sim \frac{(\log n)^3}{k^2}
	(\frac{a^3}{6} + \frac{k-1}{2}ab^2 + (k-1)(k-2)\frac{b^3}{6})
	\end{align}
	When $n$ is large, we can also show that $\frac{T_2}{(\log n)^3}$ converges to $\frac{1}{k^2}
	(\frac{a^3}{6} + \frac{k-1}{2}ab^2)$. Therefore we can get the equation array about $a$ and $b$:
	\begin{align}
	\frac{a+(k-1)b}{2k} & = e_1 \label{eq:e_1}\\
	\frac{1}{k^2}
	\left(\frac{a^3}{6} + \frac{k-1}{2}ab^2 + (k-1)(k-2)\frac{b^3}{6}\right) & = e_2 \label{eq:e_2}
	\end{align}	
	
	Solving this equation and use the assumption that $ a > b, 0 < a < e_1$ we can get the unique solution of $(a,b)$.
	This is our parameter estimation method.
	\section{Connection with Modularity Based Method}
	To simplify our analysis, we consider $k=2$ in this section. Our conclusion is that for SBM with large $n$, modularity method
	is a special model of SIBM. The $\alpha, \beta$ are fixed values, which is not optimal for the specific SBM problem. We use theoretical
	analysis and simulation to verify our declaration.

	Now we compute modularity for SBM$(n,k, a\frac{\log n}{n}, b\frac{\log n}{n})$.
	The standard formula is
	\begin{equation}
	Q = \frac{1}{2m} \sum_{ij} (A_{ij} - \frac{d_i d_j}{2m}) \delta(C_i, C_j)
	\end{equation}
	The standard way to do community detection is to maximize the modularity $Q$ using greedy method.
	Suppose we use $Q$ in the definition of the numerator of Ising model. When $n$ is very large,
	we have $d_i = \frac{\log n(a+b)}{2}, m = \frac{1}{2}n d_i$. Therefore we have $\beta = \frac{1}{2m}(1-\frac{\log n}{2n}(a+b))
	\sim \frac{1}{2m}$
	and $\alpha = \frac{1}{2m}\frac{a+b}{2}$. Since $a>b$. We have $\alpha > b \beta$, which is a necessary condition for finite sample complexity.
	However, due to the scaling effect, we can not get the conclusion on the critical value $\beta^*$.
	
	When $\alpha > b \beta$, we can do an optimization problem:
	\begin{align}\label{eq:opsig}
	\max_{\sigma} \,\,& \beta \sum_{\{i,j\} \in E(G)} \sigma_i \sigma_j - \frac{\alpha \log n}{n} \sum_{\{i,j\} \in E(G)} \sigma_i \sigma_j\\
	s.t. \,\, & \sigma \in \{\pm 1\}^n
	\end{align}
	If the above equation can be solved exactly, then the solution is the true label with probability 1 as $n\to \infty$.
	
	This fact is easy to show using SIBM theory. we can always scaling the object function such that $\beta > \beta^*$.
	Then the sample from SIBM is $\pm X$ with probability 1 and it is equivalent to do the above optimization problem.
	This conclusion also verifies that Modularity maximization method is asymptotic optimal for SBM.
	
	For the given optimization problem in Equation \eqref{eq:opsig}, we can let $\beta = 1$ as given for modularity maximization method (we omit the scaling term $\frac{1}{2m}$).
	Then our condition is $\alpha > b$. The choice of $\alpha$ for modularity method is $\frac{a+b}{2}$. We want to
	investigate whether this choice is optimal for some approximation solution of modularity maximization.
	
	We can use simulation annealing to search for the optimal value of Equation \eqref{eq:opsig}. This method has been tried
	by \cite{he2016fast}. The earliest literature for adoption is \cite{liu2010detecting}, empirically combined with other techniques to improve
	the performance.
	
	\appendix
	\section{Proofs}
	\begin{lemma}
		Suppose $e_1, e_2$ is defined from (\ref{eq:e_1}, \ref{eq:e_2}), Then the following equation array has unique solution $x=a, y=b$
		\begin{align}
	\frac{x+(k-1)y}{2k} & = e_1 \\
\frac{1}{k^2}
\left(\frac{x^3}{6} + \frac{k-1}{2}xy^2 + (k-1)(k-2)\frac{y^3}{6}\right) & = e_2	
		\end{align}
	\end{lemma}
\begin{proof}
	Using $x=2ke_1 - (k-1)y$ we can get
	\begin{equation}
	g(y): = (k-1)(y^3 - 6 e_1 y^2 + 12 e_1^2 y) + 6 e_2 - 8 k e_1^3 = 0
	\end{equation}
	This equation has unique real root since $g(y)$ is increasing on $\mathbb{R}$. $g'(y) = 3(k-1)(y-2e_1)^2 \geq 0 $
	Next we show the root lies within $(0, 2e_1)$.
	\begin{align*}
	g(0) &= \frac{1}{k^2}\left(-3(k-1)(k-2)ab^2 + (k-1)((k-2)-(k-1)^2)b^3\right) < 0 \\
	g(2e_1) &= 6e_2 - 8e_1^3 = \frac{(k-1)(a-b)^3}{k^3} > 0
	\end{align*}
	Therefore, we can get a unique solution $y$ within $(0, 2e_1)$. Since $(a,b)$ is a solution for the equation array. The conclusion follows.
	
\end{proof}
	\begin{lemma}\label{lem:ERT}
	In an ER graph parameterized by $(n,p)$ and $p=\frac{a\log n}{n}$, the number of edges is denoted by $T(G)$, then
	$\frac{T}{n \log n} \to \frac{a}{2}$ in probability.
	% Erdős–Rényi
	\end{lemma}
\begin{proof}
	Let $X_{ij}$ represents a Bernoulli random variable with parameter $p$. Then $T(G) = \sum_{i,j} X_{ij}$, $X_{ij}$ are i.i.d.
	$E[T(G)] = \frac{n(n-1)}{2}p = \frac{(n-1)\log n}{2}a$ and $\Var[T(G)] = \frac{n(n-1)}{2} p(1-p) < a\frac{(n-1)\log n}{2}$
	Then by Chebyshev's inequality
	$$
	\Pr(\Big|\frac{T}{n \log n } - \frac{a}{2} \frac{n-1}{n}\Big| > \epsilon) \leq \frac{\Var[T /(n \log n )]}{\epsilon^2} < \frac{a(n-1)}{2n^2\epsilon^2\log n}
	$$
	For a given $\epsilon$, when $n$ is sufficiently large,
	$$
	\Pr(\Big|\frac{T}{n \log n } - \frac{a}{2} \Big| > \epsilon) < \Pr(\Big|\frac{T}{n \log n } - \frac{a}{2} \frac{n-1}{n}\Big| > 2\epsilon)
	\leq \frac{n-1}{8n^2 \epsilon^2 \log n}
	$$
	Therefore, by the definition of convergence in probability, we have $\frac{T}{n \log n} \to \frac{a}{2}$ as $n\to \infty$.
\end{proof}
\begin{lemma}\label{lem:ER_tr_counting}
	In an ER graph parameterized by $(n,p)$ and $p=\frac{a\log n}{n}$, the number of triangles is denoted by $T(G)$, then
	$\frac{T}{(\log n)^3} \to \frac{a^3}{6}$ in probability.
	% Erdős–Rényi
\end{lemma}
\begin{proof}
	Let $X_{ijk}$ represents a Bernoulli random variable with parameter $p^3$.
	Then $T(G) = \sum_{i,j,k} X_{ijk}$.
	It is easy to compute that $E[T] = \binom{n}{3}p^3$.
	From \cite{holland1977method} we know that
	% Mordern version: https://stats.stackexchange.com/questions/338267/distribution-and-variance-of-count-of-triangles-in-random-graph
	$$
	\Var[T] = \binom{n}{3} p^3 + 12 \binom{n}{4} p^5 + 30 \binom{n}{5} p^6 + 20 \binom{n}{6} p^6 - \binom{n}{3}^2 p^6 = O(\log^3 n)
	$$
	Therefore
	by Chebyshev's inequality
	$$
	\Pr(\Big|\frac{T}{\log^3 n } - \frac{a^3}{6} \frac{(n-1)(n-2)}{n^2}\Big| > \epsilon) \leq \frac{\Var[T /\log^3 n ]}{\epsilon^2} 
	= \frac{1}{\epsilon^2}O(\frac{1}{\log^3 n})
	$$
	Similar to Lemma \ref{lem:ERT} we can show that the convergence of $\frac{T}{\log^3 n}$ to $\frac{a^3}{6}$.
\end{proof}
\begin{lemma}\label{lem:SBM_tr_counting_cross}
	Consider a 2 community SBM $(2n, p, q)$ and count the number of triangles $T$ which has a node in $S_1$ and an edge in $S_2$.
	Then the variance of $T$ is
	\begin{equation}\label{eq:SBM_tr_counting_cross}
	\Var[T] = \frac{n^2(n-1)}{2}q^2p + n^2(n-1)(n-2)p^2q^3 +\frac{n^2(n-1)^2}{2}q^4p - \frac{n^2(n-1)(3n-4)}{2}q^4 p^2
	\end{equation}
\end{lemma}
\begin{proof}
	$T=\sum_{i,j,k} Y_{ijk}$ where the summation is over $i \in S_1$ and $j \in S_2$. Therefore, $E[T] = n\binom{n}{2}pq^2$
	To compute the variance of $T$ we need to compute $E[T^2] = \sum_{i,j,k}\sum_{i',j',k'} Y_{ijk}Y_{i'j'k'}$.
	There are 6 cases for $E[Y_{ijk}Y_{i'j'k'}]$
	\begin{enumerate}
		\item $\{i,j,k\} = \{i',j',k'\}$ then $E[Y_{ijk}Y_{i'j'k'}] = pq^2$.
		There are $n\binom{n}{2}$ such terms in the double sum.
		\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 1 element in common and the element $\in S_1$, then $E[Y_{ijk}Y_{i'j'k'}] = p^2q^4$.
		There are $6n\binom{n}{4}$ such terms in the double sum.
		\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 1 element in common and the element $\in S_2$, then $E[Y_{ijk}Y_{i'j'k'}] = p^2q^4$.
		There are $12\binom{n}{2}\binom{n}{3}$ such terms in the double sum.
		\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 2 element in common and the two elements are both $\in S_2$, then $E[Y_{ijk}Y_{i'j'k'}] = pq^4$.
There are $2\binom{n}{2}\binom{n}{2}$ such terms in the double sum.		
\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 2 element in common and one is in $S_1$, the other is in $S_2$, then $E[Y_{ijk}Y_{i'j'k'}] = p^2q^3$.
There are $6n\binom{n}{3}$ such terms in the double sum.		
\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have 0 element in common, then $E[Y_{ijk}Y_{i'j'k'}] = p^2q^4$.
There are $2\binom{n}{2}\binom{n}{4}$ such terms in the double sum.		
	\end{enumerate}
To verify we have covered all cases, note that the sum adds up to $(n\binom{n}{2})^2$:
% Mathematica Code:
% Simplify[n^2 (n-1)/2 + 12 Binomial[n,2] Binomial[n,3] + 6 n Binomial[n,4] + Binomial[n,4] Binomial[n,2]*12 + 6 n Binomial[n,3]+ 2 Binomial[n,2]^2]
$$
n\binom{n}{2} + 6n\binom{n}{4} + 12\binom{n}{2} \binom{n}{3} + 2(\binom{n}{2})^2 + 6n\binom{n}{3} + 2\binom{n}{2}\binom{n}{4} = \left(n\binom{n}{2}\right)^2
$$
\end{proof}
\begin{lemma}\label{lem:SBM_tr_counting_3}
	Consider a 3 community SBM$(3n, p, q)$ and count the number of triangles $T$ which has a node in $S_1$, one node in $S_2$ and one node in $S_3$.
	Then the variance of $T$ is
	\begin{equation}\label{eq:SBM_tr_counting_three}
	\Var[T] = q^3 n^3 + 3q^4 n^3(n-1) + 3q^5 n^3 (n-1)^2 - n^3(3n^2-3n+1)q^6
	\end{equation}
\end{lemma}
\begin{proof}
	Similar to Proof of Lemma \ref{lem:SBM_tr_counting_cross}.
	$E[T] = n^3 q^3$ and there are 4 cases
	\begin{enumerate}
	\item $\{i,j,k\} = \{i',j',k'\}$ then $E[Y_{ijk}Y_{i'j'k'}] = q^3$.
	There are $n^3$ such terms in the double sum.
	\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 1 element in common, then $E[Y_{ijk}Y_{i'j'k'}] = q^5$.
	There are $12n\binom{n}{2}^2$ such terms in the double sum.
	\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have exactly 2 element in common, then $E[Y_{ijk}Y_{i'j'k'}] = q^4$.
	There are $6\binom{n}{2}n^2$ such terms in the double sum.
	\item If $\{i,j,k\}$ and $\{i',j',k'\}$ have 0 element in common, then $E[Y_{ijk}Y_{i'j'k'}] = q^6$.
	There are $8\binom{n}{2}^3$ such terms in the double sum.		
\end{enumerate}	
\end{proof}
\begin{theorem}
	For a SBM$(n, k, p, q)$ where $p=\frac{a\log n}{n}, q = \frac{b\log n}{n}$. The number of triangles is $T$.
	Then $\frac{T}{(\log n)^3}$ converges to $\frac{1}{k^2}(\frac{a^3}{6} + \frac{k-1}{2}ab^2 + (k-1)(k-2)\frac{b^3}{6} )$ in probability as $n \to \infty$.
\end{theorem}
\begin{proof}
	We split $T$ into three parts, the first is the number of triangles within community $i$, $T_i$. There are $k$ terms of $T_i$.
	The second is the number of triangles which have one node in community $i$ and one edge in community $j$, $T_{ij}$. There are $k(k-1)$ terms of $T_{ij}$. The third is the number of triangles which have one node in community $i$, one node in community $j$ and one node in community $k$.
	
	We only need to show that
	\begin{align}
	\frac{T_i}{\log ^3 n} &\to \frac{(a/k)^3}{6} \\
	\frac{T_{ij}}{\log^3 n}& \to \frac{1}{2}(a/k)(b/k)^2\\
	\frac{T_{ijk}}{\log^3 n} & \to (b/k)^3
	\end{align}
	The convergence of $\frac{T_i}{\log ^3 n}$ comes from Lemma \ref{lem:ER_tr_counting}.
	For $T_{ij}$ we use the conclusion from Lemma \ref{lem:SBM_tr_counting_cross}.
	We replace $n$ with $n/k$, $p=a\frac{\log n}{n}$, $q=b\frac{\log n}{n}$ in Equation \eqref{eq:SBM_tr_counting_cross}.
	$\Var[T_{ij}] \sim \frac{ab^2}{2k^3} \log^3 n$. Since the expectation of $\frac{T_{ij}}{\log^3 n}$ is $(n/k)\binom{n/k}{2}pq^2/(\log^3 n)
	=\frac{n-1}{n}\frac{ab^3}{k^3}$. By Chebyshev's inequality we can show that 
	$$
	\Pr( \Big|\frac{T_{ij}}{\log^3 n} - \frac{n-1}{n}\frac{ab^3}{k^3} \Big| > \epsilon) \leq \frac{\Var[T_{ij} / \log^3 n]}{\epsilon^2} = \frac{1}{\epsilon^2}
	O(\frac{1}{\log^3 n})
	$$
	Therefore, $\frac{T_{ij}}{\log^3 n} $ converges to $\frac{1}{2}(a/k)(b/k)^2$
	
	To prove $\frac{T_{ijk}}{\log^3 n}\to (b/k)^3$, from Lemma \ref{lem:SBM_tr_counting_3} we can get $\Var[T_{ijk}] = O(\log^5 n)$
$$
\Pr( \Big|\frac{T_{ijk}}{\log^3 n} -\frac{b^3}{k^3} \Big| > \epsilon) \leq \frac{\Var[T_{ijk} / \log^3 n]}{\epsilon^2} = \frac{1}{\epsilon^2}
O(\frac{1}{\log n})
$$
	
\end{proof}
	\bibliographystyle{plain}
	\bibliography{exportlist.bib}
\end{document}
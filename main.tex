\label{key}\documentclass[conference]{IEEEtran}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{theorem}{Theorem}%[section]
\newtheorem{definition}{Definition}%[section]
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\SIBM}{SIBM}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cG}{\mathcal{G}}
\usepackage{bbm} % provide mathbbm
\newcommand{\ide}[2]{ \delta_{#1 #2} }
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Binom}{Binom}

\title{Stochastic Ising Block Model on Multiple Communities}
\author{%
	\IEEEauthorblockN{Feng Zhao}
	\IEEEauthorblockA{Department of Electronic Engineering\\
	Tsinghua University\\ 
	Beijing, China 100084\\
	Email: zhaof17@mails.tsinghua.edu.cn}
	\and
	\IEEEauthorblockN{Min Ye}
	\IEEEauthorblockA{DSIT Research Center\\
	Tsinghua-Berkeley Shenzhen Institute\\
	Shenzhen, China 518055\\
	Email: yeemmi@sz.tsinghua.edu.cn}
	\and
	\IEEEauthorblockN{Shao-Lun Huang}
	\IEEEauthorblockA{DSIT Research Center\\
	Tsinghua-Berkeley Shenzhen Institute\\
	Shenzhen, China 518055\\
	Email: shaolun.huang@sz.tsinghua.edu.cn}
}
\begin{document}
\maketitle
\begin{abstract}
 In this paper, we will combine Stochastic Block Model (SBM) with Ising model and propose Stochastic Ising Block Model (SIBM) on multiple communities.
 We study the exact recovery problem for SIBM and compute the sample complexity for some properly chosen regime.
 Our result leads to the classical exact recovery condition $\sqrt{a} - \sqrt{b} > \sqrt{k}$ for SBM model.
 Besides, SIBM provides some theoretical guarantee for modularity maximization, which is a popular community detection method.
\end{abstract}
\section{Introduction}
% first paragraph: short intro to SBM and Ising model
In network analysis, Stochastic Block Model (SBM) is a commonly used random graph model, in which the probability of edge existence is higher within the community than between different communities \cite{holland1983stochastic}. For SBM, the condition to fully recover community labels has been investigated thoroughly and some phase transition property has been established \cite{Abbe17}. Meanwhile, Ising model is a well-known statistical model in physics which also has some kind of phase transition property \cite{ising1925beitrag}. The particles in Ising model interact with neighbors and change its state to achieve a steady state.
As a probabilistic model, Ising model is not limited to statistical physics and there are already some works to apply Ising model to investigate the social voting phenomenon \cite{sznajd2000opinion}.

In the original formulation, each particle in Ising model has only two states. Potts extended it to multiple states \cite{potts1952some}. This multiple-state Ising model
corresponds to SBM with multiple communities, thus enabling us to propose a concatenated network model: Stochastic Ising Block Model (SIBM). For 2-community case, there are already some results \cite{ye2020exact}, but little is known for multiple communities case. In this paper, we will give the formulation of SIBM on multiple
communities. We can generate multiple samples from SIBM, which more or less differ from the original labels, depending on the choice of parameters. Using the generated samples, we focus on the problem of whether it is possible to exactly recover the label. By investigating the phase transition property for our compose model,
we can compute the feasible regime of parameters for exact recovery and the corresponding sample complexity to accomplish the recovery goal.

This paper is divided as follows: In Section \ref{s:Preliminaries} the SIBM is formulated mathematically.
In Section \ref{s:trans}, the main results are introduced.
In Section \ref{s:conclusion}, the conclusion is given. Proof sketch of our main results is provided in Appendix.

% notation convetion
Within this paper, the number of community is denoted by $k$; $m$ is the number of samples; $\lfloor x \rfloor$ is the floor function of $x$; the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$;
the label of each node is a random variable $X_i$; $X_i$ is chosen from $W= \{1, \omega, \dots, \omega^{k-1}\}$ and we further require $W$
is a cyclic group with order $k$; $W^n$ is the n-ary Cartesian power of $W$; $f$ is a permutation function on $W$ and applied to $W^n$ in elementwise way; the set $\Gamma$ is used to represent all permutation functions on $W$ and $\Gamma(\sigma):=\{f(\sigma)| f\in \Gamma\}$ for $\sigma \in W^n$; the indicator function $\ide{x}{y}$ is defined as
$\ide{x}{y} = 1 $ when $x=y$, and $\ide{x}{y}=0$ when $x\neq y$; $g(n) = \Theta(f(n))$ if there exists constant $c_1 < c_2$ such that $c_1 f(n) \leq g(n) \leq c_2 f(n)$
for large $n$;
$\Lambda := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k-1\}$
where $\mathbf{1}_n$ is the all one vector with dimension $n$;
we define the distance of two vectors as:
$\dist(\sigma, X)
=|\{i\in[n]:\sigma_i\neq X_i\}| \textrm{ for } \sigma,X\in W^n
$ and the distance of a vector to a space $S\subseteq W^n$
as
$\dist(\sigma,S)
:=\min\{\dist(\sigma, \sigma') | \sigma' \in S\}
$.

\section{Mathematical Model} \label{s:Preliminaries}
We first recall the definition of Symmetric Stochastic Block Model (SSBM) with $k$ communities \cite{Abbe17} and the definition of Ising model with $k$ states.
\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
Let $0\leq p<q\leq 1$, $X=(X_1,\dots,X_n)\in W^n$ and $G=([n],E(G))$. The pair $(X,G)$ is drawn under $\SSBM(n,k,p,q)$ if 
\begin{enumerate}
\item $X$ is drawn uniformly with the constraint that $|\{v \in [n] : X_v = u\}| = \frac{n}{k}$ for $u\in W$;

\item There is an edge between the vertices $i$ and $j$ with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$; the existence of each edge is independent with each other.
\end{enumerate}
\end{definition}
From the symmetric property of SBM, the conditional distribution $P(G|X=x) = P(G|X=f(x)), \forall f \in \Gamma$. Therefore, it is only possible to recover $X$ from $G$ up to a global permutation. That is, it is only possible to recover $\Gamma(X)$.

In this paper, we focus on the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. In this regime, it is well known that exact recovery of $X$ (up to a global permutation) from $G$ is possible if $\sqrt{a}-\sqrt{b} > \sqrt{k}$ \cite{abbe2015exact}.
 
Given a labeling $X$ of $n$ vertices, the SBM specifies how to generate a random graph on these $n$ vertices according to the labeling. In some sense, Ising model works in the opposite direction, i.e., given a graph $G=([n],E(G))$, Ising model defines a probability distribution on all possible labels $W^n$ of these $n$ vertices. 

 
\begin{definition}[Ising model with $k$ states]
The Ising model on a graph $G=([n],E(G))$ with parameters $\alpha,\beta>0$ is a probability distribution on the configurations $\sigma\in W^n$ such that
\begin{align} \label{eq:isingma}
&P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\beta \sum_{\{i,j\}\in E(G)} \ide{\bar{\sigma}_i}{\bar{\sigma}_j}\notag\\
&-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} \ide{\bar{\sigma}_i}{\bar{\sigma}_j}
\Big)
\end{align}
where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and
$Z_G(\alpha,\beta)$ is the normalizing constant for this distribution.
\end{definition}

When $\alpha=0$, Equation \eqref{eq:isingma} gives the standard definition for Potts Model \cite{potts1952some}.
For our specific problems, $\alpha > 0$ is needed to guarantee that the distribution is not concentrated within
$\Lambda$. 

%By definition of Ising model we also have $P_{\sigma|G}(\sigma=\bar{\sigma})=P_{\sigma|G}(\sigma=f(\bar{\sigma}))$. This symmetric property corresponds with that of SBM.

Next we present our new model, the Stochastic Ising Block Model (SIBM), which can be viewed as a natural composition of the SSBM and the Ising model. In SIBM, we first draw a pair $(X,G)$ under $\SSBM(n,k,p,q)$.  Then we draw $m$ independent samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ from the Ising model on the graph $G$, where $\sigma^{(u)}\in W^n$ for all $u\in[m]$.

\begin{definition}[Stochastic Ising Block Model]
The triple $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ is drawn under $\SIBM(n,k, p,q,\alpha,\beta,m)$ if

\noindent
(i) the pair $(X,G)$ is drawn under $\SSBM(n,k, p,q)$;

\noindent
(ii) for every $i\in[m]$, each sample $\sigma^{(i)}=(\sigma_1^{(i)},\dots,\sigma_n^{(i)}) \in W^n$ is drawn independently according to the distribution in Equation \eqref{eq:isingma}.
\end{definition}

Notice that we only draw the graph $G$ once in SIBM, and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ are drawn independently from the Ising model on the {\em same} graph $G$.
Our objective is to recover the underlying partition (or the ground truth) $X$ from the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$, and we would like to use the smallest possible number of samples to guarantee the exact recovery of $X$ up to a global permutation.
Below we use the notation $P_{\SIBM}(A):=E_G[P_{\sigma|G}(A)]$ for an event $A$, where the expectation $E_G$ is taken with respect to the distribution given by SSBM. In other words, $P_{\sigma|G}$ is the conditional distribution of  $\sigma$ given a fixed $G$ while $P_{\SIBM}$ is the joint distribution of both $\sigma$ and $G$.
By definition, we have $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f\in \Gamma$.


\begin{definition}[Exact recovery in SIBM]
Let $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k,p,q,\alpha,\beta,m)$.
We say that exact recovery is solvable for $\SIBM(n,k,p,q,\alpha,\beta,m)$ if there is an algorithm that takes $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ as inputs and outputs $\hat{X}=\hat{X}(\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ such that
$$
P_{\SIBM}(\hat{X} \in \Gamma(X)) \to 1
\text{~~~as~} n\to\infty
$$
and we call $P_{\SIBM}(\hat{X} \in \Gamma(X))$ the success probability of the recovery algorithm.
\end{definition}

By definition, the ground truth $X$, the graph $G$ and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ form a Markov chain $X\to G\to \{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Therefore, if we cannot recover $X$ from $G$, then there is no hope to recover $X$ from $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Thus a necessary condition for the exact recovery in SIBM is $\sqrt{a}-\sqrt{b}> \sqrt{k}$, and we will limit ourselves to this case throughout the paper.

\section{Sample Complexity of SIBM}\label{s:trans}
Our main Problem is to investigate what is the smallest sample size $m^\ast$ such that exact recovery is solvable for $\SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m^\ast)$?

Our main results are as follows:

\begin{theorem} \label{thm:wt1}
For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, let
\begin{align} \label{eq:defstar}
&\beta^\ast \triangleq
\log\frac{a+b-k-\sqrt{(a+b-k)^2-4ab}}{2 b}  \\
&m^\ast \triangleq 2 \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1 \text{~~~and~~~}
\tilde{m} \triangleq \frac{k}{k-1}\Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor
\end{align}
\begin{enumerate}
	\item We discuss the case when $\alpha > b \beta$:
	\begin{enumerate}
	\item If $m\ge m^\ast$, then exact recovery is solvable in $O(n)$ time for $\SIBM(n,k, a\log(n)/n, \linebreak[4] b\log(n)/n,\alpha,\beta,m)$.
	\item If $\beta^\ast/\beta$ is not an integer and $m < \tilde{m} + \frac{1}{k-1}$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$.
	\item If $\beta^\ast/\beta$ is an integer and $m < \tilde{m} -1$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$.
	\end{enumerate}
	\item When $\alpha < b \beta$, exact recovery of $\SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m)$ is not solvable for any $m=O(\log^{1/4}(n))$.
\end{enumerate}

\end{theorem}
Note that the condition $\sqrt{a}-\sqrt{b} > \sqrt{k}$ guarantees that the term $\sqrt{(a+b-k)^2-4ab}$ in the definition of $\beta^\ast$ is a real number.
When $\alpha > b \beta$,
the above theorem establishes a recovery threshold
for the regime of $m \geq m^\ast$ and $m < \tilde{m}$ (approximately) on the number of samples. Since $\tilde{m} < m^*$,
When $m \in (\tilde{m}, m^\ast)$, we do not know whether it is possible to recover $X$ or not.
However, when $k=2$ and $\beta^\ast/\beta$ is not an integer, it can be seen that the threshold is sharp since the interval $(\tilde{m}, m^\ast + \frac{1}{k-1})=\emptyset$.

It is worth mentioning that the threshold values $m^\ast$ and $\tilde{m}$ do not depend on the parameter $\alpha$, as long as $\alpha>b\beta$.
Below we present an equivalent characterization of the recovery threshold in terms of $\beta$.
\begin{theorem} \label{thm:wt2}
	Let $a,b,\alpha,\beta> 0$ be constants satisfying $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$. 
	Let 
	$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n, k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m).
	$
	If $\lfloor \frac{m+1}{2} \rfloor \beta>\beta^\ast$, then there is an algorithm that recovers $X$ from the samples in $O(n)$ time with success probability $1-o(1)$. If $\lfloor \frac{(k-1)(m+1)}{k} \rfloor \beta <\beta^\ast$, then the success probability of any recovery algorithm is $o(1)$. 
\end{theorem}
Using the property of floor function, we can show that Theorem~\ref{thm:wt1} and Theorem~\ref{thm:wt2} are equivalent when $\alpha > b \beta$.
%$\beta$ represents the force of attraction between connected node in the graph. Theorem \ref{thm:wt2} states that
%to recover the label, the attraction force has a minimal threshold which depends on $m, \beta^*$.
When $m=1$, we have the next theorem:

\begin{theorem}  \label{thm:wt3}
Let $a,b,\alpha,\beta> 0$ be constants satisfying $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$.
Let 
$
(X,G,\{\sigma\}) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,1).
$
Define $g(\beta)  := \frac{b e^{\beta}+a e^{-\beta}}{k}-\frac{a+b}{k}+1$.
If $\beta>\beta^\ast$, then
$$
P_{\SIBM}(\sigma \in \Gamma(X)) = 1-o(1).
$$
If $\beta\le \beta^\ast$, then $0\leq g(\beta) < 1$ and
$$
P_{\SIBM}(\dist(\sigma, \Gamma(X))= \Theta(n^{g(\beta)})) = 1-o(1)
$$
\end{theorem}

The above theorem established the phase transition property for SIBM model.
Roughly speaking, the sample $\sigma$ generated from SIBM aligns with $X$ with probability 1 if $\beta > \beta^*$;
otherwise the sample differs from $X$ with the number of coordinates in $\Theta(n^{g(\beta)})$.
\section{Community Detection Methods}
Modularity maximization is a popular community detection method \cite{clauset2004finding}, which can also be used for recovery of SBM model.
In this section,
we show that how the modularity is connected with our SIBM model.

If we could choose proper $\alpha, \beta$, then from Theorem \ref{thm:wt3} one sample generated from Ising model is enough to
estimate the original label. This provides a way to do community detection for SBM model.
To some extent
SIBM provides the theoretical guarantee for such detection method.

If $\alpha > b \beta$ and $\beta < \beta^*$ then we have $\hat{\sigma} \in \Gamma(X)$ with probability
one where
\begin{equation}\label{eq:hat_sigma}
\hat{\sigma} := \arg\max_{\bar{\sigma}}\beta \sum_{\{i,j\}\in E(G)} \ide{\bar{\sigma}_i}{\bar{\sigma}_j}
-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} \ide{\bar{\sigma}_i}{\bar{\sigma}_j}
\end{equation}
We consider the modularity of a graph, which is defined by
\begin{equation}\label{eq:Q}
	Q = \frac{1}{2 |E|} \sum_{ij} (A_{ij} - \frac{d_i d_j}{2 |E|}) \delta(C_i, C_j)
\end{equation}
where $d_i$ is the degree of the $i-$th node.
The standard way to do community detection is to maximize the modularity $Q$ using greedy method.
We can show that \eqref{eq:Q} satisfies the recovery constraint asymptotically.
Indeed, we have $d_i \sim \frac{\log n(a+b)}{2}, |E| \sim \frac{1}{2}n d_i$. Therefore, we have $\beta = \frac{1}{2|E|}(1-\frac{\log n}{2n}(a+b))
\sim \frac{1}{2|E|}$
and $\alpha = \frac{1}{2|E|}\frac{a+b}{2}$. Since $a>b$. We have $\alpha > b \beta$, which is a necessary condition for finite sample complexity.

\section{Conclusion}\label{s:conclusion}
In this paper, we derive the sample complexity for Stochastic Ising Block Model on multiple communities.
Our result shares insights on the relationship of SBM and Ising model and guides the design of efficient algorithms
for community detection problems.
\appendix
\section{Sketch of the proof}
\label{sect:sketch}

In this section, we illustrate the main ideas and explain the important steps in the proof of the main results.
Some proof techniques are borrowed from \cite{ye2020exact}, which deals with the case for two communities.

\subsection{Why $\alpha > b \beta$}
Intuitively we need $P(\sigma = X) > P(\sigma = \mathbf{1}_n)$ on average to guarantee that the sample is more likely to align with the ground truth label.
That is to say
$$
\mathbb{E}_G[\log \frac{P_{\sigma|G}(\sigma = X)}{P_{\sigma|G}(\sigma = \mathbf{1}_n)}] > 0
$$
Simplifying the above equation we could get $\alpha > b \beta$ asymptotically as $n\to \infty$.

Below we will show that for $a>b>0$, if $\alpha>b\beta$, then all the samples are centered in $\Gamma(X)$ with probability $1-o(1)$;
if $\alpha<b\beta$, then all the samples are centered in $\Lambda$.

We use the concentration results for adjacency matrices of random graphs with independent edges to prove this.
Let $A$ be the adjacency matrix of the graph $G$, $J_n$ be the all one matrix and $I_n$ be the identity matrix, both of size $n\times n$.
Define a matrix
$
M:= \big(\beta+\frac{\alpha\log(n)}{n} \big) E[A|X]
-\frac{\alpha\log(n)}{n} (J_n-I_n).
$
Then we can write \eqref{eq:isingma}
as
\begin{align*}
&P_{\sigma|G}(\sigma=\bar{\sigma})
= \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j} M_{ij} \ide{\bar{\sigma}_i}{\bar{\sigma}_j} + \\
& \frac{1}{2} \sum_{i,j}\big(\beta+\frac{\alpha\log(n)}{n} \big)  (A-E[A|X])_{ij} \ide{\bar{\sigma}_i}{\bar{\sigma}_j}
\Big)
\end{align*}
One can show that if $\alpha>b\beta$, then the maximizers of $\sum_{i,j} M_{ij} \ide{\bar{\sigma}_i}{\bar{\sigma}_j}$ consist of set $\Gamma(X)$, and if $\alpha<b\beta$, then the maximizer set is $\Lambda$.
Moreover, \cite{Hajek16} proved the concentration of $A$ around its expectation $E[A|X]$ in the sense that
the spectral norm $\|A-E[A|X]\| = O(\sqrt{\log(n)})$ with probability $1-o(1)$, we can further show that the error term above is bounded by
$$
\left|\frac{1}{2} \big(\beta+\frac{\alpha\log(n)}{n} \big)\sum_{i,j} (A-E[A|X])
  \ide{\bar{\sigma}_i}{\bar{\sigma}_j} \right| = O \big( n \sqrt{\log(n)} \big)
$$
for all $\bar{\sigma}\in W^n$.
This allows us to prove that in both cases (no matter $\alpha>b\beta$ or $\alpha<b\beta$), the Hamming distance between the samples and the maximizers of $\sum_{i,j}  M_{ij} \ide{\bar{\sigma}_i}{\bar{\sigma}_j}$ is upper bounded by $kn/\log^{1/3}(n)=o(n)$.
More precisely, if $\alpha>b\beta$, then $\dist(\sigma^{(i)},\Gamma(X) )< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$;
if $\alpha<b\beta$, then $\dist(\sigma^{(i)}, \Lambda)< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$;
In the latter case, each sample only take $\sum_{j=0}^{kn/\log^{1/3}(n)}\binom{n}{j}j^{k-1}$ values, so each sample contains at most $\log_k(\sum_{j=0}^{kn/\log^{1/3}(n)}\binom{n}{j}j^{k-1})=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$ bits of information about $X$. On the other hand, $X$ itself is uniformly distributed over a set of $\frac{n!}{(n/k)^k}$ vectors, so one needs at least $\log_k\frac{n!}{(n/k)^k}=\Theta(n)$ bits of information to recover $X$. Thus if $\alpha<b\beta$, then exact recovery of $X$ requires at least $\Omega(\frac{\log^{1/3}(n)}{\log\log(n)})\ge \Omega(\log^{1/4}(n))$ samples.

For the rest of this section, we will focus on the case $\alpha>b\beta$ and establish the threshold on the sample complexity. We first analyze the typical behavior of one sample and explain how to prove Theorem~\ref{thm:wt3}. Then we use the method developed for the one sample case to analyze the distribution of multiple samples, which allows us to prove Theorem~\ref{thm:wt2}. Note that Theorem~\ref{thm:wt1} follows directly from Theorem~\ref{thm:wt2}.

\subsection{Why is $\beta^\ast$ the threshold?} \label{sect:why}

In the above section, by comparing the probability of $P(\sigma = 1)$ with $P(\sigma=X)$ we get the condition $\alpha > b \beta$.
In this section, we follow the same intuitive approach but consider comparing $P(\dist(\sigma, X) = 1)$ with $P(\sigma = X)$.
We consider the $i-$th coordinate in which $\sigma_i \neq X_i$. Since each coordinate takes values from $W$, we have $k-1$ cases for this coordinate.
Let the event $T=\{\sigma_j = X_j, j\neq i, \sigma_i = \omega^r \cdot X_i\}$ and define
\begin{equation*}
A^r_i=A^r_i(G):=|\{j\in[n]\setminus\{i\}:\{i,j\}\in E(G), X_j=\omega^r \cdot X_i\} |
\end{equation*}
to represent the number of edges whose two nodes differs by $\omega^r$.
By definition,
$A^0_i\sim \Binom(\frac{n}{k}-1,\frac{a\log(n)}{n})$ and $A^r_i\sim \Binom(\frac{n}{k}, \frac{b\log(n)}{n})$ for $r=1,\dots, k-1$, and they are independent.

We then have
\begin{align}
&\frac{P_{\sigma|G}(T)}
{P_{\sigma|G}(\sigma=X)}
= \exp\Big(\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i) \nonumber\\
&-\frac{(k-1)\alpha\log(n)}{n} \Big) 
= (1+o(1)) \exp ( \beta(A^r_i-A^0_i)) \label{eq:kbetaA}
\end{align}
where the second equality holds with high probability because $|A^r_i-A^0_i|=O(\log(n))$ with probability $1-o(1)$.

Therefore, we can estimate the mean value of \eqref{eq:kbetaA}:
\begin{align}
&\mathbb{E}_G[\exp (\beta (A^r_i-A^0_i))]
=\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^{\beta} \Big)^{n/k} \nonumber \\
&\cdot \Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-\beta} \Big)^{n/k-1}\nonumber\\
& = 
\exp\Big(\frac{\log(n)}{k} ( a e^{-\beta}+b e^{\beta} -a-b )
+o(1) \Big)\nonumber \\
& = (1+o(1)) n^{g(\beta)-1} \label{eq:gbetaminus1}
\end{align}
where $g(\beta)  := \frac{b e^{\beta}+a e^{-\beta}}{k}-\frac{a+b}{k}+1$. This $g(\beta)$ is also defined in Theorem~\ref{thm:wt3}.

Summing over all $r=1, \dots, k-1$ and $i=1, \dots, n$ we get
\begin{equation}\label{eq:diff_by_one}
\mathbb{E}_G[\frac{P_{\sigma|G}(\dist(\sigma, X)=1)}
{P_{\sigma|G}(\sigma=X)}] = (1+o(1))k n^{g(\beta)}
\end{equation}
The critical value is the zero point of $g(\beta)$, which is exactly
$$
\beta^* = \log\frac{a+b-k-\sqrt{(a+b-k)^2-4ab}}{2 b}
$$
as given in Theorem~\ref{thm:wt1}.

From \eqref{eq:diff_by_one} we can see that the mean value decreases when $\beta > \beta^*$. In section \ref{beta_large}, we will show that $\frac{P_{\sigma|G}(\dist(\sigma, X)=1)}{P_{\sigma|G}(\sigma=X)}$ decreases
in a similar behavior with probability 1 as $n\to \infty$.
In section \ref{subsect:smaller}, the case $\beta < \beta^*$ will be discussed in detail.
Combing the results of section \ref{beta_large} and \ref{subsect:smaller}, we can say that
$P_{\SIBM}(\sigma\in \Gamma(X))$ has a sharp transitions from $0$ to $1$ at $\beta^\ast$,
which is Theorem \ref{thm:wt3}.
\subsection{Proof for $\beta\in(\beta^*,+\infty)$}\label{beta_large}

We start with a more careful analysis of $\sum_{i=1}^n \exp ( \beta (A^r_i-A^0_i))$.
Since $A^r_i-A^0_i$ takes integer value between $-n/k$ and $n/k$,
we can use indicator functions to write
\begin{align*}
& \exp (\beta (A^r_i-A^0_i))
= \\  \sum_{t\log(n)=-n/k}^{n/k}
& \mathbbm{1}[A^r_i-A^0_i=t \log(n)] \exp (\beta t \log(n) ) ,
\end{align*}
where the quantity $t\log(n)$ ranges over all integer values from $-n/k$ to $n/k$ in the summation.
Define $D(G,t):=|\{i\in[n]:A^r_i-A^0_i= t\log(n)\}|=\sum_{i=1}^n \mathbbm{1}[A^r_i-A^0_i=t \log(n)]$.
Therefore,
\begin{equation}  \label{eq:gour}
 \sum_{i=1}^n \exp ( \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{n/k}
D(G,t) \exp ( \beta t \log(n) ) .
\end{equation}
By Chernoff bound, we have $P(A^r_i-A^0_i\ge 0)\le \exp\big(\log(n)(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big)$.
Define $\cG_1:=\{G:A^r_i-A^0_i< 0~\forall i\in[n]\}$. Then by union bound, $P(G\notin\cG_1)\le\exp\big(\log(n)(1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big) = o(1)$, where the equality follows from the assumption that $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
For every $G\in\cG_1$, $D(G,t)=0$ for all $t\ge 0$, and so
\begin{equation} \label{eq:duj}
 \sum_{i=1}^n \exp (\beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{-1}
D(G,t) \exp ( \beta t \log(n) ) .
\end{equation}
Define a function
\begin{align*}
&f_{\beta}(t):=\frac{1}{k}\sqrt{k^2t^2+4ab} -\frac{a+b}{k} +1 +\beta t  \\
&-t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big).
\end{align*}
Using Chernoff bound, one can show that
$$
\mathbb{E}[D(G,t)
\exp\big(\beta t \log(n) \big)]
\le \exp( f_{\beta}(t) \log(n) ) .
$$
with probability one.
The function $f_{\beta}(t)$ is a concave function
and takes maximum value at $t^\ast=\frac{b e^{\beta}-a e^{-\beta}}{k}$,
and $f_{\beta}(t^\ast)=\frac{b e^{\beta}+a e^{-\beta}}{k}-\frac{a+b}{k} + 1
=g(\beta)$. When $ \beta^* < \beta < \frac{1}{2} \log \frac{a}{b}$, we have $t^\ast < 0$
for such case, if we take expectation on both sides of \eqref{eq:gour},
then the sum on the right-hand side is concentrated on a small neighborhood of $t^\ast$.
When $\beta >  \frac{1}{2} \log \frac{a}{b}$ from Equation \eqref{eq:duj},
the right-hand is concentrated on the left neighborhood of $0$.
Therefore, for $G\in \cG_1$, the sum $\mathbb{E}[\sum_{t\log(n)=-n/k}^{n/k}
D(G,t) \exp (k\beta t \log(n) )]$
is upper bounded by $O(\log(n))n^{f_{\beta}(0)}$  when $t^\ast>0$ and $O(\log(n))n^{f_{\beta}(t^\ast)}$  when $t^\ast<0$.
Notice that $f_{\beta}(0)=g(\frac{1}{2}\log\frac{a}{b})=1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}<0$. To simplify the notation we define
$$
\tilde{g}(\beta) = \begin{cases}
g(\beta)   & \text{~if~} \beta< \frac{1}{2}\log\frac{a}{b} \\
g(\frac{1}{2} \log\frac{a}{b}) & \text{~if~} \beta\ge \frac{1}{2}\log\frac{a}{b}
\end{cases}
$$
Now using \eqref{eq:duj} and Markov inequality,
we conclude that $\sum_{r=1}^{k-1}\sum_{i=1}^n \exp ( \beta (A^r_i-A^0_i))< n^{\tilde{g}(\beta)/2}$ for almost all $G$,
and so by Equation \eqref{eq:kbetaA}, $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} < n^{\tilde{g}(\beta)/2}$ for almost all $G$. 
The analysis of $\frac{P_{\SIBM} ( \dist(\sigma, X) = r )}{P_{\SIBM}(\sigma= X)}$ for $1\le r<kn/\log^{1/3}(n)$ is
similar but with some notation difference, and we do not repeat it here.
The conclusion is $\frac{P_{\sigma|G} ( \dist(\sigma, X) = r )}{P_{\sigma|G}(\sigma= X)} < n^{r\tilde{g}(\beta)/2}$ for almost all $G$.
Using the union bound and
$
P_{\SIBM} \big(\dist(\sigma,\Gamma(X))< kn/\log^{1/3}(n) \big) = 1- o(1)
$
,
we can prove
$P_{\SIBM}(\sigma \in \Gamma)=1-o(1)$ when $\beta>\beta^\ast$ for almost all $G$.

\subsection{Proof for $\beta\le\beta^\ast$}\label{subsect:smaller}
We restrict our discussion on the case $\dist(\sigma,X)\le n/k$, i.e., $\sigma$ is closer to $X$ than to $\Gamma(X)\backslash\{X\}$.
We would show that $P(\dist(\sigma, X) = \Theta(n^{g(\beta)})) = 1 - o(1)$ conditioned on $\dist(\sigma,X)\le n/k$.
We say that $j$ is a ``bad" neighbor of vertex $i$ if the edge $\{i,j\}$ is connected in graph $G$ and $\sigma_j\neq X_j$. Then there is an integer $z>0$ such that with probability $1-o(1)$, every vertex has at most $z$ ``bad" neighbors.
Conditioning on the event every vertex has at most $z$ ``bad" neighbors, which happens with probability $1-o(1)$, it is easy to show that $P_{\sigma|G}(\sigma_i \neq X_i)$ differs from $\sum_{r=1}^{k-1}\exp (\beta (A^r_i-A^0_i))$ by at most a constant factor. 
% the factor is $\exp(\beta z)$ ?
Therefore, $\mathbb{E}_{\sigma|G}[\dist(\sigma,X)]=\sum_{i=1}^n P_{\sigma|G}(\sigma_i \neq X_i)$ differs from
$\sum_{r=1}^{k-1} \sum_{i=1}^n\exp (\beta (A^r_i-A^0_i))$ by at most a constant factor for almost all $G$.
We can further prove that $\dist(\sigma,X)$ concentrates around its expectation.
Thus, we conclude that $\dist(\sigma,X)$ differs from
$\sum_{r=1}^{k-1} \sum_{i=1}^n\exp (\beta (A^r_i-A^0_i))$ by at most a constant factor for almost all $G$.
When $\beta\le\beta^\ast$, we can prove a very tight concentration around the expectation. Then from Equation \eqref{eq:gbetaminus1}, for almost all graph $G$, we have $\sum_{i=1}^n\exp (\beta (A^r_i-A^0_i))=(1+o(1))n^{g(\beta)-1}$.
Combining this with the above analysis,
we conclude that $\dist(\sigma,X)=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ when $\beta\le\beta^\ast$.
This completes the sketched proof of Theorem~\ref{thm:wt3}.

\subsection{Multiple sample case: Proof of Theorem~\ref{thm:wt2}}
\label{sect:multi}

		\begin{algorithm}
			\caption{\texttt{LearnSIBM} in $O(n)$ time} \label{alg:ez}
			Inputs: the samples $\sigma^{(1)},\sigma^{(2)}\dots,\sigma^{(m)}$ \\
			Output: $\hat{X}$
			\begin{algorithmic}[1]
				\Statex 
				{\bf Step 1: Align all the samples with $\sigma^{(1)}$ }
				\For {$j=2,3,\dots,m$}
				\State $f^* = \arg\max_{f \in \Gamma} \sum_{i=1}^n h(f(\sigma^{(j)}_i), \sigma^{(1)}_i)$
				\State $\sigma^{(j)} \gets f^*(\sigma^{(j)})$
				\EndFor
				\Statex
				{\bf Step 2: Majority vote at each coordinate}
				\For {$i=1,2,\dots,n$}
				\State $g(r) = |\{j | \sigma^{(j)}_i = \omega^r,1\leq j \leq m\}|$  for $ 0 \leq r \leq k-1$
				\State $\hat{X}_i \gets w^{r*}$ where $r*=\arg\max_r g(r)$
			\State\Comment{If the max of $g(r)$ is not unique, assign $\hat{X}_i$ randomly to one of \texttt{argmax}}
				\EndFor
				\State Output $\hat{X}$
			\end{algorithmic}
		\end{algorithm}
For the multiple-sample case,
we prove that Algorithm \ref{alg:ez} can recover $X$ with probability $1-o(1)$ when $\lfloor \frac{m+1}{2} \rfloor \beta>\beta^\ast$.
We already showed that each sample is very close to $\Gamma(X)$ when $\alpha > b \beta$,
so after the alignment step in Algorithm~\ref{alg:ez},
all the samples are simultaneously aligned with the same element from $\Gamma(X)$.
We assume all samples are aligned with $X$ in the following analysis.
From Subsection \ref{subsect:smaller},
with probability $1-o(1)$, $P_{\sigma|G}(\sigma_i^{(j)} \neq X_i)$ differs from
$\sum_{r=1}^{k-1} \exp (\beta (A^r_i-A^0_i))$ by at most a constant factor for all $j\in[m]$.
Since the samples are independent,
we further obtain that $P_{\sigma|G}(\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i]\ge u)$ differs from $\sum_{r=1}^{k-1} \exp ( u \beta (A^r_i-A^0_i))$
by at most a constant factor.
Here $u\beta$ plays the role of $\beta$ in the single-sample case.
Therefore, if $u\beta>\beta^\ast$, then with probability $1-o(1)$ we have $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i] \le u-1$ for all $i\in[n]$. Let $u=\lfloor \frac{m+1}{2} \rfloor$,
then we have $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i] \le \lfloor \frac{m-1}{2} \rfloor $ while $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} = X_i]
= m - \lfloor \frac{m-1}{2} \rfloor > \sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i] $
which implies that $\hat{X}=X$ after the majority voting step in Algorithm~\ref{alg:ez}.

The proof of the converse results, i.e., even ML algorithm cannot recover $X$ with probability $1-o(1)$ when $\lfloor \frac{(k-1)(m+1)}{k} \rfloor  \beta < \beta^\ast$ is rather similar to the proof of $\beta\le\beta^\ast$ for the single-sample case.

\bibliographystyle{IEEEtran}
\bibliography{exportlist}
\end{document}

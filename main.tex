\documentclass[conference]{IEEEtran}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{theorem}{Theorem}%[section]
\newtheorem{definition}{Definition}%[section]
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\SIBM}{SIBM}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cG}{\mathcal{G}}
\usepackage{bbm} % provide mathbbm
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Binom}{Binom}

\title{Stochastic Ising Block Model for multiple communities}
\author{%
	\IEEEauthorblockN{Min Ye, Shao-Lun Huang}
	\IEEEauthorblockA{DSIT Research Center\\
		Tsinghua-Berkeley Shenzhen Institute\\
		Shenzhen, China 518055\\
		Email: yeemmi@sz.tsinghua.edu.cn}
	\and
	\IEEEauthorblockN{Feng Zhao}
	\IEEEauthorblockA{Department of Electronic Engineering\\
		Tsinghua University\\ 
		Beijing, China 10084\\
		Email: zhaof17@mails.tsinghua.edu.cn}
}
\begin{document}
\maketitle
\begin{abstract}
 Stochastic Block Model (SBM) is an important model in community detection while Ising model arises from ferromagnetism in statistical physics.
 In this paper, we will concatenate SBM and Ising model and propose Stochastic Ising Block Model (SBIM) for multiple communities.
 The random graph, generated by SBM, is used to generate Ising samples and our goal is to recover the label of the communities.
 For SIBM, the phase transition phenomenon is established and the sample complexity is computed.
\end{abstract}
\section{Introduction}
% first paragraph: short intro to SBM and Ising model
In network analysis, Stochastic Block Model (SBM) is a commonly used random graph model, in which the probability of edge existence is higher within the community than between different communities \cite{holland1983stochastic}. Within SBM, the condition to fully recover community labels has been investigated thoroughly and belongs to some phase transition property \cite{Abbe17}. On the other hand, Ising model is a well-known statistical model in physics which also has some kind of phase translation property \cite{ising1925beitrag}. The particles in Ising model interacts with neighboring particles and change its state.
Also Ising model is not limited to statistical physics and there has already some works to apply Ising model to investigate the social voting phenomenon \cite{sznajd2000opinion}.

In the original formulation, each particle in Ising model has only two states. Potts extended it to multiple state \cite{potts1952some}. This multiple state Ising model
corresponds to SBM with multiple communities, thus enabling us to propose a concatenated network model: Stochastic Ising Block Model (SIBM). For 2 community case, there has already been some results on SIBM \cite{ye2020exact}, but little is know for multiple communities case. In this paper, we will give the formulation of SIBM for multiple
communities. We can generate multiple samples from SIBM, which more or less differs from the original labels, depending on the choice of parameters. Using the generated samples, we focus on the problem of whether it is possible to exactly recover the label in this paper. By inversting the phase translation property for our compose model,
we can compute the feasible regime of recovery and the corresponding sample complexity.

This paper is divided as follows: In Section \ref{s:Preliminaries} the SIBM is formulated mathematically;
In Section \ref{s:trans}, the main results are introduced;
In Section \ref{s:conclusion}, the conclusion is given. Proof sketch of results is provided in the appendix.

% notation convetion
Within this paper, the number of community is denoted by $k$; the number of node is $n$; $m$ is the number of samples; the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$;
the label of each node is a random variable $X_i$; $X_i$ is chosen from $W= \{1, \omega, \dots, \omega^{k-1}\}$ and we further require $W$
is a cyclic group with order $k$; $W^n$ is the n-ary Cartesian power of $W$; $f$ is a permutation function on $W$ and applies to $W^n$ element-wisely; the set $\Gamma$ is used to represent all permutation function on $W$ and $\Gamma(\sigma):=\{f(\sigma)| f\in \Gamma\}$ for $\sigma \in W^n$; the indicator function $I(x, y)$ is defined as
$I(x, y) = k-1 $ when $x=y$ and $I(x,y)=-1$ when $x\neq y$;

\section{Mathematical Model} \label{s:Preliminaries}
We first recall the definition of Symmetric Stochastic Block Model (SSBM) with $k$ communities \cite{Abbe17} and the definition of Ising model with $k$ state.
\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
Let $p,q\in[0,1]$ be two real numbers, $X=(X_1,\dots,X_n)\in W^n$ and $G=([n],E(G))$. The pair $(X,G)$ is drawn under $\SSBM(n,k,p,q)$ if 
\begin{enumerate}
\item $X$ is drawn uniformly with the constraint that $|\{v \in [n] : X_v = \omega^i\}| = \frac{n}{k}$;

\item the vertices $i$ and $j$ in $G$ are connected with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$, independently of other pairs of vertices.
\end{enumerate}
\end{definition}
From the symmetric property of SBM, the conditional distribution $P(G|X=x) = P(G|X=f(x))$. Therefore, it is only possible to recover $X$ from $G$ up to a global permutation.

In this paper, we focus on the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. In this regime, it is well known that exact recovery of $X$ (up to a global permutation) from $G$ is possible if and only if $\sqrt{a}-\sqrt{b} > \sqrt{k}$  \cite{abbe2015exact}.
 
Given a partition/labeling $X$ on $n$ vertices, the SBM specifies how to generate a random graph on these $n$ vertices according to the labeling. In some sense, Ising model works in the opposite direction, i.e., given a graph $G=([n],E(G))$, Ising model defines a probability distribution on all possible labelings $W^n$ of these $n$ vertices. 

 
\begin{definition}[Ising model]
The Ising model on a graph $G=([n],E(G))$ with parameters $\alpha,\beta>0$ is a probability distribution on the configurations $\sigma\in W^n$ such that
\begin{align} \label{eq:isingma}
&P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\beta\sum_{\{i,j\}\in E(G)} I(\bar{\sigma}_i ,\bar{\sigma}_j)\notag\\
&-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j)\Big) ,
\end{align}
where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and
$Z_G(\alpha,\beta)$ is the normalizing constant for this distribution.
\end{definition}

By definition of Ising model we also have $P_{\sigma|G}(\sigma=\bar{\sigma})=P_{\sigma|G}(\sigma=f(\bar{\sigma}))$. This symmetric property corresponds with that of SBM.

Next we present our new model, the Stochastic Ising Block Model (SIBM), which can be viewed as a natural composition of the SSBM and the Ising model. In SIBM, we first draw a pair $(X,G)$ under $\SSBM(n,k,p,q)$.  Then we draw $m$ independent samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ from the Ising model on the graph $G$, where $\sigma^{(u)}\in W^n$ for all $u\in[m]$.

\begin{definition}[Stochastic Ising Block Model]
The triple $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ is drawn under $\SIBM(n,k, p,q,\alpha,\beta,m)$ if

\noindent
(i) the pair $(X,G)$ is drawn under $\SSBM(n,k, p,q)$;

\noindent
(ii) for every $i\in[m]$, each sample $\sigma^{(i)}=(\sigma_1^{(i)},\dots,\sigma_n^{(i)}) \in W^n$ is drawn independently according to the distribution \eqref{eq:isingma}.
\end{definition}

Notice that we only draw the graph $G$ once in SIBM, and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ are drawn independently from the Ising model on the {\em same} graph $G$.
Our objective is to recover the underlying partition (or the ground truth) $X$ from the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$, and we would like to use the smallest possible number of samples to guarantee the exact recovery of $X$ up to a global permutation.
Below we use the notation $P_{\SIBM}(A):=E_G[P_{\sigma|G}(A)]$ for an event $A$, where the expectation $E_G$ is taken with respect to the distribution given by SSBM. In other words, $P_{\sigma|G}$ is the conditional distribution of  $\sigma$ given a fixed $G$ while $P_{\SIBM}$ is the joint distribution of both $\sigma$ and $G$.
By definition, we have $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$.


\begin{definition}[Exact recovery in SIBM]
Let $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k,p,q,\alpha,\beta,m)$.
We say that exact recovery is solvable for $\SIBM(n,k,p,q,\alpha,\beta,m)$ if there is an algorithm that takes $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ as inputs and outputs $\hat{X}=\hat{X}(\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ such that
$$
P_{\SIBM}(\hat{X} \in \Gamma(X)) \to 1
\text{~~~as~} n\to\infty ,
$$
and we call $P_{\SIBM}(\hat{X} \in \Gamma(X))$ the success probability of the recovery/decoding algorithm.
\end{definition}

By definition, the ground truth $X$, the graph $G$ and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ form a Markov chain $X\to G\to \{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Therefore, if we cannot recover $X$ from $G$, then there is no hope to recover $X$ from $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Thus a necessary condition for the exact recovery in SIBM is $\sqrt{a}-\sqrt{b}> \sqrt{k}$, and we will limit ourselves to this case throughout the paper.

\section{Phase Translation and Sample Complexity of SIBM}\label{s:trans}
Our main Problem is to investigate what is the smallest sample size $m^\ast$ such that exact recovery is solvable for $\SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m^\ast)$?

Our main results are as follows:

\begin{theorem} \label{thm:wt1}
For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, let
\begin{align} \label{eq:defstar}
&\beta^\ast := \frac{1}{k}
\log\frac{a+b-k-\sqrt{(a+b-k)^2-4ab}}{2 b}  \\
&m^\ast := 2 \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1 \text{~~~and~~~} \tilde{m} = \frac{k}{k-1}\lfloor \frac{\beta^\ast}{\beta} \rfloor .
\end{align}
{\bf Case (i) when $\alpha>b\beta$}: If $m\ge m^\ast$, then exact recovery is solvable in $O(n)$ time for $\SIBM(n,k, a\log(n)/n, \linebreak[4] b\log(n)/n,\alpha,\beta,m)$.
If $\beta^\ast/\beta$ is not an integer and $m < \tilde{m} + \frac{1}{k-1}$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$.
If $\beta^\ast/\beta$ is an integer and $m < \tilde{m} -1$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$.
{\bf Case (ii) when $\alpha<b\beta$}: Exact recovery of $\SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m)$ is not solvable for any $m=O(\log^{1/4}(n))$, and in particular, it is not solvable for any constant $m$ that does not grow with $n$.
\end{theorem}
Note that the condition $\sqrt{a}-\sqrt{b} > \sqrt{k}$ guarantees that the term $\sqrt{(a+b-k)^2-4ab}$ in the definition of $\beta^\ast$ is a real number.
When $\alpha>b\beta$ and $\beta^\ast/\beta$ is not an integer,
the above theorem establishes a sharp recovery threshold $m^\ast$ on the number of samples. It is worth mentioning that the threshold $m^\ast$ does not depend on the value of the parameter $\alpha$, as long as $\alpha$ satisfies $\alpha>b\beta$.
Below we present an equivalent characterization of the recovery threshold in terms of $\beta$.
\begin{theorem} \label{thm:wt2}
	Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$. 
	Let 
	$$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n, k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m).
	$$
	If $\lfloor \frac{m+1}{2} \rfloor \beta>\beta^\ast$, then there is an algorithm that recovers $X$ from the samples in $O(n)$ time with success probability $1-o(1)$. If $\lfloor \frac{(k-1)(m+1)}{k} \rfloor \beta <\beta^\ast$, then the success probability of any recovery algorithm is $o(1)$. 
\end{theorem}
Note that $\lfloor \frac{m+1}{2} \rfloor \beta>\beta^\ast$ if and only if $m\ge 2 \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1$, so Theorem~\ref{thm:wt1} and Theorem~\ref{thm:wt2} give the same threshold\footnote{We give a proof of the equivalence between the two inequalities: $\lfloor \frac{m+1}{2} \rfloor \beta>\beta^\ast$ implies that $\frac{\beta^\ast}{\beta}<\lfloor \frac{m+1}{2} \rfloor$. The smallest integer that is larger than $\frac{\beta^\ast}{\beta}$ is $\lfloor \frac{\beta^\ast}{\beta}\rfloor +1$, so $\lfloor \frac{\beta^\ast}{\beta} \rfloor +1 \le \lfloor \frac{m+1}{2} \rfloor\le \frac{m+1}{2}$, and thus $m\ge 2 \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1$. Now assume $m\ge 2 \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1$, then $\frac{m-1}{2} \ge \lfloor \frac{\beta^\ast}{\beta} \rfloor$. Since the right hand side is an integer, we have $\lfloor \frac{m+1}{2} \rfloor = \lfloor \frac{m-1}{2} \rfloor +1  \ge \lfloor \frac{\beta^\ast}{\beta} \rfloor +1 >\frac{\beta^\ast}{\beta}$.}.
Notation convention:
For $\cI \subseteq [n]$, we denote the event $\sigma = X^{(\sim \cI)}$ as $\sigma_i \neq X_i$ for all $i \in \cI$ and $\sigma_i = X_i$ for all $i \not\in \cI$.
When $\cI$ only contains one element, e.g., $\cI=\{i\}$, we write the event $\sigma = X^{(\sim i)}$ instead of $\sigma = X^{(\sim\{i\})}$.

For $\sigma,X\in W^n$, we define
$$
\dist(\sigma, X)
:=|\{i\in[n]:\sigma_i\neq X_i\}| 
\quad \text{and} \quad
\dist(\sigma,\Gamma)
:=\min\{\dist(\sigma, f(X)) | f(X) \in \Gamma\} .
$$
\begin{theorem}  \label{thm:wt3}
Let $a,b,\alpha,\beta> 0$ be constants satisfying that $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$. Let $m$ be a constant integer that does not grow with $n$.
Let 
$$
(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m).
$$
Define $g(\beta)  := \frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1$.
If $\beta>\beta^\ast$, then
$$
P_{\SIBM}(\sigma^{(i)} \in \Gamma \text{~for all~} i\in[m]) = 1-o(1).
$$
If $\beta\le \beta^\ast$, then
$$
P_{\SIBM}(\dist(\sigma^{(i)} \in \Gamma)= \Theta(n^{g(\beta)}) \text{~for all~} i\in[m]) = 1-o(1) .
$$
\end{theorem}
\section{Conclusion}\label{s:conclusion}

\appendix
\section{Sketch of the proof}
\label{sect:sketch}

In this section, we illustrate the main ideas and explain the important steps in the proof of the main results. The complete proofs are given in arxiv.
As the first step, we prove that for $a>b>0$, if $\alpha>b\beta$, then all the samples are centered in $\Gamma$ with probability $1-o(1)$; if $\alpha<b\beta$, then all the samples are centered in $\Theta := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k\}$ where $\mathbf{1}_n$ is the all one vector with dimension $n$.

We use the concentration results for adjacency matrices of random graphs with independent edges to prove this. Let $A=A(G)$ be the adjacency matrix of the graph $G$. Then \eqref{eq:isingma} can be written as
\begin{align*}
 P_{\sigma|G}(\sigma=\bar{\sigma})  
=  \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j}\Big( \big(\beta+\frac{\alpha\log(n)}{n} \big) A
-\frac{\alpha\log(n)}{n} (J_n-I_n) \Big)_{ij} I(\bar{\sigma}_i,\bar{\sigma}_j) 
\Big)  ,
\end{align*}
where $J_n$ is the all one matrix and $I_n$ is the identity matrix, both of size $n\times n$.
Define a matrix
$
M:= \big(\beta+\frac{\alpha\log(n)}{n} \big) E[A|X]
-\frac{\alpha\log(n)}{n} (J_n-I_n).
$
Then we can further write \eqref{eq:isingma}
as
$$
P_{\sigma|G}(\sigma=\bar{\sigma})
= \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) + \frac{1}{2} \sum_{i,j}\big(\beta+\frac{\alpha\log(n)}{n} \big)  (A-E[A|X])_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) 
  \bar{\sigma}^T 
\Big)  .
$$
One can show that if $\alpha>b\beta$, then the maximizers of $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ consist of set $\Gamma$, and if $\alpha<b\beta$, then the maximizer set is $\Theta$.
Moreover, \cite{Hajek16} proved the concentration of $A$ around its expectation $E[A|X]$ in the sense that
$\|A-E[A|X]\| = O(\sqrt{\log(n)})$ with probability $1-o(1)$, we can further show that the error term above is bounded by
$$
\left|\frac{1}{2} \big(\beta+\frac{\alpha\log(n)}{n} \big)\sum_{i,j} (A-E[A|X])
 I( \bar{\sigma}_i, \bar{\sigma}_j) \right| = O \big( n \sqrt{\log(n)} \big)
  \text{~~for all~} \bar{\sigma}\in\{\pm 1\}^n  .
$$
This allows us to prove that in both cases (no matter $\alpha>b\beta$ or $\alpha<b\beta$), the Hamming distance between the samples and the maximizers of $\sum_{i,j}  M_{ij}I(\bar{\sigma}_i, \bar{\sigma}_j)$ is upper bounded by $kn/\log^{1/3}(n)=o(n)$.
More precisely, if $\alpha>b\beta$, then $\dist(\sigma^{(i)},\Gamma )< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$; if $\alpha<b\beta$, then $\dist(\sigma^{(i)}, \Theta)< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$; see Proposition~\ref{prop:1} for a rigorous proof.
In the latter case, each sample only take $\sum_{j=0}^{kn/\log^{1/3}(n)}\binom{n}{j}j^{k-1}$ values, so each sample contains at most $\log_k(\sum_{j=0}^{kn/\log^{1/3}(n)}\binom{n}{j}j^{k-1})=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$ bits of information about $X$. On the other hand, $X$ itself is uniformly distributed over a set of $\frac{n!}{(n/k)^k}$ vectors, so one needs at least $\log_k\frac{n!}{(n/k)^k}=\Theta(n)$ bits of information to recover $X$. Thus if $\alpha<b\beta$, then exact recovery of $X$ requires at least $\Omega(\frac{\log^{1/3}(n)}{\log\log(n)})\ge \Omega(\log^{1/4}(n))$ samples; see Proposition~\ref{prop:ab} for a rigorous proof.

For the rest of this section, we will focus on the case $\alpha>b\beta$ and establish the sharp threshold on the sample complexity. We first analyze the typical behavior of one sample and explain how to prove Theorem~\ref{thm:wt3}. Then we use the method developed for the one sample case to analyze the distribution of multiple samples, which allows us to prove Theorem~\ref{thm:wt2}. Note that Theorem~\ref{thm:wt1} follows directly from Theorem~\ref{thm:wt2}.

\subsection{Why is $\beta^\ast$ the threshold?} \label{sect:why}

Let us analyze the one sample case, i.e., we take $m=1$.
Theorem~\ref{thm:wt3} implies that $\beta^\ast$ is a sharp threshold for the event $\{\sigma \in \Gamma\}$, i.e., $P_{\SIBM}(\sigma\in \Gamma)=1-o(1)$ if $\beta$ is above this threshold and $P_{\SIBM}(\sigma\in \Gamma)=o(1)$ if $\beta$ is below this threshold.
We already know that
$
P_{\SIBM} \big(\dist(\sigma,\pm X)< kn/\log^{1/3}(n) \big) = 1- o(1) .
$
Therefore the following three statements are equivalent:
\begin{enumerate}
\item $P_{\SIBM}(\sigma\in \Gamma)$ has a sharp transitions from $0$ to $1$ at $\beta^\ast$.
\item $P_{\SIBM} \big( 1\le \dist(\sigma,\Gamma)< kn/\log^{1/3}(n) \big)$ has a sharp transitions from $1$ to $0$ at $\beta^\ast$.
\item $\frac{P_{\SIBM} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\SIBM}(\sigma= X)}$ has a sharp transitions from $\infty$ (or $\omega(1)$) to $0$ at $\beta^\ast$.
\end{enumerate}
Statements (2) and (3) are equivalent because $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f$ is a permutation function.
We will show that the above three statements are further equivalent to
  $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}$ has a sharp transitions from $\infty$ (or $\omega(1)$) to $0$ at $\beta^\ast$.

We first prove (4) and then show that it is equivalent to statement (3).
Instead of analyzing $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}$, we analyze $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}$ for a typical graph $G$.

Then,
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}
=\sum_{i=1}^n \frac{P_{\sigma|G} ( \sigma= X^{(\sim i)} )} {P_{\sigma|G}(\sigma= X)} .
$$
Given the ground truth $X$, a graph $G$ and a vertex $i\in[n]$, define
\begin{equation*}
A^r_i=A^r_i(G):=|\{j\in[n]\setminus\{i\}:\{i,j\}\in E(G), X_j=\omega^r \cdot X_i\} |, r=0, \dots, k-1
\end{equation*}
Then by \eqref{eq:isingma}, we have
\begin{align*}
\frac{P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)}
 = \sum_{r=1}^{k-1}\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i)
-\frac{2(k-1)\alpha\log(n)}{n} \Big) 
 = (1+o(1)) \sum_{r=1}^{k-1}\exp (k \beta(A^r_i-A^0_i))  ,
\end{align*}
where the second equality holds with high probability because $|A^r_i-A^0_i|=O(\log(n))$ with probability $1-o(1)$.
By definition,
$A^0_i\sim \Binom(\frac{n}{k}-1,\frac{a\log(n)}{n})$ and $A^r_i\sim \Binom(\frac{n}{k}, \frac{b\log(n)}{n})$ for $r=1,\dots, k-1$, and they are independent. Therefore,
\begin{align*}
E_G[\exp (k \beta (A^r_i-A^0_i))]
& =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^{k\beta} \Big)^{n/k}
\Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-k\beta} \Big)^{n/k-1}  \\
& = 
\exp\Big(\frac{\log(n)}{k} ( a e^{-k\beta}+b e^{k\beta} -a-b )
+o(1) \Big) 
 = (1+o(1)) n^{g(\beta)-1} ,
\end{align*}
where $E_G$ means that the expectation is taken over the randomness of $G$, and the function
$g(\beta)  := \frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1$ is defined in Theorem~\ref{thm:wt3}.
As a consequence,
\begin{equation} \label{eq:mew}
E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} \Big]
= (1+o(1)) \sum_{i=1}^n\sum_{r=1}^{k-1} E_G[\exp (2 \beta (A^r_i-A^0_i))]
= (k-1+o(1)) n^{g(\beta)} .
\end{equation}
One can show that $g(\beta)$ is a convex function and takes minimum at $\beta=\frac{1}{2k}\log\frac{a}{b}$, so $g(\beta)$ is strictly decreasing in the interval $(0,\frac{1}{2k}\log\frac{a}{b})$. Furthermore, $\beta^\ast$ is a root of $g(\beta)=0$, and $0<\beta^\ast<\frac{1}{2k}\log\frac{a}{b}$, so $g(\beta)>0$ for $\beta<\beta^\ast$ and $g(\beta)<0$ for $\beta^\ast<\beta<\frac{1}{2k}\log\frac{a}{b}$.
Taking this into the above equation, we conclude that the expectation of $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}$ has a sharp transition from $\omega(1)$ to $o(1)$ at $\beta^\ast$.
This at least intuitively explains why $\beta^\ast$ is the threshold. However, in order to formally establish statement (4) above, we need to prove that this sharp transition happens for a typical graph $G$, not just for the expectation.
Moreover, $g(\beta)$ is an increasing function in the interval $\beta\in(\frac{1}{2k}\log\frac{a}{b}, +\infty)$, so the expectation first decreases in the interval $(0,\frac{1}{2k}\log\frac{a}{b}]$ and then starts increasing. We will prove that there is a ``cut-off" effect when $\beta>\frac{1}{2k}\log\frac{a}{b}$, i.e., although the expectation becomes much larger than $n^{g(\frac{1}{2k}\log\frac{a}{b})}$, for a typical graph $G$, we always have
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} 
= O( n^{g(\frac{1}{4}\log\frac{a}{b})} ) =o(1)
$$
whenever $\beta>\frac{1}{2k}\log\frac{a}{b}$.
Below we divide the proof into three cases: (i) $\beta\in(0,\beta^\ast]$, (ii) $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$, and (iii) $\beta\in(\frac{1}{2k}\log\frac{a}{b},+\infty)$.
Case (ii) is the simplest case, and its proof is essentially an application of Markov inequality, so we start with this case.

\subsection{Proof for $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$: An application of Markov inequality}
\label{sect:simreg}

We know that $g(\beta)<0$ for $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$. By \eqref{eq:mew} and Markov inequality, for almost all\footnote{By almost all $G$, we mean there is a set $\cG$ such that $P(G\in\cG)=1-o(1)$ and for every $G\in\cG$ certain property holds. The probability $P(G\in\cG)$ is calculated according to SSBM defined in Definition~\ref{def:SSBM}.} $G$, we have $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}=o(1)$. This proves that $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}=o(1)$ in this interval. With a bit more extra effort, let us also prove that $\frac{P_{\SIBM} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\SIBM}(\sigma= X)}=o(1)$.

For each $I$, we introduce a vector $v$ with $\mathrm{dim}(v) = |I|=r$. Each element of $v$ takes from $\{\omega, \omega^2, \omega^{k-1}\}$.
$X^{(\sim I, v)}$ is defined as flipping $i \in I$ as : $X_i \to v_{\mathrm{index}(i)}\cdot X_i$.
For example, if $I = {1,3}, n=3, v=(\omega^2, \omega)$ then $X^{(\sim I,v)} = (\omega^2 \cdot X_1, X_2, \omega \cdot X_3)$.
Then $P_{\sigma|G}(\sigma = X^{(\sim I)})=\sum_{v}P_{\sigma|G}(\sigma = X^{(\sim I,v)})$

By definition,
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = r )}{P_{\sigma|G}(\sigma= X)}
=\sum_{\cI\subseteq[n],|\cI|=r} \frac{P_{\sigma|G} ( \sigma= X^{(\sim \cI)} )} {P_{\sigma|G}(\sigma= X)} .
$$
Similarly to $A_i$ and $A^r_i$, for a set $\cI\subseteq[n]$, we define
define
$A_{\cI}=A_{\cI}(G):=|\{i,j\}\in E(G):  \{i, j\} \not\subseteq [n]\setminus\cI, X_j=X_i\}|$ and  
$B_{\cI,v}=B_{\cI,v}(G):=|\{\{i,j\}\in E(G): \{i, j\} \not\subseteq [n]\setminus\cI, X_j= v_{\mathrm{index}(i)} \cdot X_i\}|$.
Then by \eqref{eq:isingma} one can show that
$$
 \frac{P_{\sigma|G}(\sigma=X^{(\sim\cI)})}{P_{\sigma|G}(\sigma=X)} 
\le \sum_{v}\exp\Big( k\big(\beta+\frac{\alpha\log(n)}{n} \big) (B_{\cI,v}-A_{\cI})
\Big) = \sum_{v}\exp ( k (\beta + o(1)) (B_{\cI,v}-A_{\cI})
 ) .
$$
Since we are only interested in the case $|\cI|<kn/\log^{1/3}(n)=o(n)$,
by definition we have $A_{\cI}\sim\Binom((\frac{n}{k}-o(n))|\cI|,\frac{a\log(n)}{n})$ and $B_{\cI}\sim\Binom((\frac{n}{k}-o(n))|\cI|,\frac{b\log(n)}{n})$, and they are independent. Therefore,
\begin{align*}
E_G[\exp ( k (\beta +o(1)) (B_{\cI,v}-A_{\cI}) ) ] = &
\exp\Big(\frac{|\cI|\log(n)}{k} ( a e^{-k\beta}+b e^{k\beta} -a-b +o(1) )
 \Big) \\
 = & n^{|\cI|(g(\beta)-1+o(1))} .
\end{align*}
As a consequence,
\begin{equation} \label{eq:nn}
E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = r)}{P_{\sigma|G}(\sigma= X)} \Big]
\le \sum_{\cI\subseteq[n],|\cI|=r} 
n^{k (g(\beta)-1+o(1))}
= \binom{n}{r} (k-1)^r n^{r (g(\beta)-1+o(1))}
< (k-1)^rn^{r (g(\beta)+o(1))} .
\end{equation}
Then by Markov inequality, there is a set $\cG^{(r)}$ such that $P(G\in\cG^{(r)})=1-(k-1)^rn^{r g(\beta)/4}$ and for every $G\in\cG^{(r)}$, $\frac{P_{\sigma|G} ( \dist(\sigma, X) = k )}{P_{\sigma|G}(\sigma= X)} \le (k-1)^rn^{r g(\beta)/2}$.
Let $\cG=\cap_{r=1}^{kn/\log^{1/3}(n)} \cG^{(r)}$. By union bound, we have $P(G\in\cG)>1-\sum_{r=1}^\infty (k-1)^rn^{r g(\beta)/4} = 1-o(1)$. Moreover, for every $G\in\cG$,
$\frac{P_{\sigma|G} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\sigma|G}(\sigma= X)} < \sum_{r=1}^\infty (k-1)^rn^{r g(\beta)/2} = o(1)$.
This proves that $\frac{P_{\SIBM} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\SIBM}(\sigma= X)}=o(1)$, and so $P_{\SIBM}(\sigma \in \Gamma)=1-o(1)$ when $\beta\in(\beta^\ast,\frac{1}{2k}\log\frac{a}{b}]$.


\subsection{Proof for $\beta\in(\frac{1}{2k}\log\frac{a}{b},+\infty)$: The ``cut-off" effect}

The analysis in this interval is more delicate. Since $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} 
= (1+o(1)) \sum_{i=1}^n \sum_{r=1}^{k-1}\exp (k\beta (A^r_i-A^0_i))$, we start with a more careful analysis of $\sum_{i=1}^n \exp (2 \beta (A^r_i-A^0_i))$.
Since $A^r_i-A^0_i$ takes integer value between $-n/k$ and $n/k$,
we can use indicator functions to write
\begin{align*}
 \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{n/k}
\mathbbm{1}[A^r_i-A^0_i=t \log(n)] \exp (k\beta t \log(n) ) ,
\end{align*}
where the quantity $t\log(n)$ ranges over all integer values from $-n/k$ to $n/k$ in the summation.
Define $D(G,t):=|\{i\in[n]:A^r_i-A^0_i= t\log(n)\}|=\sum_{i=1}^n \mathbbm{1}[A^r_i-A^0_i=t \log(n)]$.
Therefore,
\begin{equation}  \label{eq:gour}
 \sum_{i=1}^n \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{n/k}
D(G,t) \exp (k\beta t \log(n) ) .
\end{equation}
By Chernoff bound, we have $P(A^r_i-A^0_i\ge 0)\le \exp\big(\log(n)(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big)$.
Define $\cG_1:=\{G:A^r_i-A^0_i< 0~\forall i\in[n]\}$. Then by union bound, $P(G\notin\cG_1)\le\exp\big(\log(n)(1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big) = o(1)$, where the equality follows from the assumption that $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
For every $G\in\cG_1$, $D(G,t)=0$ for all $t\ge 0$, and so
\begin{equation} \label{eq:duj}
 \sum_{i=1}^n \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{-1}
D(G,t) \exp (k\beta t \log(n) ) .
\end{equation}
This indicates that there is a ``cut-off" effect at $t>0$, i.e., $D(G,t) \exp (2\beta t \log(n) )=0$ for all positive $t$ with probability $1-o(1)$, although its expectation can be very large, as we will show next.
Define a function
$$
f_{\beta}(t):=\frac{1}{k}\sqrt{k^2t^2+4ab} -t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big) -\frac{a+b}{k} +1 +k\beta t.
$$
Using Chernoff bound, one can show that 
$$
E[D(G,t)
\exp\big(2\beta t \log(n) \big)]
\le \exp( f_{\beta}(t) \log(n) ) .
$$
with probability one (Using a more careful analysis, one can show that this bound is tight up to a $\frac{1}{\sqrt{\log(n)}}$ factor; see Appendix~\ref{ap:um}.)
The function $f_{\beta}(t)$ is a concave function and takes maximum value at $t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k}$, and its maximum value is $f_{\beta}(t^\ast)=\frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1
=g(\beta)$.
Therefore, if we take expectation on both sides of \eqref{eq:gour}, then the sum on the right-hand side is concentrated on a small neighborhood of $t^\ast$. When $\beta>\frac{1}{2k}\log\frac{a}{b}$, we have $t^\ast>0$. Due to the ``cut-off" effect at $t>0$, we have $D(G,t)=0$ for all $t$ in the neighborhood of $t^\ast$ with probability $1-o(1)$, so the main contribution to the expectation comes from a rare event $G\notin\cG_1$. This explains why the behavior of a typical graph $G$ deviates from the behavior of the expectation.
Since $f_{\beta}(t)$ is a concave function, the sum $E[\sum_{t\log(n)=-n/k}^{-1}
D(G,t) \exp (k\beta t \log(n) )]$
is upper bounded by $O(\log(n))n^{f_{\beta}(0)}$  when $t^\ast>0$.
Notice that $f_{\beta}(0)=g(\frac{1}{2k}\log\frac{a}{b})=1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}<0$.
Now using \eqref{eq:duj} and Markov inequality, we conclude that $\sum_{i=1}^n \exp (2 \beta (A^r_i-A^0_i))=o(1)$ for almost all $G$,
and so $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} =o(1)$ for almost all $G$. Thus we have shown that $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}=o(1)$ when
$\beta>\frac{1}{2r}\log\frac{a}{b}$.
The analysis of $\frac{P_{\SIBM} ( \dist(\sigma, X) = r )}{P_{\SIBM}(\sigma= X)}$ for $1\le r<kn/\log^{1/3}(n)$ is
similar to the analysis in Section~\ref{sect:simreg}, and we do not repeat it here. By now we have given a sketched proof of
$P_{\SIBM}(\sigma \in \Gamma)=1-o(1)$ when $\beta>\beta^\ast$; see Section~\ref{sect:equal} for a rigorous proof. Next we move to the case $\beta\le\beta^\ast$.

\subsection{Proof for $\beta\le\beta^\ast$: Structural results and tight concentration}

In the last inequality of \eqref{eq:nn}, we use a coarse bound $\binom{n}{r}<n^r$. Now let us use a tighter bound $\binom{n}{r}<n^r/(r!)$. 
For $r>n^{g(\beta)+\delta}$, we have
$
r!>(r/e)^r
=\exp(r\log(r)-r)
>\exp(r(g(\beta)+\delta)\log(n)-r)
=n^{r(g(\beta)+\delta-o(1))} .
$
Taking these into the last inequality of \eqref{eq:nn}, we obtain that for all $r>n^{g(\beta)+\delta}$,
$$
E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = r )}{P_{\sigma|G}(\sigma= X)} \Big]
\le  (k-1)^r\binom{n}{k} n^{k (g(\beta)-1+o(1))}
< n^{r (g(\beta)+o(1))} /(r!) 
< (k-1)^rn^{-r(\delta-o(1))} .
$$
This immediately implies that  $P_{\SIBM} (\dist(\sigma, \Gamma)<n^{g(\beta)+\delta} ) = 1- o(1)$ for any $\delta>0$. Since $g(\beta)<1$ for all $0<\beta\le\beta^\ast$, we have $P_{\SIBM} (\dist(\sigma, \Gamma)<n^{\theta} ) = 1- o(1)$ for all $\theta\in (g(\beta), 1)$. This improves upon the upper bound 
$\dist(\sigma, \Gamma)< kn/\log^{1/3}(n)$ we obtained using spectral method at the beginning of this section.

More importantly, this allows us to prove a powerful structural result. (All the discussions below are conditioning on the event $\dist(\sigma,X)\le n/k$, i.e., $\sigma$ is closer to $X$ than to $\Gamma\backslash\{X\}$.) We say that $j$ is a ``bad" neighbor of vertex $i$ if the edge $\{i,j\}$ is connected in graph $G$ and $\sigma_j\neq X_j$. Then there is an integer $z>0$ such that with probability $1-o(1)$, every vertex has at most $z$ ``bad" neighbors.
Conditioning on the event every vertex has at most $z$ ``bad" neighbors, it is easy to show that $P_{\sigma|G}(\sigma_i \neq X_i)$ differs from $\sum_{r=1}^{k-1}\exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor $\exp(k^2\beta z)$.
Therefore, $E_{\sigma|G}[\dist(\sigma,X)]=\sum_{i=1}^n P_{\sigma|G}(\sigma_i \neq X_i)$ differs from $\sum_{r=1}^{k-1} \sum_{i=1}^n\exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor for almost all $G$.
We can further prove that $\dist(\sigma,X)$ concentrates around its expectation. Thus we conclude that $\dist(\sigma,X)$ differs from $\sum_{r=1}^{k-1} \sum_{i=1}^n\exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor for almost all $G$.
Quite surprisingly, when $\beta\le\beta^\ast$, we can prove a very tight concentration around the expectation: For almost all graph $G$, we have $\sum_{i=1}^n\exp (k \beta (A^r_i-A^0_i))=(1+o(1))n^{g(\beta)}$; see Proposition~\ref{prop:con} in Section~\ref{sect:struct} for a proof. Combining this with the above analysis, we conclude that $\dist(\sigma,X)=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ when $\beta\le\beta^\ast$. This completes the sketched proof of Theorem~\ref{thm:wt3}.
See Sections~\ref{sect:theta} and Sections~\ref{sect:struct} for the rigorous proof of the above arguments.

\subsection{Multiple sample case: Proof of Theorem~\ref{thm:wt2}}
\label{sect:multi}

\begin{center}
	\begin{minipage}{.55\textwidth}
		\begin{algorithm}[H]
			\caption{\texttt{LearnSIBM} in $O(n)$ time} \label{alg:ez}
			Inputs: the samples $\sigma^{(1)},\sigma^{(2)}\dots,\sigma^{(m)}$ \\
			Output: $\hat{X}$
			\begin{algorithmic}[1]
				\Statex \hspace*{-0.3in} 
				{\bf Step 1: Align all the samples with $\sigma^{(1)}$ }
				\For {$j=2,3,\dots,m$}
				\State $f=\arg\max_{f_{\gamma}} \sum_{i=1}^n I(f_{\gamma}(\sigma^{(j)}_i), \sigma^{(1)}_i)$
				\State $\sigma^{(j)} \gets f(\sigma^{(j)})$
				\EndFor
				\Statex \hspace*{-0.3in}
				{\bf Step 2: Majority vote at each coordinate}
				\For {i=1,2,\dots,n}
				\State $g(r) = |\{j | \sigma^{(j)}_i = \omega^r,1\leq j \leq m\}|$ for $ 0 \leq r \leq k-1$
				\State $\hat{X}_i \gets  \arg\max_r \omega^{g(r)}$
				\State \Comment{If the max of $g(r)$ is not unique, assign $\hat{X}_i$ randomly to one of \texttt{argmax}}
				\EndFor
				\State Output $\hat{X}$
			\end{algorithmic}
		\end{algorithm}
	\end{minipage}
\end{center}
For the multiple-sample case, we prove that the above simple algorithm can recover $X$ with probability $1-o(1)$ if and only if the Maximum Likelihood (ML) algorithm recovers $X$ with probability $1-o(1)$. We already showed that each sample is either very close to $\Gamma$, so after the alignment step in Algorithm~\ref{alg:ez}, all the samples are either simultaneously aligned with one of $\Gamma$. WLOG, we assume all samples are aligned with $X$.
By the structural results discussed above, with probability $1-o(1)$, $P_{\sigma|G}(\sigma_i^{(j)} \neq X_i)$ differs from $\sum_{r=1}^{k-1} \exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor for all $j\in[m]$. Since the samples are independent, we further obtain that $P_{\sigma|G}(\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i]\ge u)$ differs from $\sum_{r=1}^{k-1} \exp (k u \beta (A^r_i-A^0_i))$
by at most a constant factor.
Here $u\beta$ plays the role of $\beta$ in the single-sample case.
Therefore, if $u\beta>\beta^\ast$, then with probability $1-o(1)$ we have $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i] \le u-1$, or equivalently, $X_i(\sum_{j=1}^m \sigma_i^{(j)} ) \ge m-2u+2$ for all $i\in[n]$. Let $u=\lfloor \frac{m+1}{2} \rfloor$,
then we have $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i] \le \lfloor \frac{m-1}{2} \rfloor $ while $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} = X_i]
= m - \lfloor \frac{m-1}{2} \rfloor > \sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i] $
which implies that $\hat{X}=X$ after the majority voting step in Algorithm~\ref{alg:ez}. See Section~\ref{sect:direct} for a rigorous proof of the above argument.

The proof of the converse results, i.e., even ML algorithm cannot recover $X$ with probability $1-o(1)$ when $\lfloor \frac{m+1}{2} \rfloor \beta < \beta^\ast$, also relies on the structural result and it is rather similar to the proof of $\beta\le\beta^\ast$ for the single-sample case. We refer the readers to Section~\ref{sect:converse} for details.


\bibliographystyle{IEEEtran}
\bibliography{exportlist}
\end{document}

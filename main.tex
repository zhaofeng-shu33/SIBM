\label{key}\documentclass[conference]{IEEEtran}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{theorem}{Theorem}%[section]
\newtheorem{definition}{Definition}%[section]
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\SIBM}{SIBM}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cG}{\mathcal{G}}
\usepackage{bbm} % provide mathbbm
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Binom}{Binom}

\title{Stochastic Ising Block Model on Multiple Communities}
\author{%
	\IEEEauthorblockN{Feng Zhao}
	\IEEEauthorblockA{Department of Electronic Engineering\\
	Tsinghua University\\ 
	Beijing, China 100084\\
	Email: zhaof17@mails.tsinghua.edu.cn}	
	\and
	\IEEEauthorblockN{Shao-Lun Huang}
	\IEEEauthorblockA{DSIT Research Center\\
	Tsinghua-Berkeley Shenzhen Institute\\
	Shenzhen, China 518055\\
	Email: shaolun.huang@sz.tsinghua.edu.cn}
}
\begin{document}
\maketitle
\begin{abstract}
 Stochastic Block Model (SBM) is an important model in community detection while Ising model arises from ferromagnetism in statistical physics.
 In this paper, we will concatenate SBM with Ising model and propose Stochastic Ising Block Model (SBIM) on multiple communities.
 The random graph, generated by SBM, is used to generate Ising samples and our goal is to recover the label of the communities.
 For the exact recovery problem on SIBM, the phase transition property is established and the sample complexity is computed.
\end{abstract}
\section{Introduction}
% first paragraph: short intro to SBM and Ising model
In network analysis, Stochastic Block Model (SBM) is a commonly used random graph model, in which the probability of edge existence is higher within the community than between different communities \cite{holland1983stochastic}. For SBM, the condition to fully recover community labels has been investigated thoroughly and some phase transition property has been established \cite{Abbe17}. Meanwhile, Ising model is a well-known statistical model in physics which also has some kind of phase transition property \cite{ising1925beitrag}. The particles in Ising model interact with neighbors and change its state to achieve a steady state.
As a probabilistic model, Ising model is not limited to statistical physics and there are already some works to apply Ising model to investigate the social voting phenomenon \cite{sznajd2000opinion}.

In the original formulation, each particle in Ising model has only two states. Potts extended it to multiple states \cite{potts1952some}. This multiple-state Ising model
corresponds to SBM with multiple communities, thus enabling us to propose a concatenated network model: Stochastic Ising Block Model (SIBM). For 2-community case, there are already some results \cite{ye2020exact}, but little is known for multiple communities case. In this paper, we will give the formulation of SIBM on multiple
communities. We can generate multiple samples from SIBM, which more or less differ from the original labels, depending on the choice of parameters. Using the generated samples, we focus on the problem of whether it is possible to exactly recover the label. By investigating the phase transition property for our compose model,
we can compute the feasible regime of parameters for exact recovery and the corresponding sample complexity to accomplish the recovery goal.

This paper is divided as follows: In Section \ref{s:Preliminaries} the SIBM is formulated mathematically;
In Section \ref{s:trans}, the main results are introduced;
In Section \ref{s:conclusion}, the conclusion is given. Proof sketch of our main results is provided in Appendix.

% notation convetion
Within this paper, the number of community is denoted by $k$; $m$ is the number of samples; $\lfloor x \rfloor$ is the floor function of $x$; the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$;
the label of each node is a random variable $X_i$; $X_i$ is chosen from $W= \{1, \omega, \dots, \omega^{k-1}\}$ and we further require $W$
is a cyclic group with order $k$; $W^n$ is the n-ary Cartesian power of $W$; $f$ is a permutation function on $W$ and applied to $W^n$ in elementwise way; the set $\Gamma$ is used to represent all permutation functions on $W$ and $\Gamma(\sigma):=\{f(\sigma)| f\in \Gamma\}$ for $\sigma \in W^n$; the indicator function $I(x, y)$ is defined as
$I(x, y) = k-1 $ when $x=y$, and $I(x,y)=-1$ when $x\neq y$; $g(n) = \Theta(f(n))$ if there exists constant $c_1 < c_2$ such that $c_1 f(n) \leq g(n) \leq c_2 f(n)$
for large $n$;
$\Lambda := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k-1\}$
where $\mathbf{1}_n$ is the all one vector with dimension $n$;
we define the distance of two vectors as:
$\dist(\sigma, X)
=|\{i\in[n]:\sigma_i\neq X_i\}| \textrm{ for } \sigma,X\in W^n
$ and the distance of a vector to a space $S\subseteq W^n$
as
$\dist(\sigma,S)
:=\min\{\dist(\sigma, \sigma') | \sigma' \in S\}
$.

\section{Mathematical Model} \label{s:Preliminaries}
We first recall the definition of Symmetric Stochastic Block Model (SSBM) with $k$ communities \cite{Abbe17} and the definition of Ising model with $k$ states.
\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
Let $0\leq p<q\leq 1$, $X=(X_1,\dots,X_n)\in W^n$ and $G=([n],E(G))$. The pair $(X,G)$ is drawn under $\SSBM(n,k,p,q)$ if 
\begin{enumerate}
\item $X$ is drawn uniformly with the constraint that $|\{v \in [n] : X_v = u\}| = \frac{n}{k}$ for $u\in W$;

\item There is an edge between the vertices $i$ and $j$ with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$; the existence of each edge is independent with each other.
\end{enumerate}
\end{definition}
From the symmetric property of SBM, the conditional distribution $P(G|X=x) = P(G|X=f(x)), \forall f \in \Gamma$. Therefore, it is only possible to recover $X$ from $G$ up to a global permutation. That is, it is only possible to recover $\Gamma(X)$.

In this paper, we focus on the regime of $p=a\log(n)/n$ and $q=b\log(n)/n$, where $a>b> 0$ are constants. In this regime, it is well known that exact recovery of $X$ (up to a global permutation) from $G$ is possible if $\sqrt{a}-\sqrt{b} > \sqrt{k}$ \cite{abbe2015exact}.
 
Given a labeling $X$ of $n$ vertices, the SBM specifies how to generate a random graph on these $n$ vertices according to the labeling. In some sense, Ising model works in the opposite direction, i.e., given a graph $G=([n],E(G))$, Ising model defines a probability distribution on all possible labels $W^n$ of these $n$ vertices. 

 
\begin{definition}[Ising model with $k$ states]
The Ising model on a graph $G=([n],E(G))$ with parameters $\alpha,\beta>0$ is a probability distribution on the configurations $\sigma\in W^n$ such that
\begin{align} \label{eq:isingma}
&P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
\exp\Big(\beta \sum_{\{i,j\}\in E(G)} I(\bar{\sigma}_i ,\bar{\sigma}_j)\notag\\
&-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} I(\bar{\sigma}_i, \bar{\sigma}_j)\Big)
\end{align}
where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and
$Z_G(\alpha,\beta)$ is the normalizing constant for this distribution.
\end{definition}

When $\alpha=0$ and $k=2$, Equation \eqref{eq:isingma} gives the standard definition for Ising Model.
For our specific problems, $\alpha > 0$ is needed to guarantee that the distribution is not concentrated within
$\Lambda$. The definition of indicator function $I(x, y)$ is chosen such that the number of summation terms is balanced: $\sum_{\{i,j\}\in E(G)} | I(\bar{\sigma}_i ,\bar{\sigma}_j) | = (\frac{n}{k} - 1)(k-1)$ and $\sum_{\{i,j\}\not\in E(G)} | I(\bar{\sigma}_i ,\bar{\sigma}_j)| = \frac{n}{k} (k-1)$ for each fixed $i$. Therefore, the two
quantities are approximately equal when $n$ is large.
%By definition of Ising model we also have $P_{\sigma|G}(\sigma=\bar{\sigma})=P_{\sigma|G}(\sigma=f(\bar{\sigma}))$. This symmetric property corresponds with that of SBM.

Next we present our new model, the Stochastic Ising Block Model (SIBM), which can be viewed as a natural composition of the SSBM and the Ising model. In SIBM, we first draw a pair $(X,G)$ under $\SSBM(n,k,p,q)$.  Then we draw $m$ independent samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ from the Ising model on the graph $G$, where $\sigma^{(u)}\in W^n$ for all $u\in[m]$.

\begin{definition}[Stochastic Ising Block Model]
The triple $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ is drawn under $\SIBM(n,k, p,q,\alpha,\beta,m)$ if

\noindent
(i) the pair $(X,G)$ is drawn under $\SSBM(n,k, p,q)$;

\noindent
(ii) for every $i\in[m]$, each sample $\sigma^{(i)}=(\sigma_1^{(i)},\dots,\sigma_n^{(i)}) \in W^n$ is drawn independently according to the distribution in Equation \eqref{eq:isingma}.
\end{definition}

Notice that we only draw the graph $G$ once in SIBM, and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ are drawn independently from the Ising model on the {\em same} graph $G$.
Our objective is to recover the underlying partition (or the ground truth) $X$ from the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$, and we would like to use the smallest possible number of samples to guarantee the exact recovery of $X$ up to a global permutation.
Below we use the notation $P_{\SIBM}(A):=E_G[P_{\sigma|G}(A)]$ for an event $A$, where the expectation $E_G$ is taken with respect to the distribution given by SSBM. In other words, $P_{\sigma|G}$ is the conditional distribution of  $\sigma$ given a fixed $G$ while $P_{\SIBM}$ is the joint distribution of both $\sigma$ and $G$.
By definition, we have $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f\in \Gamma$.


\begin{definition}[Exact recovery in SIBM]
Let $(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n,k,p,q,\alpha,\beta,m)$.
We say that exact recovery is solvable for $\SIBM(n,k,p,q,\alpha,\beta,m)$ if there is an algorithm that takes $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ as inputs and outputs $\hat{X}=\hat{X}(\{\sigma^{(1)},\dots,\sigma^{(m)}\})$ such that
$$
P_{\SIBM}(\hat{X} \in \Gamma(X)) \to 1
\text{~~~as~} n\to\infty
$$
and we call $P_{\SIBM}(\hat{X} \in \Gamma(X))$ the success probability of the recovery algorithm.
\end{definition}

By definition, the ground truth $X$, the graph $G$ and the samples $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$ form a Markov chain $X\to G\to \{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Therefore, if we cannot recover $X$ from $G$, then there is no hope to recover $X$ from $\{\sigma^{(1)},\dots,\sigma^{(m)}\}$. Thus a necessary condition for the exact recovery in SIBM is $\sqrt{a}-\sqrt{b}> \sqrt{k}$, and we will limit ourselves to this case throughout the paper.

\section{Phase Transition and Sample Complexity of SIBM}\label{s:trans}
Our main Problem is to investigate what is the smallest sample size $m^\ast$ such that exact recovery is solvable for $\SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m^\ast)$?

Our main results are as follows:

\begin{theorem} \label{thm:wt1}
For any $a,b> 0$ such that $\sqrt{a}-\sqrt{b}> \sqrt{k}$ and any $\alpha,\beta>0$, let
\begin{align} \label{eq:defstar}
&\beta^\ast \triangleq \frac{1}{k}
\log\frac{a+b-k-\sqrt{(a+b-k)^2-4ab}}{2 b}  \\
&m^\ast \triangleq 2 \Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor +1 \text{~~~and~~~}
\tilde{m} \triangleq \frac{k}{k-1}\Big\lfloor \frac{\beta^\ast}{\beta} \Big\rfloor
\end{align}
\begin{enumerate}
	\item We discuss the case when $\alpha > b \beta$:
	\begin{enumerate}
	\item If $m\ge m^\ast$, then exact recovery is solvable in $O(n)$ time for $\SIBM(n,k, a\log(n)/n, \linebreak[4] b\log(n)/n,\alpha,\beta,m)$.
	\item If $\beta^\ast/\beta$ is not an integer and $m < \tilde{m} + \frac{1}{k-1}$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$.
	\item If $\beta^\ast/\beta$ is an integer and $m < \tilde{m} -1$, then the success probability of all recovery algorithms approaches $0$ as $n\to\infty$.
	\end{enumerate}
	\item When $\alpha < b \beta$, exact recovery of $\SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m)$ is not solvable for any $m=O(\log^{1/4}(n))$.
\end{enumerate}

\end{theorem}
Note that the condition $\sqrt{a}-\sqrt{b} > \sqrt{k}$ guarantees that the term $\sqrt{(a+b-k)^2-4ab}$ in the definition of $\beta^\ast$ is a real number.
When $\alpha > b \beta$,
the above theorem establishes a recovery threshold
for the regime of $m \geq m^\ast$ and $m < \tilde{m}$ (approximately) on the number of samples. Since $\tilde{m} < m^*$,
When $m \in (\tilde{m}, m^\ast)$, we do not know whether it is possible to recover $X$ or not.
However, when $k=2$ and $\beta^\ast/\beta$ is not an integer, it can be seen that the threshold is sharp since the interval $(\tilde{m}, m^\ast + \frac{1}{k-1})=\emptyset$.

It is worth mentioning that the threshold values $m^\ast$ and $\tilde{m}$ do not depend on the parameter $\alpha$, as long as $\alpha>b\beta$.
Below we present an equivalent characterization of the recovery threshold in terms of $\beta$.
\begin{theorem} \label{thm:wt2}
	Let $a,b,\alpha,\beta> 0$ be constants satisfying $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$. 
	Let 
	$
	(X,G,\{\sigma^{(1)},\dots,\sigma^{(m)}\}) \sim \SIBM(n, k, a\log(n)/n, b\log(n)/n,\alpha,\beta,m).
	$
	If $\lfloor \frac{m+1}{2} \rfloor \beta>\beta^\ast$, then there is an algorithm that recovers $X$ from the samples in $O(n)$ time with success probability $1-o(1)$. If $\lfloor \frac{(k-1)(m+1)}{k} \rfloor \beta <\beta^\ast$, then the success probability of any recovery algorithm is $o(1)$. 
\end{theorem}
Using the property of floor function, we can show that Theorem~\ref{thm:wt1} and Theorem~\ref{thm:wt2} are equivalent when $\alpha > b \beta$.
%$\beta$ represents the force of attraction between connected node in the graph. Theorem \ref{thm:wt2} states that
%to recover the label, the attraction force has a minimal threshold which depends on $m, \beta^*$.
When $m=1$, we have the next theorem:

\begin{theorem}  \label{thm:wt3}
Let $a,b,\alpha,\beta> 0$ be constants satisfying $\sqrt{a}-\sqrt{b} > \sqrt{k}$ and $\alpha>b\beta$.
Let 
$
(X,G,\{\sigma\}) \sim \SIBM(n,k, a\log(n)/n, b\log(n)/n,\alpha,\beta,1).
$
Define $g(\beta)  := \frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1$.
If $\beta>\beta^\ast$, then
$$
P_{\SIBM}(\sigma \in \Gamma(X)) = 1-o(1).
$$
If $\beta\le \beta^\ast$, then $0\leq g(\beta) < 1$ and
$$
P_{\SIBM}(\dist(\sigma, \Gamma(X))= \Theta(n^{g(\beta)})) = 1-o(1)
$$
\end{theorem}

The above theorem established the phase transition property for SIBM model.
Roughly speaking, the sample $\sigma$ generated from SIBM aligns with $X$ with probability 1 if $\beta > \beta^*$;
otherwise the sample differs from $X$ with the number of coordinates in $\Theta(n^{g(\beta)})$.

If we could choose proper $\alpha, \beta$, then from Theorem \ref{thm:wt3} one sample generated from Ising model is enough to
estimate the original label. This provides a way to do community detection for SBM model. To some extend
SIBM provides the theoretical guarantee for such detection method.

\section{Conclusion}\label{s:conclusion}
In this paper, we derive the sample complexity for Stochastic Ising Block Model on multiple communities.
Our result shares insights on the relationship of SBM and Ising model and guides the design of efficient algorithms
for community detection problems.
\appendix
\section{Sketch of the proof}
\label{sect:sketch}

In this section, we illustrate the main ideas and explain the important steps in the proof of the main results.
The complete proofs are given in arxiv. Most proofs are similar with the case for two communities in \cite{ye2020exact}. The difference
is that we can not use matrix-vector product now and the critical values differ.

As the first step, we prove that for $a>b>0$, if $\alpha>b\beta$, then all the samples are centered in $\Gamma(X)$ with probability $1-o(1)$;
if $\alpha<b\beta$, then all the samples are centered in $\Lambda$.

We use the concentration results for adjacency matrices of random graphs with independent edges to prove this.
Let $A$ be the adjacency matrix of the graph $G$, $J_n$ be the all one matrix and $I_n$ be the identity matrix, both of size $n\times n$.
Define a matrix
$
M:= \big(\beta+\frac{\alpha\log(n)}{n} \big) E[A|X]
-\frac{\alpha\log(n)}{n} (J_n-I_n).
$
Then we can write \eqref{eq:isingma}
as
\begin{align*}
&P_{\sigma|G}(\sigma=\bar{\sigma})
= \frac{1}{Z_G(\alpha,\beta)}
\exp\Big( \frac{1}{2} \sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) + \\
& \frac{1}{2} \sum_{i,j}\big(\beta+\frac{\alpha\log(n)}{n} \big)  (A-E[A|X])_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j) 
\Big)
\end{align*}
One can show that if $\alpha>b\beta$, then the maximizers of $\sum_{i,j} M_{ij} I(\bar{\sigma}_i, \bar{\sigma}_j)$ consist of set $\Gamma(X)$, and if $\alpha<b\beta$, then the maximizer set is $\Lambda$.
Moreover, \cite{Hajek16} proved the concentration of $A$ around its expectation $E[A|X]$ in the sense that
the spectral norm $\|A-E[A|X]\| = O(\sqrt{\log(n)})$ with probability $1-o(1)$, we can further show that the error term above is bounded by
$$
\left|\frac{1}{2} \big(\beta+\frac{\alpha\log(n)}{n} \big)\sum_{i,j} (A-E[A|X])
 I( \bar{\sigma}_i, \bar{\sigma}_j) \right| = O \big( n \sqrt{\log(n)} \big)
$$
for all $\bar{\sigma}\in W^n$.
This allows us to prove that in both cases (no matter $\alpha>b\beta$ or $\alpha<b\beta$), the Hamming distance between the samples and the maximizers of $\sum_{i,j}  M_{ij}I(\bar{\sigma}_i, \bar{\sigma}_j)$ is upper bounded by $kn/\log^{1/3}(n)=o(n)$.
More precisely, if $\alpha>b\beta$, then $\dist(\sigma^{(i)},\Gamma(X) )< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$;
if $\alpha<b\beta$, then $\dist(\sigma^{(i)}, \Lambda)< kn/\log^{1/3}(n)$ for all $i\in[m]$ with probability $1-o(1)$;
In the latter case, each sample only take $\sum_{j=0}^{kn/\log^{1/3}(n)}\binom{n}{j}j^{k-1}$ values, so each sample contains at most $\log_k(\sum_{j=0}^{kn/\log^{1/3}(n)}\binom{n}{j}j^{k-1})=O(\frac{\log\log(n)}{\log^{1/3}(n)} n)$ bits of information about $X$. On the other hand, $X$ itself is uniformly distributed over a set of $\frac{n!}{(n/k)^k}$ vectors, so one needs at least $\log_k\frac{n!}{(n/k)^k}=\Theta(n)$ bits of information to recover $X$. Thus if $\alpha<b\beta$, then exact recovery of $X$ requires at least $\Omega(\frac{\log^{1/3}(n)}{\log\log(n)})\ge \Omega(\log^{1/4}(n))$ samples.

For the rest of this section, we will focus on the case $\alpha>b\beta$ and establish the threshold on the sample complexity. We first analyze the typical behavior of one sample and explain how to prove Theorem~\ref{thm:wt3}. Then we use the method developed for the one sample case to analyze the distribution of multiple samples, which allows us to prove Theorem~\ref{thm:wt2}. Note that Theorem~\ref{thm:wt1} follows directly from Theorem~\ref{thm:wt2}.

\subsection{Why is $\beta^\ast$ the threshold?} \label{sect:why}

Let us analyze the one sample case, i.e., we take $m=1$.
Theorem~\ref{thm:wt3} implies that $\beta^\ast$ is a sharp threshold for the event $\{\sigma \in \Gamma(X)\}$, i.e., $P_{\SIBM}(\sigma\in \Gamma(X))=1-o(1)$
if $\beta$ is above this threshold and $P_{\SIBM}(\sigma\in \Gamma(X))=o(1)$ if $\beta$ is below this threshold.
We already know that
$
P_{\SIBM} \big(\dist(\sigma,\Gamma(X))< kn/\log^{1/3}(n) \big) = 1- o(1) .
$
Therefore the following three statements are equivalent:
\begin{enumerate}
\item $P_{\SIBM}(\sigma\in \Gamma(X))$ has a sharp transitions from $0$ to $1$ at $\beta^\ast$.
\item $P_{\SIBM} \big( 1\le \dist(\sigma,\Gamma(X))< kn/\log^{1/3}(n) \big)$ has a sharp transitions from $1$ to $0$ at $\beta^\ast$.
\item $\frac{P_{\SIBM} ( 1\le \dist(\sigma, X)< kn/\log^{1/3}(n) )}{P_{\SIBM}(\sigma= X)}$ has a sharp transition from $\infty$ to $0$ at $\beta^\ast$.
\end{enumerate}
Statements 2) and 3) are equivalent because $P_{\SIBM}(\sigma=\bar{\sigma})=P_{\SIBM}(\sigma=f(\bar{\sigma}))$ for all $\bar{\sigma}\in W^n$ and $f \in \Gamma$.
We will show that the above three statements are further equivalent to

4)  $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}$ has a sharp transition from $\infty$ to $0$ at $\beta^\ast$.

We first prove 4) and then show that it is equivalent to statement 3).
Instead of analyzing $\frac{P_{\SIBM} ( \dist(\sigma, X) = 1 )}{P_{\SIBM}(\sigma= X)}$, we analyze $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}$ for a typical graph $G$.
For $\cI \subseteq [n]$, we denote the event $\sigma = X^{(\sim \cI)}$ as $\sigma_i \neq X_i$ for all $i \in \cI$ and $\sigma_i = X_i$ for all $i \not\in \cI$.
When $\cI$ only contains one element, e.g., $\cI=\{i\}$, we write the event $\sigma = X^{(\sim i)}$ instead of $\sigma = X^{(\sim\{i\})}$.
Then,
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}
=\sum_{i=1}^n \frac{P_{\sigma|G} ( \sigma= X^{(\sim i)} )} {P_{\sigma|G}(\sigma= X)} .
$$
Given the ground truth $X$, a graph $G$ and a vertex $i\in[n]$, define
\begin{equation*}
A^r_i=A^r_i(G):=|\{j\in[n]\setminus\{i\}:\{i,j\}\in E(G), X_j=\omega^r \cdot X_i\} |
\end{equation*}
for $ r=0, \dots, k-1$.
Then by Equation \eqref{eq:isingma}, we have
\begin{align}
&\frac{P_{\sigma|G}(\sigma=X^{(\sim i)} )}
{P_{\sigma|G}(\sigma=X)}
 = \sum_{r=1}^{k-1}\exp\Big(k\big(\beta+\frac{\alpha\log(n)}{n} \big) (A^r_i-A^0_i) \nonumber\\
&-\frac{2(k-1)\alpha\log(n)}{n} \Big) 
 = (1+o(1)) \sum_{r=1}^{k-1}\exp (k \beta(A^r_i-A^0_i)) \label{eq:kbetaA}
\end{align}
where the second equality holds with high probability because $|A^r_i-A^0_i|=O(\log(n))$ with probability $1-o(1)$.
By definition,
$A^0_i\sim \Binom(\frac{n}{k}-1,\frac{a\log(n)}{n})$ and $A^r_i\sim \Binom(\frac{n}{k}, \frac{b\log(n)}{n})$ for $r=1,\dots, k-1$, and they are independent. Therefore,
\begin{align}
&E_G[\exp (k \beta (A^r_i-A^0_i))]
 =\Big(1-\frac{b\log(n)}{n}+\frac{b\log(n)}{n} e^{k\beta} \Big)^{n/k} \nonumber \\
&\cdot \Big(1-\frac{a\log(n)}{n}+\frac{a\log(n)}{n} e^{-k\beta} \Big)^{n/k-1}\nonumber\\
& = 
\exp\Big(\frac{\log(n)}{k} ( a e^{-k\beta}+b e^{k\beta} -a-b )
+o(1) \Big)\nonumber \\
& = (1+o(1)) n^{g(\beta)-1} \label{eq:gbetaminus1}
\end{align}
where $E_G$ means that the expectation is taken over the randomness of $G$, and the function
$g(\beta)  := \frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1$ is defined in Theorem~\ref{thm:wt3}.
As a consequence,
\begin{align} \label{eq:mew}
&E_G \Big[ \frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} \Big]
= (1+o(1)) \notag\\
&\sum_{i=1}^n\sum_{r=1}^{k-1} E_G[\exp (2 \beta (A^r_i-A^0_i))]
= (k-1+o(1)) n^{g(\beta)} 
\end{align}
One can show that $g(\beta)$ is a convex function and takes minimum at $\beta=\frac{1}{2k}\log\frac{a}{b}$, so $g(\beta)$ is strictly decreasing in the interval $(0,\frac{1}{2k}\log\frac{a}{b})$. Furthermore, $\beta^\ast$ is a root of $g(\beta)=0$, and $0<\beta^\ast<\frac{1}{2k}\log\frac{a}{b}$, so $g(\beta)>0$ for $\beta<\beta^\ast$ and $g(\beta)<0$ for $\beta^\ast<\beta<\frac{1}{2k}\log\frac{a}{b}$.
Taking this into the above equation, we conclude that the expectation of $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)}$ has a sharp transition from $\infty$ to $o(1)$ at $\beta^\ast$.
This at least intuitively explains why $\beta^\ast$ is the threshold. However, in order to formally establish statement (4) above, we need to prove that this sharp transition happens for a typical graph $G$, not just for the expectation.
Moreover, $g(\beta)$ is an increasing function in the interval $\beta\in(\frac{1}{2k}\log\frac{a}{b}, +\infty)$, so the expectation first decreases in the interval $(0,\frac{1}{2k}\log\frac{a}{b}]$ and then starts increasing. We also need to show that
$$
\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} 
= o(1)
$$
when $\beta>\frac{1}{2k}\log\frac{a}{b}$.
Below we divide the proof into (i) $\beta\in(\beta^\ast,+\infty)$, (ii) $\beta\in(0,\beta^\ast]$.

\subsection{Proof for $\beta\in(\beta^*,+\infty)$}

We start with a more careful analysis of $\sum_{i=1}^n \exp (k \beta (A^r_i-A^0_i))$.
Since $A^r_i-A^0_i$ takes integer value between $-n/k$ and $n/k$,
we can use indicator functions to write
\begin{align*}
& \exp (k \beta (A^r_i-A^0_i))
= \\  \sum_{t\log(n)=-n/k}^{n/k}
& \mathbbm{1}[A^r_i-A^0_i=t \log(n)] \exp (k\beta t \log(n) ) ,
\end{align*}
where the quantity $t\log(n)$ ranges over all integer values from $-n/k$ to $n/k$ in the summation.
Define $D(G,t):=|\{i\in[n]:A^r_i-A^0_i= t\log(n)\}|=\sum_{i=1}^n \mathbbm{1}[A^r_i-A^0_i=t \log(n)]$.
Therefore,
\begin{equation}  \label{eq:gour}
 \sum_{i=1}^n \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{n/k}
D(G,t) \exp (k\beta t \log(n) ) .
\end{equation}
By Chernoff bound, we have $P(A^r_i-A^0_i\ge 0)\le \exp\big(\log(n)(-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big)$.
Define $\cG_1:=\{G:A^r_i-A^0_i< 0~\forall i\in[n]\}$. Then by union bound, $P(G\notin\cG_1)\le\exp\big(\log(n)(1-\frac{(\sqrt{a}-\sqrt{b})^2}{k} +o(1)) \big) = o(1)$, where the equality follows from the assumption that $\sqrt{a}-\sqrt{b}>\sqrt{k}$.
For every $G\in\cG_1$, $D(G,t)=0$ for all $t\ge 0$, and so
\begin{equation} \label{eq:duj}
 \sum_{i=1}^n \exp (k \beta (A^r_i-A^0_i))
=  \sum_{t\log(n)=-n/k}^{-1}
D(G,t) \exp (k\beta t \log(n) ) .
\end{equation}
Define a function
\begin{align*}
&f_{\beta}(t):=\frac{1}{k}\sqrt{k^2t^2+4ab} -t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big) \\
&-\frac{a+b}{k} +1 +k\beta t.
\end{align*}
Using Chernoff bound, one can show that 
$$
E[D(G,t)
\exp\big(2\beta t \log(n) \big)]
\le \exp( f_{\beta}(t) \log(n) ) .
$$
with probability one.
The function $f_{\beta}(t)$ is a concave function and takes maximum value at $t^\ast=\frac{b e^{k\beta}-a e^{-k\beta}}{k}$, and $f_{\beta}(t^\ast)=\frac{b e^{k\beta}+a e^{-k\beta}}{k}-\frac{a+b}{k}+1
=g(\beta)$. When $ \beta < \frac{1}{2k} \log \frac{a}{b}$, $t^\ast < 0$
for such case, if we take expectation on both sides of \eqref{eq:gour}, then the sum on the right-hand side is concentrated on a small neighborhood of $t^\ast$.
When $ \beta^* < \beta \leq \frac{1}{2k} \log \frac{a}{b}$ from Equation \eqref{eq:duj}, the right-hand is concentrated on the left neighborhood of $0$.
Therefore, for $G\in \cG_1$, the sum $E[\sum_{t\log(n)=-n/k}^{n/k}
D(G,t) \exp (k\beta t \log(n) )]$
is upper bounded by $O(\log(n))n^{f_{\beta}(0)}$  when $t^\ast>0$ and $O(\log(n))n^{f_{\beta}(t^\ast)}$  when $t^\ast<0$.
Notice that $f_{\beta}(0)=g(\frac{1}{2k}\log\frac{a}{b})=1-\frac{(\sqrt{a}-\sqrt{b})^2}{k}<0$. To simplify the notation we define
$$
\tilde{g}(\beta) = \begin{cases}
g(\beta)   & \text{~if~} \beta< \frac{1}{2k}\log\frac{a}{b} \\
g(\frac{1}{2k}\log\frac{a}{b}) & \text{~if~} \beta\ge \frac{1}{2k}\log\frac{a}{b}
\end{cases}
$$
Now using \eqref{eq:duj} and Markov inequality, we conclude that $\sum_{r=1}^{k-1}\sum_{i=1}^n \exp (k \beta (A^r_i-A^0_i))< n^{\tilde{g}(\beta)/2}$ for almost all $G$,
and so by Equation \eqref{eq:kbetaA}, $\frac{P_{\sigma|G} ( \dist(\sigma, X) = 1 )}{P_{\sigma|G}(\sigma= X)} < n^{\tilde{g}(\beta)/2}$ for almost all $G$. 
The analysis of $\frac{P_{\SIBM} ( \dist(\sigma, X) = r )}{P_{\SIBM}(\sigma= X)}$ for $1\le r<kn/\log^{1/3}(n)$ is
similar but with some notation difference, and we do not repeat it here.
The conclusion is $\frac{P_{\sigma|G} ( \dist(\sigma, X) = r )}{P_{\sigma|G}(\sigma= X)} < n^{r\tilde{g}(\beta)/2}$ for almost all $G$.
Using the union bound,
we can prove
$P_{\SIBM}(\sigma \in \Gamma)=1-o(1)$ when $\beta>\beta^\ast$ for almost all $G$.

\subsection{Proof for $\beta\le\beta^\ast$}\label{subsect:smaller}
We restrict our discussion on the case $\dist(\sigma,X)\le n/k$, i.e., $\sigma$ is closer to $X$ than to $\Gamma(X)\backslash\{X\}$.
We would show that $P(\dist(\sigma, X) = \Theta(n^{g(\beta)})) = 1 - o(1)$ conditioned on $\dist(\sigma,X)\le n/k$.
We say that $j$ is a ``bad" neighbor of vertex $i$ if the edge $\{i,j\}$ is connected in graph $G$ and $\sigma_j\neq X_j$. Then there is an integer $z>0$ such that with probability $1-o(1)$, every vertex has at most $z$ ``bad" neighbors.
Conditioning on the event every vertex has at most $z$ ``bad" neighbors, which happens with probability $1-o(1)$, it is easy to show that $P_{\sigma|G}(\sigma_i \neq X_i)$ differs from $\sum_{r=1}^{k-1}\exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor $\exp(k^2\beta z)$.
Therefore, $E_{\sigma|G}[\dist(\sigma,X)]=\sum_{i=1}^n P_{\sigma|G}(\sigma_i \neq X_i)$ differs from $\sum_{r=1}^{k-1} \sum_{i=1}^n\exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor for almost all $G$.
We can further prove that $\dist(\sigma,X)$ concentrates around its expectation. Thus we conclude that $\dist(\sigma,X)$ differs from $\sum_{r=1}^{k-1} \sum_{i=1}^n\exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor for almost all $G$.
When $\beta\le\beta^\ast$, we can prove a very tight concentration around the expectation. Then from Equation \eqref{eq:gbetaminus1}, for almost all graph $G$, we have $\sum_{i=1}^n\exp (k \beta (A^r_i-A^0_i))=(1+o(1))n^{g(\beta)-1}$.
Combining this with the above analysis, we conclude that $\dist(\sigma,X)=\Theta(n^{g(\beta)})$ with probability $1-o(1)$ when $\beta\le\beta^\ast$.
This completes the sketched proof of Theorem~\ref{thm:wt3}.

\subsection{Multiple sample case: Proof of Theorem~\ref{thm:wt2}}
\label{sect:multi}

		\begin{algorithm}
			\caption{\texttt{LearnSIBM} in $O(n)$ time} \label{alg:ez}
			Inputs: the samples $\sigma^{(1)},\sigma^{(2)}\dots,\sigma^{(m)}$ \\
			Output: $\hat{X}$
			\begin{algorithmic}[1]
				\Statex \hspace*{-0.3in} 
				{\bf Step 1: Align all the samples with $\sigma^{(1)}$ }
				\For {$j=2,3,\dots,m$}
				\State $f=\arg\max_{f_{\gamma}} \sum_{i=1}^n I(f_{\gamma}(\sigma^{(j)}_i), \sigma^{(1)}_i)$
				\State $\sigma^{(j)} \gets f(\sigma^{(j)})$
				\EndFor
				\Statex \hspace*{-0.3in}
				{\bf Step 2: Majority vote at each coordinate}
				\For {i=1,2,\dots,n}
				\State $g(r) = |\{j | \sigma^{(j)}_i = \omega^r,1\leq j \leq m\}|$  for $ 0 \leq r \leq k-1$
				\State $\hat{X}_i \gets w^{r*}$ where $r*=\arg\max_r g(r)$
			\State\Comment{If the max of $g(r)$ is not unique, assign $\hat{X}_i$ randomly to one of \texttt{argmax}}
				\EndFor
				\State Output $\hat{X}$
			\end{algorithmic}
		\end{algorithm}
For the multiple-sample case, we prove that Algorithm \ref{alg:ez} can recover $X$ with probability $1-o(1)$ when $\lfloor \frac{m+1}{2} \rfloor \beta>\beta^\ast$. We already showed that each sample is very close to $\Gamma(X)$ when $\alpha > b \beta$, so after the alignment step in Algorithm~\ref{alg:ez}, all the samples are simultaneously aligned with the same element from $\Gamma(X)$. We assume all samples are aligned with $X$ in the following analysis.
From Subsection \ref{subsect:smaller}, with probability $1-o(1)$, $P_{\sigma|G}(\sigma_i^{(j)} \neq X_i)$ differs from $\sum_{r=1}^{k-1} \exp (k \beta (A^r_i-A^0_i))$ by at most a constant factor for all $j\in[m]$. Since the samples are independent, we further obtain that $P_{\sigma|G}(\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i]\ge u)$ differs from $\sum_{r=1}^{k-1} \exp (k u \beta (A^r_i-A^0_i))$
by at most a constant factor.
Here $u\beta$ plays the role of $\beta$ in the single-sample case.
Therefore, if $u\beta>\beta^\ast$, then with probability $1-o(1)$ we have $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i] \le u-1$ for all $i\in[n]$. Let $u=\lfloor \frac{m+1}{2} \rfloor$,
then we have $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i] \le \lfloor \frac{m-1}{2} \rfloor $ while $\sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} = X_i]
= m - \lfloor \frac{m-1}{2} \rfloor > \sum_{j=1}^m\mathbbm{1}[\sigma_i^{(j)} \neq X_i] $
which implies that $\hat{X}=X$ after the majority voting step in Algorithm~\ref{alg:ez}.

The proof of the converse results, i.e., even ML algorithm cannot recover $X$ with probability $1-o(1)$ when $\lfloor \frac{(k-1)(m+1)}{k} \rfloor  \beta < \beta^\ast$ is rather similar to the proof of $\beta\le\beta^\ast$ for the single-sample case and can be found in arxiv.


\bibliographystyle{IEEEtran}
\bibliography{exportlist}
\end{document}
